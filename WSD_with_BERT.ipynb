{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5I3y0a98T_H"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ0wHYFs8bjo"
      },
      "source": [
        "## Install modules if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRHidmor8iDt",
        "outputId": "684115b4-61fb-4290-cc49-6b3f622971d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (4.12.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (1.21.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: sacremoses in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: colorama in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from requests->transformers) (2.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: six in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: click in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: xmltodict in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (0.12.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (1.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from pandas) (1.21.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from pandas) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (1.21.4)\n",
            "Looking in links: https://download.pytorch.org/whl/cu102/torch_stable.html\n",
            "Requirement already satisfied: torch==1.10.0+cu102 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (1.10.0+cu102)\n",
            "Requirement already satisfied: torchvision==0.11.1+cu102 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (0.11.1+cu102)\n",
            "Requirement already satisfied: torchaudio===0.10.0+cu102 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from torch==1.10.0+cu102) (4.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from torchvision==0.11.1+cu102) (8.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from torchvision==0.11.1+cu102) (1.21.4)\n",
            "Requirement already satisfied: nltk in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (3.6.5)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from nltk) (2021.11.10)\n",
            "Requirement already satisfied: joblib in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: click in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from click->nltk) (0.4.4)\n",
            "Requirement already satisfied: sklearn in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from scikit-learn->sklearn) (1.21.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hms17\\miniconda3\\envs\\wsd_env\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install xmltodict\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip3 install torch==1.10.0+cu102 torchvision==0.11.1+cu102 torchaudio===0.10.0+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html\n",
        "!pip install nltk\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGHkvHoK8Pqm"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkp0g8yO75EP",
        "outputId": "c5801c56-4634-456e-810a-1cb29c7d3271"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\hms17\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "from torch import nn\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.metrics import f1_score\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qytmMQ628IVN"
      },
      "outputs": [],
      "source": [
        "import transformers as trf\n",
        "import random, time, datetime, warnings, re\n",
        "from collections import defaultdict\n",
        "warnings.filterwarnings('ignore') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm7VNWZU8wTM"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbFaiOfG85dd"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DccQvagqSal3"
      },
      "outputs": [],
      "source": [
        "# This function reads a dataset xml file and its gold.key.txt file, and \n",
        "def loadDataset(path):\n",
        "  data = ET.parse(path + '.data.xml')\n",
        "  with open(path + '.gold.key.txt') as file:\n",
        "      labels =  [line.rstrip() for line in file.readlines()]\n",
        "  root = data.getroot()\n",
        "  dataset = []\n",
        "  for doc in root:\n",
        "    for raw_sent in doc:\n",
        "      whole_sentence = []\n",
        "      instances = [(i, x) for i, x in enumerate(raw_sent) if x.tag == 'instance']\n",
        "      for term in raw_sent:\n",
        "        whole_sentence.append(term.text.lower())\n",
        "      whole_sentence = ' '.join(whole_sentence)\n",
        "      for idx, inst in instances:\n",
        "        gold_label = labels.pop(0).split()\n",
        "        assert(gold_label[0] == inst.attrib['id'])\n",
        "        all_senses = [lemma for sense in wn.synsets(inst.text) for lemma in sense.lemmas()]\n",
        "        try:\n",
        "          label = torch.tensor([ [x.key() for x in all_senses].index(gold_label[1]) ])\n",
        "        except ValueError:\n",
        "          continue\n",
        "\n",
        "        if any(c in set(\".-\\\\/~()\") for c in inst.text): continue\n",
        "        dataset.append({\n",
        "            'sentence': whole_sentence,\n",
        "            'idx': whole_sentence.split().index(inst.text.lower()),\n",
        "            'polyseme': inst.text,\n",
        "            'lemma' : inst.attrib['lemma'],\n",
        "            'senses': all_senses,\n",
        "            'label': label\n",
        "        })\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "xxIXyO1K0wJx"
      },
      "outputs": [],
      "source": [
        "training = loadDataset('Datasets/Training/SemCor/semcor')\n",
        "validation = loadDataset('Datasets/Validation/semeval2007/semeval2007')\n",
        "testing = {\n",
        "    'SE2': loadDataset('Datasets/Testing/senseval2/senseval2'),\n",
        "    'SE3': loadDataset('Datasets/Testing/senseval2/senseval2'),\n",
        "    'SE13': loadDataset('Datasets/Testing/semeval2013/semeval2013'),\n",
        "    'SE15': loadDataset('Datasets/Testing/semeval2015/semeval2015')\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcV86t9-G_S"
      },
      "source": [
        "## DistilBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1BjCVRy-MQq",
        "outputId": "90ddb5fb-0c25-457f-c775-09c5e2c7c5b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# BERT-base\n",
        "tokenizer = trf.DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, padding=True, do_lower_case=True)\n",
        "config = trf.DistilBertConfig.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
        "model = trf.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mjvadBVjwfq"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qgb4ROeDn-v_"
      },
      "outputs": [],
      "source": [
        "def getEncodedIndeces(text, idx):\n",
        "  encodings = [(x, tokenizer(x, add_special_tokens=False)['input_ids']) for x in text.split()]\n",
        "  start, end = 0, 0\n",
        "  for i, encoding in enumerate(encodings):\n",
        "    if i == idx:\n",
        "      start += 1\n",
        "      end = start + len(encoding[1])\n",
        "      return (start, end)\n",
        "    else:\n",
        "      start += len(encoding[1])\n",
        "\n",
        "cached_output = ('', None)\n",
        "def getFeatureVec(data):\n",
        "  global cached_output\n",
        "  text = data['sentence']\n",
        "  if text == cached_output[0]:\n",
        "    hidden_states = cached_output[1]\n",
        "  else:\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    hidden_states = output[1][0][0]\n",
        "    cached_output = (text, hidden_states.clone().detach())\n",
        "  s, e = getEncodedIndeces(text, data['idx'])\n",
        "  avg = torch.mean(hidden_states[s:e], 0)\n",
        "  return avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataset):\n",
        "  loss_function = nn.CrossEntropyLoss().to(dev)\n",
        "  loss = 0.0\n",
        "  f1 = 0\n",
        "  predictions = defaultdict(list)\n",
        "  labels = defaultdict(list)\n",
        "  model.eval().cuda()\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataset):\n",
        "      feature_vec = getFeatureVec(data).to(dev)\n",
        "      label = data['label'].to(dev)\n",
        "      outputs = torch.reshape(model(feature_vec.to(dev), data), (1, -1))\n",
        "      loss = loss_function(outputs, label)\n",
        "      predictions[data['polyseme']].append(outputs)\n",
        "      labels[data['polyseme']].append(label)\n",
        "      \n",
        "      loss += loss.item()\n",
        "\n",
        "      if i % 100 == 99:\n",
        "        print(\"{}/{} complete.\".format(i+1, len(dataset)))\n",
        "    \n",
        "    all_f1_scores = []\n",
        "    f1_weights = []\n",
        "    for polyseme in labels.keys():\n",
        "      preds_tensor = torch.stack(predictions[polyseme])\n",
        "      preds_tensor = torch.argmax(preds_tensor.reshape((-1, preds_tensor.shape[-1])), dim=1)\n",
        "      labels_tensor = torch.stack(labels[polyseme]).reshape((-1,))\n",
        "      all_f1_scores.append(f1_score(labels_tensor.cpu(), preds_tensor.cpu(), average='weighted'))\n",
        "      f1_weights.append(len(labels_tensor))\n",
        "    \n",
        "    avgLoss = loss / len(dataset)\n",
        "    f1 = np.average(all_f1_scores, weights=f1_weights) * 100\n",
        "    return avgLoss, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for sanity-checking that getEncodedIndeces extracts the correct tokens.\n",
        "\n",
        "# for data in training:\n",
        "#     f = getFeatureVec(data)\n",
        "#     # print(data['sentence'], \"| Definition of:\", data['polyseme'])\n",
        "#     # data['senses'][data['label']].synset().definition()\n",
        "#     encodings = {x:tokenizer(x, add_special_tokens=False)['input_ids'] for x in data['sentence'].split()}\n",
        "#     idxs = getEncodedIndeces(data['sentence'], data['idx'])\n",
        "#     # print(encodings)\n",
        "#     extracted_token = tokenizer(data['sentence'])['input_ids'][idxs[0]:idxs[1]]\n",
        "#     # print(extracted_token)\n",
        "#     try:\n",
        "#         assert(encodings[data['polyseme'].lower()] == extracted_token)\n",
        "#     except:\n",
        "#         print(encodings)\n",
        "#         print(data['polyseme'], encodings[data['polyseme'].lower()])\n",
        "#         print(extracted_token)\n",
        "#         break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP Neural Net Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-cmmtwiNjwFQ"
      },
      "outputs": [],
      "source": [
        "class MLP_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.H = 768\n",
        "        self.L1 = nn.Linear(self.H, self.H)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.L2 = nn.ModuleDict({})\n",
        "\n",
        "    def forward(self, features, data):\n",
        "        x = self.L1(features)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        polyseme, numSenses = \"polyseme_\" + data['polyseme'], len(data['senses'])\n",
        "        if polyseme not in self.L2: \n",
        "          self.L2.update({polyseme: nn.Linear(self.H, numSenses).to(dev)})\n",
        "        \n",
        "        x = self.L2[polyseme](x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHLH8ZXWnOhl",
        "outputId": "4b8cf52e-6880-44f9-c80e-eda23ce06359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Training loss after mini-batch   500/101301: 1.961\n",
            "Time remaining for current epoch: 0:23:56.360849\n",
            "Training loss after mini-batch  1000/101301: 2.091\n",
            "Time remaining for current epoch: 0:24:02.428924\n",
            "Training loss after mini-batch  1500/101301: 2.418\n",
            "Time remaining for current epoch: 0:33:46.860319\n",
            "Training loss after mini-batch  2000/101301: 2.120\n",
            "Time remaining for current epoch: 0:30:53.453493\n",
            "Training loss after mini-batch  2500/101301: 1.888\n",
            "Time remaining for current epoch: 0:27:34.713650\n",
            "Training loss after mini-batch  3000/101301: 1.818\n",
            "Time remaining for current epoch: 0:25:29.938777\n",
            "Training loss after mini-batch  3500/101301: 1.607\n",
            "Time remaining for current epoch: 0:23:16.995865\n",
            "Training loss after mini-batch  4000/101301: 2.187\n",
            "Time remaining for current epoch: 0:25:09.152498\n",
            "Training loss after mini-batch  4500/101301: 1.952\n",
            "Time remaining for current epoch: 0:24:05.623735\n",
            "Training loss after mini-batch  5000/101301: 1.914\n",
            "Time remaining for current epoch: 0:22:42.754259\n",
            "Training loss after mini-batch  5500/101301: 1.900\n",
            "Time remaining for current epoch: 0:24:51.761692\n",
            "Training loss after mini-batch  6000/101301: 1.858\n",
            "Time remaining for current epoch: 0:24:47.878383\n",
            "Training loss after mini-batch  6500/101301: 1.890\n",
            "Time remaining for current epoch: 0:25:33.004324\n",
            "Training loss after mini-batch  7000/101301: 1.926\n",
            "Time remaining for current epoch: 0:25:41.365024\n",
            "Training loss after mini-batch  7500/101301: 2.061\n",
            "Time remaining for current epoch: 0:31:34.603507\n",
            "Training loss after mini-batch  8000/101301: 1.939\n",
            "Time remaining for current epoch: 0:28:17.662607\n",
            "Training loss after mini-batch  8500/101301: 1.551\n",
            "Time remaining for current epoch: 0:24:36.314328\n",
            "Training loss after mini-batch  9000/101301: 1.358\n",
            "Time remaining for current epoch: 0:22:52.471011\n",
            "Training loss after mini-batch  9500/101301: 1.341\n",
            "Time remaining for current epoch: 0:21:49.941261\n",
            "Training loss after mini-batch 10000/101301: 2.024\n",
            "Time remaining for current epoch: 0:23:37.538066\n",
            "Training loss after mini-batch 10500/101301: 2.242\n",
            "Time remaining for current epoch: 0:27:43.092064\n",
            "Training loss after mini-batch 11000/101301: 2.133\n",
            "Time remaining for current epoch: 0:21:47.480674\n",
            "Training loss after mini-batch 11500/101301: 1.651\n",
            "Time remaining for current epoch: 0:21:17.319102\n",
            "Training loss after mini-batch 12000/101301: 1.649\n",
            "Time remaining for current epoch: 0:23:31.605363\n",
            "Training loss after mini-batch 12500/101301: 1.694\n",
            "Time remaining for current epoch: 0:22:58.878501\n",
            "Training loss after mini-batch 13000/101301: 1.391\n",
            "Time remaining for current epoch: 0:21:43.727959\n",
            "Training loss after mini-batch 13500/101301: 1.505\n",
            "Time remaining for current epoch: 0:22:48.415557\n",
            "Training loss after mini-batch 14000/101301: 1.586\n",
            "Time remaining for current epoch: 0:20:49.245626\n",
            "Training loss after mini-batch 14500/101301: 1.598\n",
            "Time remaining for current epoch: 0:21:17.916493\n",
            "Training loss after mini-batch 15000/101301: 1.712\n",
            "Time remaining for current epoch: 0:24:23.516406\n",
            "Training loss after mini-batch 15500/101301: 1.776\n",
            "Time remaining for current epoch: 0:25:49.713506\n",
            "Training loss after mini-batch 16000/101301: 1.633\n",
            "Time remaining for current epoch: 0:28:08.301008\n",
            "Training loss after mini-batch 16500/101301: 1.986\n",
            "Time remaining for current epoch: 0:27:50.118791\n",
            "Training loss after mini-batch 17000/101301: 2.018\n",
            "Time remaining for current epoch: 0:25:30.848503\n",
            "Training loss after mini-batch 17500/101301: 1.742\n",
            "Time remaining for current epoch: 0:26:23.037202\n",
            "Training loss after mini-batch 18000/101301: 1.356\n",
            "Time remaining for current epoch: 0:20:03.329790\n",
            "Training loss after mini-batch 18500/101301: 1.381\n",
            "Time remaining for current epoch: 0:19:37.370781\n",
            "Training loss after mini-batch 19000/101301: 1.871\n",
            "Time remaining for current epoch: 0:23:10.226703\n",
            "Training loss after mini-batch 19500/101301: 1.777\n",
            "Time remaining for current epoch: 0:22:40.985224\n",
            "Training loss after mini-batch 20000/101301: 1.784\n",
            "Time remaining for current epoch: 0:23:36.654162\n",
            "Training loss after mini-batch 20500/101301: 1.948\n",
            "Time remaining for current epoch: 0:25:56.038756\n",
            "Training loss after mini-batch 21000/101301: 1.955\n",
            "Time remaining for current epoch: 0:26:12.299356\n",
            "Training loss after mini-batch 21500/101301: 1.494\n",
            "Time remaining for current epoch: 0:25:00.771453\n",
            "Training loss after mini-batch 22000/101301: 1.359\n",
            "Time remaining for current epoch: 0:24:35.706918\n",
            "Training loss after mini-batch 22500/101301: 1.682\n",
            "Time remaining for current epoch: 0:24:58.063466\n",
            "Training loss after mini-batch 23000/101301: 1.952\n",
            "Time remaining for current epoch: 0:20:12.651805\n",
            "Training loss after mini-batch 23500/101301: 1.537\n",
            "Time remaining for current epoch: 0:20:11.072030\n",
            "Training loss after mini-batch 24000/101301: 1.641\n",
            "Time remaining for current epoch: 0:18:58.688258\n",
            "Training loss after mini-batch 24500/101301: 1.215\n",
            "Time remaining for current epoch: 0:19:12.850338\n",
            "Training loss after mini-batch 25000/101301: 1.703\n",
            "Time remaining for current epoch: 0:21:00.513293\n",
            "Training loss after mini-batch 25500/101301: 1.724\n",
            "Time remaining for current epoch: 0:20:08.268233\n",
            "Training loss after mini-batch 26000/101301: 1.517\n",
            "Time remaining for current epoch: 0:17:25.628046\n",
            "Training loss after mini-batch 26500/101301: 1.662\n",
            "Time remaining for current epoch: 0:17:52.252364\n",
            "Training loss after mini-batch 27000/101301: 1.927\n",
            "Time remaining for current epoch: 0:20:36.383758\n",
            "Training loss after mini-batch 27500/101301: 1.396\n",
            "Time remaining for current epoch: 0:18:14.292306\n",
            "Training loss after mini-batch 28000/101301: 1.277\n",
            "Time remaining for current epoch: 0:17:25.849590\n",
            "Training loss after mini-batch 28500/101301: 1.842\n",
            "Time remaining for current epoch: 0:21:04.991663\n",
            "Training loss after mini-batch 29000/101301: 2.076\n",
            "Time remaining for current epoch: 0:19:02.721154\n",
            "Training loss after mini-batch 29500/101301: 1.343\n",
            "Time remaining for current epoch: 0:16:35.158172\n",
            "Training loss after mini-batch 30000/101301: 1.113\n",
            "Time remaining for current epoch: 0:15:19.252217\n",
            "Training loss after mini-batch 30500/101301: 1.725\n",
            "Time remaining for current epoch: 0:20:09.629315\n",
            "Training loss after mini-batch 31000/101301: 1.597\n",
            "Time remaining for current epoch: 0:19:52.881811\n",
            "Training loss after mini-batch 31500/101301: 1.172\n",
            "Time remaining for current epoch: 0:15:54.258990\n",
            "Training loss after mini-batch 32000/101301: 1.019\n",
            "Time remaining for current epoch: 0:15:34.802698\n",
            "Training loss after mini-batch 32500/101301: 1.554\n",
            "Time remaining for current epoch: 0:15:50.536866\n",
            "Training loss after mini-batch 33000/101301: 1.439\n",
            "Time remaining for current epoch: 0:16:08.367766\n",
            "Training loss after mini-batch 33500/101301: 1.531\n",
            "Time remaining for current epoch: 0:17:14.313477\n",
            "Training loss after mini-batch 34000/101301: 1.697\n",
            "Time remaining for current epoch: 0:20:04.467694\n",
            "Training loss after mini-batch 34500/101301: 1.781\n",
            "Time remaining for current epoch: 0:19:22.249587\n",
            "Training loss after mini-batch 35000/101301: 1.663\n",
            "Time remaining for current epoch: 0:17:37.427559\n",
            "Training loss after mini-batch 35500/101301: 1.432\n",
            "Time remaining for current epoch: 0:15:37.592084\n",
            "Training loss after mini-batch 36000/101301: 1.649\n",
            "Time remaining for current epoch: 0:20:02.851089\n",
            "Training loss after mini-batch 36500/101301: 1.848\n",
            "Time remaining for current epoch: 0:19:17.954466\n",
            "Training loss after mini-batch 37000/101301: 1.911\n",
            "Time remaining for current epoch: 0:19:13.895713\n",
            "Training loss after mini-batch 37500/101301: 1.833\n",
            "Time remaining for current epoch: 0:17:04.544878\n",
            "Training loss after mini-batch 38000/101301: 1.615\n",
            "Time remaining for current epoch: 0:13:55.788945\n",
            "Training loss after mini-batch 38500/101301: 1.617\n",
            "Time remaining for current epoch: 0:15:50.041925\n",
            "Training loss after mini-batch 39000/101301: 1.819\n",
            "Time remaining for current epoch: 0:15:19.450985\n",
            "Training loss after mini-batch 39500/101301: 1.583\n",
            "Time remaining for current epoch: 0:14:00.559988\n",
            "Training loss after mini-batch 40000/101301: 1.378\n",
            "Time remaining for current epoch: 0:13:59.488296\n",
            "Training loss after mini-batch 40500/101301: 1.837\n",
            "Time remaining for current epoch: 0:16:43.741113\n",
            "Training loss after mini-batch 41000/101301: 1.564\n",
            "Time remaining for current epoch: 0:13:49.317598\n",
            "Training loss after mini-batch 41500/101301: 1.337\n",
            "Time remaining for current epoch: 0:13:34.933454\n",
            "Training loss after mini-batch 42000/101301: 1.333\n",
            "Time remaining for current epoch: 0:13:21.905353\n",
            "Training loss after mini-batch 42500/101301: 1.390\n",
            "Time remaining for current epoch: 0:13:37.194218\n",
            "Training loss after mini-batch 43000/101301: 1.499\n",
            "Time remaining for current epoch: 0:14:48.955976\n",
            "Training loss after mini-batch 43500/101301: 1.444\n",
            "Time remaining for current epoch: 0:16:13.410798\n",
            "Training loss after mini-batch 44000/101301: 1.686\n",
            "Time remaining for current epoch: 0:14:11.766319\n",
            "Training loss after mini-batch 44500/101301: 1.627\n",
            "Time remaining for current epoch: 0:16:40.331813\n",
            "Training loss after mini-batch 45000/101301: 1.760\n",
            "Time remaining for current epoch: 0:16:30.065866\n",
            "Training loss after mini-batch 45500/101301: 2.061\n",
            "Time remaining for current epoch: 0:14:10.847776\n",
            "Training loss after mini-batch 46000/101301: 1.671\n",
            "Time remaining for current epoch: 0:13:20.453649\n",
            "Training loss after mini-batch 46500/101301: 1.255\n",
            "Time remaining for current epoch: 0:12:38.319556\n",
            "Training loss after mini-batch 47000/101301: 1.652\n",
            "Time remaining for current epoch: 0:13:29.015849\n",
            "Training loss after mini-batch 47500/101301: 1.320\n",
            "Time remaining for current epoch: 0:12:48.250944\n",
            "Training loss after mini-batch 48000/101301: 1.748\n",
            "Time remaining for current epoch: 0:13:01.396515\n",
            "Training loss after mini-batch 48500/101301: 1.738\n",
            "Time remaining for current epoch: 0:13:16.974586\n",
            "Training loss after mini-batch 49000/101301: 1.032\n",
            "Time remaining for current epoch: 0:11:58.068540\n",
            "Training loss after mini-batch 49500/101301: 0.944\n",
            "Time remaining for current epoch: 0:11:27.118744\n",
            "Training loss after mini-batch 50000/101301: 0.862\n",
            "Time remaining for current epoch: 0:11:09.486719\n",
            "Training loss after mini-batch 50500/101301: 1.169\n",
            "Time remaining for current epoch: 0:12:14.318592\n",
            "Training loss after mini-batch 51000/101301: 1.702\n",
            "Time remaining for current epoch: 0:12:37.873794\n",
            "Training loss after mini-batch 51500/101301: 1.521\n",
            "Time remaining for current epoch: 0:11:31.445648\n",
            "Training loss after mini-batch 52000/101301: 1.261\n",
            "Time remaining for current epoch: 0:12:13.101445\n",
            "Training loss after mini-batch 52500/101301: 1.420\n",
            "Time remaining for current epoch: 0:11:08.937170\n",
            "Training loss after mini-batch 53000/101301: 1.540\n",
            "Time remaining for current epoch: 0:11:07.617022\n",
            "Training loss after mini-batch 53500/101301: 1.309\n",
            "Time remaining for current epoch: 0:11:40.130800\n",
            "Training loss after mini-batch 54000/101301: 1.237\n",
            "Time remaining for current epoch: 0:12:03.866915\n",
            "Training loss after mini-batch 54500/101301: 1.695\n",
            "Time remaining for current epoch: 0:12:34.251954\n",
            "Training loss after mini-batch 55000/101301: 1.850\n",
            "Time remaining for current epoch: 0:12:07.881471\n",
            "Training loss after mini-batch 55500/101301: 1.836\n",
            "Time remaining for current epoch: 0:12:37.852306\n",
            "Training loss after mini-batch 56000/101301: 1.716\n",
            "Time remaining for current epoch: 0:10:43.489361\n",
            "Training loss after mini-batch 56500/101301: 1.614\n",
            "Time remaining for current epoch: 0:10:25.853234\n",
            "Training loss after mini-batch 57000/101301: 1.833\n",
            "Time remaining for current epoch: 0:10:13.768408\n",
            "Training loss after mini-batch 57500/101301: 1.824\n",
            "Time remaining for current epoch: 0:09:38.417578\n",
            "Training loss after mini-batch 58000/101301: 1.817\n",
            "Time remaining for current epoch: 0:12:19.942037\n",
            "Training loss after mini-batch 58500/101301: 1.627\n",
            "Time remaining for current epoch: 0:11:33.907869\n",
            "Training loss after mini-batch 59000/101301: 1.476\n",
            "Time remaining for current epoch: 0:09:51.050514\n",
            "Training loss after mini-batch 59500/101301: 1.370\n",
            "Time remaining for current epoch: 0:10:11.310085\n",
            "Training loss after mini-batch 60000/101301: 1.368\n",
            "Time remaining for current epoch: 0:10:02.396966\n",
            "Training loss after mini-batch 60500/101301: 1.779\n",
            "Time remaining for current epoch: 0:10:35.277667\n",
            "Training loss after mini-batch 61000/101301: 1.697\n",
            "Time remaining for current epoch: 0:10:20.466794\n",
            "Training loss after mini-batch 61500/101301: 1.324\n",
            "Time remaining for current epoch: 0:09:09.499315\n",
            "Training loss after mini-batch 62000/101301: 1.552\n",
            "Time remaining for current epoch: 0:08:56.408580\n",
            "Training loss after mini-batch 62500/101301: 1.584\n",
            "Time remaining for current epoch: 0:13:33.090327\n",
            "Training loss after mini-batch 63000/101301: 1.552\n",
            "Time remaining for current epoch: 0:13:00.078334\n",
            "Training loss after mini-batch 63500/101301: 1.751\n",
            "Time remaining for current epoch: 0:09:27.390315\n",
            "Training loss after mini-batch 64000/101301: 1.534\n",
            "Time remaining for current epoch: 0:08:37.787544\n",
            "Training loss after mini-batch 64500/101301: 1.576\n",
            "Time remaining for current epoch: 0:08:56.000215\n",
            "Training loss after mini-batch 65000/101301: 1.197\n",
            "Time remaining for current epoch: 0:08:40.449597\n",
            "Training loss after mini-batch 65500/101301: 0.811\n",
            "Time remaining for current epoch: 0:08:39.564020\n",
            "Training loss after mini-batch 66000/101301: 1.820\n",
            "Time remaining for current epoch: 0:09:34.550139\n",
            "Training loss after mini-batch 66500/101301: 1.393\n",
            "Time remaining for current epoch: 0:09:23.692408\n",
            "Training loss after mini-batch 67000/101301: 1.417\n",
            "Time remaining for current epoch: 0:09:26.239661\n",
            "Training loss after mini-batch 67500/101301: 1.597\n",
            "Time remaining for current epoch: 0:11:07.809096\n",
            "Training loss after mini-batch 68000/101301: 1.721\n",
            "Time remaining for current epoch: 0:08:11.001042\n",
            "Training loss after mini-batch 68500/101301: 1.653\n",
            "Time remaining for current epoch: 0:08:27.661022\n",
            "Training loss after mini-batch 69000/101301: 1.531\n",
            "Time remaining for current epoch: 0:08:15.454141\n",
            "Training loss after mini-batch 69500/101301: 1.648\n",
            "Time remaining for current epoch: 0:08:16.474892\n",
            "Training loss after mini-batch 70000/101301: 1.611\n",
            "Time remaining for current epoch: 0:10:13.468701\n",
            "Training loss after mini-batch 70500/101301: 1.430\n",
            "Time remaining for current epoch: 0:10:00.689631\n",
            "Training loss after mini-batch 71000/101301: 1.094\n",
            "Time remaining for current epoch: 0:08:37.017560\n",
            "Training loss after mini-batch 71500/101301: 1.163\n",
            "Time remaining for current epoch: 0:07:29.182116\n",
            "Training loss after mini-batch 72000/101301: 1.529\n",
            "Time remaining for current epoch: 0:06:46.804251\n",
            "Training loss after mini-batch 72500/101301: 1.469\n",
            "Time remaining for current epoch: 0:06:25.662690\n",
            "Training loss after mini-batch 73000/101301: 1.294\n",
            "Time remaining for current epoch: 0:06:35.340606\n",
            "Training loss after mini-batch 73500/101301: 1.553\n",
            "Time remaining for current epoch: 0:07:23.775546\n",
            "Training loss after mini-batch 74000/101301: 1.617\n",
            "Time remaining for current epoch: 0:07:24.227209\n",
            "Training loss after mini-batch 74500/101301: 1.569\n",
            "Time remaining for current epoch: 0:07:17.099541\n",
            "Training loss after mini-batch 75000/101301: 1.826\n",
            "Time remaining for current epoch: 0:06:57.496947\n",
            "Training loss after mini-batch 75500/101301: 1.707\n",
            "Time remaining for current epoch: 0:06:54.426016\n",
            "Training loss after mini-batch 76000/101301: 1.411\n",
            "Time remaining for current epoch: 0:05:42.052569\n",
            "Training loss after mini-batch 76500/101301: 1.189\n",
            "Time remaining for current epoch: 0:05:30.763035\n",
            "Training loss after mini-batch 77000/101301: 1.624\n",
            "Time remaining for current epoch: 0:07:10.111867\n",
            "Training loss after mini-batch 77500/101301: 1.603\n",
            "Time remaining for current epoch: 0:07:33.807525\n",
            "Training loss after mini-batch 78000/101301: 1.493\n",
            "Time remaining for current epoch: 0:05:52.626541\n",
            "Training loss after mini-batch 78500/101301: 1.606\n",
            "Time remaining for current epoch: 0:05:28.552687\n",
            "Training loss after mini-batch 79000/101301: 1.684\n",
            "Time remaining for current epoch: 0:05:10.708871\n",
            "Training loss after mini-batch 79500/101301: 1.468\n",
            "Time remaining for current epoch: 0:04:58.650337\n",
            "Training loss after mini-batch 80000/101301: 1.363\n",
            "Time remaining for current epoch: 0:05:02.477792\n",
            "Training loss after mini-batch 80500/101301: 1.824\n",
            "Time remaining for current epoch: 0:05:34.365071\n",
            "Training loss after mini-batch 81000/101301: 1.495\n",
            "Time remaining for current epoch: 0:05:26.566832\n",
            "Training loss after mini-batch 81500/101301: 1.022\n",
            "Time remaining for current epoch: 0:04:39.862411\n",
            "Training loss after mini-batch 82000/101301: 1.390\n",
            "Time remaining for current epoch: 0:04:26.997124\n",
            "Training loss after mini-batch 82500/101301: 0.869\n",
            "Time remaining for current epoch: 0:04:11.006563\n",
            "Training loss after mini-batch 83000/101301: 0.973\n",
            "Time remaining for current epoch: 0:04:12.743240\n",
            "Training loss after mini-batch 83500/101301: 1.208\n",
            "Time remaining for current epoch: 0:04:27.036399\n",
            "Training loss after mini-batch 84000/101301: 1.270\n",
            "Time remaining for current epoch: 0:04:05.821867\n",
            "Training loss after mini-batch 84500/101301: 1.094\n",
            "Time remaining for current epoch: 0:03:49.020488\n",
            "Training loss after mini-batch 85000/101301: 1.250\n",
            "Time remaining for current epoch: 0:03:58.602449\n",
            "Training loss after mini-batch 85500/101301: 1.523\n",
            "Time remaining for current epoch: 0:04:32.118917\n",
            "Training loss after mini-batch 86000/101301: 1.692\n",
            "Time remaining for current epoch: 0:04:59.364693\n",
            "Training loss after mini-batch 86500/101301: 1.551\n",
            "Time remaining for current epoch: 0:04:58.719247\n",
            "Training loss after mini-batch 87000/101301: 1.555\n",
            "Time remaining for current epoch: 0:03:38.041595\n",
            "Training loss after mini-batch 87500/101301: 1.634\n",
            "Time remaining for current epoch: 0:03:51.897268\n",
            "Training loss after mini-batch 88000/101301: 1.649\n",
            "Time remaining for current epoch: 0:03:29.932074\n",
            "Training loss after mini-batch 88500/101301: 1.416\n",
            "Time remaining for current epoch: 0:03:20.928538\n",
            "Training loss after mini-batch 89000/101301: 1.311\n",
            "Time remaining for current epoch: 0:02:53.989304\n",
            "Training loss after mini-batch 89500/101301: 1.532\n",
            "Time remaining for current epoch: 0:02:58.215351\n",
            "Training loss after mini-batch 90000/101301: 1.599\n",
            "Time remaining for current epoch: 0:02:52.623994\n",
            "Training loss after mini-batch 90500/101301: 1.270\n",
            "Time remaining for current epoch: 0:02:27.972905\n",
            "Training loss after mini-batch 91000/101301: 1.234\n",
            "Time remaining for current epoch: 0:02:34.519126\n",
            "Training loss after mini-batch 91500/101301: 1.509\n",
            "Time remaining for current epoch: 0:02:32.858267\n",
            "Training loss after mini-batch 92000/101301: 1.591\n",
            "Time remaining for current epoch: 0:02:39.663360\n",
            "Training loss after mini-batch 92500/101301: 1.195\n",
            "Time remaining for current epoch: 0:01:58.670587\n",
            "Training loss after mini-batch 93000/101301: 1.082\n",
            "Time remaining for current epoch: 0:01:51.553079\n",
            "Training loss after mini-batch 93500/101301: 1.011\n",
            "Time remaining for current epoch: 0:01:42.169731\n",
            "Training loss after mini-batch 94000/101301: 0.933\n",
            "Time remaining for current epoch: 0:01:35.359572\n",
            "Training loss after mini-batch 94500/101301: 1.277\n",
            "Time remaining for current epoch: 0:01:46.862983\n",
            "Training loss after mini-batch 95000/101301: 0.979\n",
            "Time remaining for current epoch: 0:01:35.730110\n",
            "Training loss after mini-batch 95500/101301: 1.030\n",
            "Time remaining for current epoch: 0:01:18.937440\n",
            "Training loss after mini-batch 96000/101301: 1.131\n",
            "Time remaining for current epoch: 0:01:30.892434\n",
            "Training loss after mini-batch 96500/101301: 1.460\n",
            "Time remaining for current epoch: 0:01:39.668873\n",
            "Training loss after mini-batch 97000/101301: 1.680\n",
            "Time remaining for current epoch: 0:01:07.899391\n",
            "Training loss after mini-batch 97500/101301: 1.785\n",
            "Time remaining for current epoch: 0:00:59.992832\n",
            "Training loss after mini-batch 98000/101301: 1.484\n",
            "Time remaining for current epoch: 0:00:53.554193\n",
            "Training loss after mini-batch 98500/101301: 1.451\n",
            "Time remaining for current epoch: 0:00:47.788786\n",
            "Training loss after mini-batch 99000/101301: 1.499\n",
            "Time remaining for current epoch: 0:00:33.785924\n",
            "Training loss after mini-batch 99500/101301: 1.414\n",
            "Time remaining for current epoch: 0:00:24.335882\n",
            "Training loss after mini-batch 100000/101301: 1.256\n",
            "Time remaining for current epoch: 0:00:18.211973\n",
            "Training loss after mini-batch 100500/101301: 0.945\n",
            "Time remaining for current epoch: 0:00:11.177528\n",
            "Training loss after mini-batch 101000/101301: 0.855\n",
            "Time remaining for current epoch: 0:00:04.203921\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 1 complete. Training loss: 1.55276. Validation loss: 0.00002. Validation F1 score: 39.18960\n",
            "F1 score increased (0.000000--->39.189602) \t Saving The Model...\n",
            "Starting epoch 2\n",
            "Training loss after mini-batch   500/101301: 1.416\n",
            "Time remaining for current epoch: 0:27:25.231339\n",
            "Training loss after mini-batch  1000/101301: 1.083\n",
            "Time remaining for current epoch: 0:25:01.100804\n",
            "Training loss after mini-batch  1500/101301: 1.168\n",
            "Time remaining for current epoch: 0:23:57.633378\n",
            "Training loss after mini-batch  2000/101301: 1.447\n",
            "Time remaining for current epoch: 0:30:09.913799\n",
            "Training loss after mini-batch  2500/101301: 1.783\n",
            "Time remaining for current epoch: 0:24:41.715886\n",
            "Training loss after mini-batch  3000/101301: 1.580\n",
            "Time remaining for current epoch: 0:25:42.139686\n",
            "Training loss after mini-batch  3500/101301: 1.446\n",
            "Time remaining for current epoch: 0:22:17.917010\n",
            "Training loss after mini-batch  4000/101301: 1.287\n",
            "Time remaining for current epoch: 0:21:38.925424\n",
            "Training loss after mini-batch  4500/101301: 1.490\n",
            "Time remaining for current epoch: 0:27:05.814068\n",
            "Training loss after mini-batch  5000/101301: 1.422\n",
            "Time remaining for current epoch: 0:29:08.746979\n",
            "Training loss after mini-batch  5500/101301: 1.864\n",
            "Time remaining for current epoch: 0:27:13.809888\n",
            "Training loss after mini-batch  6000/101301: 1.561\n",
            "Time remaining for current epoch: 0:29:50.074582\n",
            "Training loss after mini-batch  6500/101301: 1.550\n",
            "Time remaining for current epoch: 0:28:11.383977\n",
            "Training loss after mini-batch  7000/101301: 1.405\n",
            "Time remaining for current epoch: 0:27:33.321066\n",
            "Training loss after mini-batch  7500/101301: 1.510\n",
            "Time remaining for current epoch: 0:25:36.897024\n",
            "Training loss after mini-batch  8000/101301: 1.270\n",
            "Time remaining for current epoch: 0:21:45.250849\n",
            "Training loss after mini-batch  8500/101301: 1.170\n",
            "Time remaining for current epoch: 0:23:41.846788\n",
            "Training loss after mini-batch  9000/101301: 1.366\n",
            "Time remaining for current epoch: 0:24:55.978315\n",
            "Training loss after mini-batch  9500/101301: 1.551\n",
            "Time remaining for current epoch: 0:22:24.095499\n",
            "Training loss after mini-batch 10000/101301: 1.507\n",
            "Time remaining for current epoch: 0:23:22.339358\n",
            "Training loss after mini-batch 10500/101301: 1.692\n",
            "Time remaining for current epoch: 0:22:33.584888\n",
            "Training loss after mini-batch 11000/101301: 1.531\n",
            "Time remaining for current epoch: 0:21:38.437751\n",
            "Training loss after mini-batch 11500/101301: 1.735\n",
            "Time remaining for current epoch: 0:20:35.773028\n",
            "Training loss after mini-batch 12000/101301: 1.310\n",
            "Time remaining for current epoch: 0:19:47.872283\n",
            "Training loss after mini-batch 12500/101301: 1.537\n",
            "Time remaining for current epoch: 0:19:52.374306\n",
            "Training loss after mini-batch 13000/101301: 1.269\n",
            "Time remaining for current epoch: 0:19:43.363935\n",
            "Training loss after mini-batch 13500/101301: 1.098\n",
            "Time remaining for current epoch: 0:19:58.850411\n",
            "Training loss after mini-batch 14000/101301: 1.258\n",
            "Time remaining for current epoch: 0:23:44.133321\n",
            "Training loss after mini-batch 14500/101301: 1.343\n",
            "Time remaining for current epoch: 0:25:57.102621\n",
            "Training loss after mini-batch 15000/101301: 1.273\n",
            "Time remaining for current epoch: 0:20:42.605892\n",
            "Training loss after mini-batch 15500/101301: 1.128\n",
            "Time remaining for current epoch: 0:20:08.811768\n",
            "Training loss after mini-batch 16000/101301: 1.402\n",
            "Time remaining for current epoch: 0:21:35.147950\n",
            "Training loss after mini-batch 16500/101301: 1.268\n",
            "Time remaining for current epoch: 0:20:43.438447\n",
            "Training loss after mini-batch 17000/101301: 0.898\n",
            "Time remaining for current epoch: 0:24:30.041780\n",
            "Training loss after mini-batch 17500/101301: 1.456\n",
            "Time remaining for current epoch: 0:20:51.930202\n",
            "Training loss after mini-batch 18000/101301: 1.363\n",
            "Time remaining for current epoch: 0:21:00.607966\n",
            "Training loss after mini-batch 18500/101301: 1.766\n",
            "Time remaining for current epoch: 0:20:24.257704\n",
            "Training loss after mini-batch 19000/101301: 1.332\n",
            "Time remaining for current epoch: 0:19:34.336569\n",
            "Training loss after mini-batch 19500/101301: 0.908\n",
            "Time remaining for current epoch: 0:19:08.882725\n",
            "Training loss after mini-batch 20000/101301: 1.268\n",
            "Time remaining for current epoch: 0:19:39.631202\n",
            "Training loss after mini-batch 20500/101301: 1.319\n",
            "Time remaining for current epoch: 0:19:58.024379\n",
            "Training loss after mini-batch 21000/101301: 1.203\n",
            "Time remaining for current epoch: 0:17:12.536586\n",
            "Training loss after mini-batch 21500/101301: 1.282\n",
            "Time remaining for current epoch: 0:17:03.171133\n",
            "Training loss after mini-batch 22000/101301: 1.579\n",
            "Time remaining for current epoch: 0:17:21.772819\n",
            "Training loss after mini-batch 22500/101301: 1.357\n",
            "Time remaining for current epoch: 0:17:02.319796\n",
            "Training loss after mini-batch 23000/101301: 1.714\n",
            "Time remaining for current epoch: 0:20:21.829533\n",
            "Training loss after mini-batch 23500/101301: 1.457\n",
            "Time remaining for current epoch: 0:23:07.371788\n",
            "Training loss after mini-batch 24000/101301: 1.449\n",
            "Time remaining for current epoch: 0:23:13.302872\n",
            "Training loss after mini-batch 24500/101301: 1.533\n",
            "Time remaining for current epoch: 0:21:02.065603\n",
            "Training loss after mini-batch 25000/101301: 1.694\n",
            "Time remaining for current epoch: 0:19:28.679562\n",
            "Training loss after mini-batch 25500/101301: 1.479\n",
            "Time remaining for current epoch: 0:21:35.352463\n",
            "Training loss after mini-batch 26000/101301: 1.489\n",
            "Time remaining for current epoch: 0:19:31.367508\n",
            "Training loss after mini-batch 26500/101301: 1.498\n",
            "Time remaining for current epoch: 0:16:50.649769\n",
            "Training loss after mini-batch 27000/101301: 1.093\n",
            "Time remaining for current epoch: 0:16:48.067914\n",
            "Training loss after mini-batch 27500/101301: 1.043\n",
            "Time remaining for current epoch: 0:16:42.793898\n",
            "Training loss after mini-batch 28000/101301: 1.334\n",
            "Time remaining for current epoch: 0:18:24.584496\n",
            "Training loss after mini-batch 28500/101301: 1.332\n",
            "Time remaining for current epoch: 0:22:33.219004\n",
            "Training loss after mini-batch 29000/101301: 1.540\n",
            "Time remaining for current epoch: 0:17:13.350357\n",
            "Training loss after mini-batch 29500/101301: 1.435\n",
            "Time remaining for current epoch: 0:16:47.266326\n",
            "Training loss after mini-batch 30000/101301: 1.412\n",
            "Time remaining for current epoch: 0:16:43.820158\n",
            "Training loss after mini-batch 30500/101301: 1.156\n",
            "Time remaining for current epoch: 0:16:38.068472\n",
            "Training loss after mini-batch 31000/101301: 1.435\n",
            "Time remaining for current epoch: 0:18:02.015247\n",
            "Training loss after mini-batch 31500/101301: 1.444\n",
            "Time remaining for current epoch: 0:19:20.142205\n",
            "Training loss after mini-batch 32000/101301: 1.438\n",
            "Time remaining for current epoch: 0:19:59.775644\n",
            "Training loss after mini-batch 32500/101301: 1.308\n",
            "Time remaining for current epoch: 0:15:16.739193\n",
            "Training loss after mini-batch 33000/101301: 1.293\n",
            "Time remaining for current epoch: 0:15:38.070374\n",
            "Training loss after mini-batch 33500/101301: 1.371\n",
            "Time remaining for current epoch: 0:14:39.729381\n",
            "Training loss after mini-batch 34000/101301: 1.247\n",
            "Time remaining for current epoch: 0:14:43.315516\n",
            "Training loss after mini-batch 34500/101301: 1.578\n",
            "Time remaining for current epoch: 0:15:52.028858\n",
            "Training loss after mini-batch 35000/101301: 1.282\n",
            "Time remaining for current epoch: 0:15:49.289804\n",
            "Training loss after mini-batch 35500/101301: 1.424\n",
            "Time remaining for current epoch: 0:16:59.516895\n",
            "Training loss after mini-batch 36000/101301: 1.310\n",
            "Time remaining for current epoch: 0:17:34.765476\n",
            "Training loss after mini-batch 36500/101301: 1.571\n",
            "Time remaining for current epoch: 0:14:44.702059\n",
            "Training loss after mini-batch 37000/101301: 1.392\n",
            "Time remaining for current epoch: 0:15:53.438001\n",
            "Training loss after mini-batch 37500/101301: 1.167\n",
            "Time remaining for current epoch: 0:14:37.104426\n",
            "Training loss after mini-batch 38000/101301: 1.454\n",
            "Time remaining for current epoch: 0:16:02.945611\n",
            "Training loss after mini-batch 38500/101301: 1.435\n",
            "Time remaining for current epoch: 0:14:59.071243\n",
            "Training loss after mini-batch 39000/101301: 1.471\n",
            "Time remaining for current epoch: 0:15:28.899256\n",
            "Training loss after mini-batch 39500/101301: 1.390\n",
            "Time remaining for current epoch: 0:13:56.455514\n",
            "Training loss after mini-batch 40000/101301: 1.322\n",
            "Time remaining for current epoch: 0:14:05.765333\n",
            "Training loss after mini-batch 40500/101301: 1.149\n",
            "Time remaining for current epoch: 0:13:08.977684\n",
            "Training loss after mini-batch 41000/101301: 1.050\n",
            "Time remaining for current epoch: 0:13:28.242532\n",
            "Training loss after mini-batch 41500/101301: 1.579\n",
            "Time remaining for current epoch: 0:14:32.444445\n",
            "Training loss after mini-batch 42000/101301: 1.566\n",
            "Time remaining for current epoch: 0:15:23.296899\n",
            "Training loss after mini-batch 42500/101301: 1.419\n",
            "Time remaining for current epoch: 0:18:13.434948\n",
            "Training loss after mini-batch 43000/101301: 1.574\n",
            "Time remaining for current epoch: 0:16:16.179710\n",
            "Training loss after mini-batch 43500/101301: 1.355\n",
            "Time remaining for current epoch: 0:14:26.748313\n",
            "Training loss after mini-batch 44000/101301: 1.397\n",
            "Time remaining for current epoch: 0:14:34.592104\n",
            "Training loss after mini-batch 44500/101301: 1.327\n",
            "Time remaining for current epoch: 0:15:40.918833\n",
            "Training loss after mini-batch 45000/101301: 1.197\n",
            "Time remaining for current epoch: 0:13:12.814186\n",
            "Training loss after mini-batch 45500/101301: 1.481\n",
            "Time remaining for current epoch: 0:13:13.906623\n",
            "Training loss after mini-batch 46000/101301: 1.574\n",
            "Time remaining for current epoch: 0:12:32.992797\n",
            "Training loss after mini-batch 46500/101301: 1.473\n",
            "Time remaining for current epoch: 0:13:08.067653\n",
            "Training loss after mini-batch 47000/101301: 1.274\n",
            "Time remaining for current epoch: 0:12:11.445644\n",
            "Training loss after mini-batch 47500/101301: 1.180\n",
            "Time remaining for current epoch: 0:12:32.625362\n",
            "Training loss after mini-batch 48000/101301: 1.041\n",
            "Time remaining for current epoch: 0:12:05.730998\n",
            "Training loss after mini-batch 48500/101301: 1.026\n",
            "Time remaining for current epoch: 0:11:37.533062\n",
            "Training loss after mini-batch 49000/101301: 1.050\n",
            "Time remaining for current epoch: 0:12:40.808403\n",
            "Training loss after mini-batch 49500/101301: 1.397\n",
            "Time remaining for current epoch: 0:11:36.055214\n",
            "Training loss after mini-batch 50000/101301: 1.549\n",
            "Time remaining for current epoch: 0:11:40.934277\n",
            "Training loss after mini-batch 50500/101301: 1.692\n",
            "Time remaining for current epoch: 0:13:28.825903\n",
            "Training loss after mini-batch 51000/101301: 1.538\n",
            "Time remaining for current epoch: 0:12:23.508874\n",
            "Training loss after mini-batch 51500/101301: 0.799\n",
            "Time remaining for current epoch: 0:11:47.976268\n",
            "Training loss after mini-batch 52000/101301: 1.119\n",
            "Time remaining for current epoch: 0:11:25.913840\n",
            "Training loss after mini-batch 52500/101301: 1.562\n",
            "Time remaining for current epoch: 0:12:35.422214\n",
            "Training loss after mini-batch 53000/101301: 1.511\n",
            "Time remaining for current epoch: 0:13:23.706178\n",
            "Training loss after mini-batch 53500/101301: 1.561\n",
            "Time remaining for current epoch: 0:15:42.187070\n",
            "Training loss after mini-batch 54000/101301: 1.564\n",
            "Time remaining for current epoch: 0:12:13.269027\n",
            "Training loss after mini-batch 54500/101301: 1.678\n",
            "Time remaining for current epoch: 0:12:25.216155\n",
            "Training loss after mini-batch 55000/101301: 1.663\n",
            "Time remaining for current epoch: 0:13:16.614243\n",
            "Training loss after mini-batch 55500/101301: 1.696\n",
            "Time remaining for current epoch: 0:11:13.261173\n",
            "Training loss after mini-batch 56000/101301: 1.489\n",
            "Time remaining for current epoch: 0:11:25.802699\n",
            "Training loss after mini-batch 56500/101301: 1.687\n",
            "Time remaining for current epoch: 0:11:00.549382\n",
            "Training loss after mini-batch 57000/101301: 1.646\n",
            "Time remaining for current epoch: 0:10:01.405839\n",
            "Training loss after mini-batch 57500/101301: 1.492\n",
            "Time remaining for current epoch: 0:12:35.759359\n",
            "Training loss after mini-batch 58000/101301: 1.688\n",
            "Time remaining for current epoch: 0:11:54.810591\n",
            "Training loss after mini-batch 58500/101301: 1.538\n",
            "Time remaining for current epoch: 0:11:33.796068\n",
            "Training loss after mini-batch 59000/101301: 0.948\n",
            "Time remaining for current epoch: 0:09:50.783333\n",
            "Training loss after mini-batch 59500/101301: 0.846\n",
            "Time remaining for current epoch: 0:09:22.992987\n",
            "Training loss after mini-batch 60000/101301: 1.358\n",
            "Time remaining for current epoch: 0:09:24.947217\n",
            "Training loss after mini-batch 60500/101301: 1.492\n",
            "Time remaining for current epoch: 0:10:06.398469\n",
            "Training loss after mini-batch 61000/101301: 1.407\n",
            "Time remaining for current epoch: 0:10:13.834522\n",
            "Training loss after mini-batch 61500/101301: 1.281\n",
            "Time remaining for current epoch: 0:11:57.640089\n",
            "Training loss after mini-batch 62000/101301: 1.506\n",
            "Time remaining for current epoch: 0:08:42.557809\n",
            "Training loss after mini-batch 62500/101301: 1.434\n",
            "Time remaining for current epoch: 0:09:42.521634\n",
            "Training loss after mini-batch 63000/101301: 1.473\n",
            "Time remaining for current epoch: 0:10:06.303676\n",
            "Training loss after mini-batch 63500/101301: 2.081\n",
            "Time remaining for current epoch: 0:21:47.381617\n",
            "Training loss after mini-batch 64000/101301: 2.145\n",
            "Time remaining for current epoch: 0:24:37.772521\n",
            "Training loss after mini-batch 64500/101301: 1.678\n",
            "Time remaining for current epoch: 0:22:32.377631\n",
            "Training loss after mini-batch 65000/101301: 2.003\n",
            "Time remaining for current epoch: 0:21:09.279374\n",
            "Training loss after mini-batch 65500/101301: 2.022\n",
            "Time remaining for current epoch: 0:20:05.691242\n",
            "Training loss after mini-batch 66000/101301: 2.088\n",
            "Time remaining for current epoch: 0:21:23.760162\n",
            "Training loss after mini-batch 66500/101301: 2.017\n",
            "Time remaining for current epoch: 0:21:44.072037\n",
            "Training loss after mini-batch 67000/101301: 1.939\n",
            "Time remaining for current epoch: 0:21:44.038281\n",
            "Training loss after mini-batch 67500/101301: 2.063\n",
            "Time remaining for current epoch: 0:19:45.813895\n",
            "Training loss after mini-batch 68000/101301: 1.705\n",
            "Time remaining for current epoch: 0:17:24.290703\n",
            "Training loss after mini-batch 68500/101301: 1.954\n",
            "Time remaining for current epoch: 0:18:21.278084\n",
            "Training loss after mini-batch 69000/101301: 2.027\n",
            "Time remaining for current epoch: 0:22:39.851827\n",
            "Training loss after mini-batch 69500/101301: 1.875\n",
            "Time remaining for current epoch: 0:21:57.063358\n",
            "Training loss after mini-batch 70000/101301: 2.001\n",
            "Time remaining for current epoch: 0:19:36.217831\n",
            "Training loss after mini-batch 70500/101301: 1.891\n",
            "Time remaining for current epoch: 0:17:50.181461\n",
            "Training loss after mini-batch 71000/101301: 1.913\n",
            "Time remaining for current epoch: 0:18:00.979512\n",
            "Training loss after mini-batch 71500/101301: 1.960\n",
            "Time remaining for current epoch: 0:17:45.384428\n",
            "Training loss after mini-batch 72000/101301: 1.891\n",
            "Time remaining for current epoch: 0:17:52.153829\n",
            "Training loss after mini-batch 72500/101301: 1.803\n",
            "Time remaining for current epoch: 0:17:16.915283\n",
            "Training loss after mini-batch 73000/101301: 1.731\n",
            "Time remaining for current epoch: 0:16:11.044069\n",
            "Training loss after mini-batch 73500/101301: 1.622\n",
            "Time remaining for current epoch: 0:15:55.703798\n",
            "Training loss after mini-batch 74000/101301: 1.870\n",
            "Time remaining for current epoch: 0:17:57.038075\n",
            "Training loss after mini-batch 74500/101301: 1.890\n",
            "Time remaining for current epoch: 0:16:38.351717\n",
            "Training loss after mini-batch 75000/101301: 1.971\n",
            "Time remaining for current epoch: 0:17:01.501259\n",
            "Training loss after mini-batch 75500/101301: 1.893\n",
            "Time remaining for current epoch: 0:14:57.574369\n",
            "Training loss after mini-batch 76000/101301: 1.858\n",
            "Time remaining for current epoch: 0:16:08.574428\n",
            "Training loss after mini-batch 76500/101301: 1.875\n",
            "Time remaining for current epoch: 0:14:26.320603\n",
            "Training loss after mini-batch 77000/101301: 1.740\n",
            "Time remaining for current epoch: 0:14:07.613089\n",
            "Training loss after mini-batch 77500/101301: 1.883\n",
            "Time remaining for current epoch: 0:13:27.539825\n",
            "Training loss after mini-batch 78000/101301: 1.973\n",
            "Time remaining for current epoch: 0:14:15.851775\n",
            "Training loss after mini-batch 78500/101301: 1.923\n",
            "Time remaining for current epoch: 0:13:19.176794\n",
            "Training loss after mini-batch 79000/101301: 1.760\n",
            "Time remaining for current epoch: 0:13:40.829518\n",
            "Training loss after mini-batch 79500/101301: 2.250\n",
            "Time remaining for current epoch: 0:13:51.017733\n",
            "Training loss after mini-batch 80000/101301: 1.796\n",
            "Time remaining for current epoch: 0:13:24.488685\n",
            "Training loss after mini-batch 80500/101301: 1.795\n",
            "Time remaining for current epoch: 0:12:25.144036\n",
            "Training loss after mini-batch 81000/101301: 1.882\n",
            "Time remaining for current epoch: 0:12:15.495327\n",
            "Training loss after mini-batch 81500/101301: 1.799\n",
            "Time remaining for current epoch: 0:11:43.369098\n",
            "Training loss after mini-batch 82000/101301: 1.722\n",
            "Time remaining for current epoch: 0:11:47.909469\n",
            "Training loss after mini-batch 82500/101301: 1.702\n",
            "Time remaining for current epoch: 0:11:52.310987\n",
            "Training loss after mini-batch 83000/101301: 1.726\n",
            "Time remaining for current epoch: 0:10:41.091102\n",
            "Training loss after mini-batch 83500/101301: 1.740\n",
            "Time remaining for current epoch: 0:11:40.569076\n",
            "Training loss after mini-batch 84000/101301: 1.743\n",
            "Time remaining for current epoch: 0:11:40.480783\n",
            "Training loss after mini-batch 84500/101301: 1.646\n",
            "Time remaining for current epoch: 0:10:56.299766\n",
            "Training loss after mini-batch 85000/101301: 1.796\n",
            "Time remaining for current epoch: 0:09:08.058110\n",
            "Training loss after mini-batch 85500/101301: 1.649\n",
            "Time remaining for current epoch: 0:09:00.983041\n",
            "Training loss after mini-batch 86000/101301: 1.762\n",
            "Time remaining for current epoch: 0:09:12.252455\n",
            "Training loss after mini-batch 86500/101301: 1.783\n",
            "Time remaining for current epoch: 0:09:44.006921\n",
            "Training loss after mini-batch 87000/101301: 1.868\n",
            "Time remaining for current epoch: 0:10:02.221331\n",
            "Training loss after mini-batch 87500/101301: 1.821\n",
            "Time remaining for current epoch: 0:08:41.673571\n",
            "Training loss after mini-batch 88000/101301: 1.598\n",
            "Time remaining for current epoch: 0:07:56.137532\n",
            "Training loss after mini-batch 88500/101301: 1.936\n",
            "Time remaining for current epoch: 0:07:45.418023\n",
            "Training loss after mini-batch 89000/101301: 1.739\n",
            "Time remaining for current epoch: 0:07:48.975414\n",
            "Training loss after mini-batch 89500/101301: 1.682\n",
            "Time remaining for current epoch: 0:07:08.123829\n",
            "Training loss after mini-batch 90000/101301: 1.477\n",
            "Time remaining for current epoch: 0:07:08.493063\n",
            "Training loss after mini-batch 90500/101301: 1.618\n",
            "Time remaining for current epoch: 0:06:26.562572\n",
            "Training loss after mini-batch 91000/101301: 1.534\n",
            "Time remaining for current epoch: 0:05:19.338780\n",
            "Training loss after mini-batch 91500/101301: 1.729\n",
            "Time remaining for current epoch: 0:06:08.917733\n",
            "Training loss after mini-batch 92000/101301: 1.994\n",
            "Time remaining for current epoch: 0:06:12.772659\n",
            "Training loss after mini-batch 92500/101301: 1.542\n",
            "Time remaining for current epoch: 0:05:17.731379\n",
            "Training loss after mini-batch 93000/101301: 1.703\n",
            "Time remaining for current epoch: 0:04:34.212839\n",
            "Training loss after mini-batch 93500/101301: 2.007\n",
            "Time remaining for current epoch: 0:04:48.857075\n",
            "Training loss after mini-batch 94000/101301: 1.913\n",
            "Time remaining for current epoch: 0:04:33.176377\n",
            "Training loss after mini-batch 94500/101301: 1.785\n",
            "Time remaining for current epoch: 0:04:06.957774\n",
            "Training loss after mini-batch 95000/101301: 1.624\n",
            "Time remaining for current epoch: 0:04:11.026379\n",
            "Training loss after mini-batch 95500/101301: 1.472\n",
            "Time remaining for current epoch: 0:03:30.769330\n",
            "Training loss after mini-batch 96000/101301: 1.763\n",
            "Time remaining for current epoch: 0:03:01.807193\n",
            "Training loss after mini-batch 96500/101301: 1.820\n",
            "Time remaining for current epoch: 0:02:42.166591\n",
            "Training loss after mini-batch 97000/101301: 1.740\n",
            "Time remaining for current epoch: 0:02:23.912276\n",
            "Training loss after mini-batch 97500/101301: 1.814\n",
            "Time remaining for current epoch: 0:02:08.123612\n",
            "Training loss after mini-batch 98000/101301: 1.723\n",
            "Time remaining for current epoch: 0:01:47.068368\n",
            "Training loss after mini-batch 98500/101301: 1.636\n",
            "Time remaining for current epoch: 0:01:42.816640\n",
            "Training loss after mini-batch 99000/101301: 1.863\n",
            "Time remaining for current epoch: 0:01:19.936911\n",
            "Training loss after mini-batch 99500/101301: 1.694\n",
            "Time remaining for current epoch: 0:01:04.477757\n",
            "Training loss after mini-batch 100000/101301: 1.727\n",
            "Time remaining for current epoch: 0:00:45.022055\n",
            "Training loss after mini-batch 100500/101301: 1.653\n",
            "Time remaining for current epoch: 0:00:26.098311\n",
            "Training loss after mini-batch 101000/101301: 1.833\n",
            "Time remaining for current epoch: 0:00:10.655875\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 2 complete. Training loss: 1.55302. Validation loss: 0.00003. Validation F1 score: 39.74006\n",
            "F1 score increased (39.189602--->39.740061) \t Saving The Model...\n",
            "Starting epoch 3\n",
            "Training loss after mini-batch   500/101301: 1.448\n",
            "Time remaining for current epoch: 0:27:02.675805\n",
            "Training loss after mini-batch  1000/101301: 1.606\n",
            "Time remaining for current epoch: 0:25:12.757021\n",
            "Training loss after mini-batch  1500/101301: 1.708\n",
            "Time remaining for current epoch: 0:30:09.567601\n",
            "Training loss after mini-batch  2000/101301: 1.529\n",
            "Time remaining for current epoch: 0:26:53.080574\n",
            "Training loss after mini-batch  2500/101301: 1.358\n",
            "Time remaining for current epoch: 0:24:37.511188\n",
            "Training loss after mini-batch  3000/101301: 1.325\n",
            "Time remaining for current epoch: 0:23:38.835434\n",
            "Training loss after mini-batch  3500/101301: 1.186\n",
            "Time remaining for current epoch: 0:21:36.208910\n",
            "Training loss after mini-batch  4000/101301: 1.629\n",
            "Time remaining for current epoch: 0:23:15.899605\n",
            "Training loss after mini-batch  4500/101301: 1.399\n",
            "Time remaining for current epoch: 0:23:45.434966\n",
            "Training loss after mini-batch  5000/101301: 1.373\n",
            "Time remaining for current epoch: 0:23:06.102368\n",
            "Training loss after mini-batch  5500/101301: 1.408\n",
            "Time remaining for current epoch: 0:24:01.521889\n",
            "Training loss after mini-batch  6000/101301: 1.402\n",
            "Time remaining for current epoch: 0:23:22.067029\n",
            "Training loss after mini-batch  6500/101301: 1.372\n",
            "Time remaining for current epoch: 0:23:53.028749\n",
            "Training loss after mini-batch  7000/101301: 1.458\n",
            "Time remaining for current epoch: 0:24:14.009686\n",
            "Training loss after mini-batch  7500/101301: 1.478\n",
            "Time remaining for current epoch: 0:29:56.675294\n",
            "Training loss after mini-batch  8000/101301: 1.425\n",
            "Time remaining for current epoch: 0:26:49.911791\n",
            "Training loss after mini-batch  8500/101301: 1.269\n",
            "Time remaining for current epoch: 0:24:55.863652\n",
            "Training loss after mini-batch  9000/101301: 1.055\n",
            "Time remaining for current epoch: 0:21:38.920456\n",
            "Training loss after mini-batch  9500/101301: 1.027\n",
            "Time remaining for current epoch: 0:20:58.961211\n",
            "Training loss after mini-batch 10000/101301: 1.481\n",
            "Time remaining for current epoch: 0:22:48.155989\n",
            "Training loss after mini-batch 10500/101301: 1.813\n",
            "Time remaining for current epoch: 0:26:09.330785\n",
            "Training loss after mini-batch 11000/101301: 1.573\n",
            "Time remaining for current epoch: 0:20:23.447453\n",
            "Training loss after mini-batch 11500/101301: 1.200\n",
            "Time remaining for current epoch: 0:19:58.286461\n",
            "Training loss after mini-batch 12000/101301: 1.245\n",
            "Time remaining for current epoch: 0:21:50.156177\n",
            "Training loss after mini-batch 12500/101301: 1.289\n",
            "Time remaining for current epoch: 0:22:03.498005\n",
            "Training loss after mini-batch 13000/101301: 0.953\n",
            "Time remaining for current epoch: 0:19:55.625684\n",
            "Training loss after mini-batch 13500/101301: 1.121\n",
            "Time remaining for current epoch: 0:20:37.492067\n",
            "Training loss after mini-batch 14000/101301: 1.048\n",
            "Time remaining for current epoch: 0:19:30.777831\n",
            "Training loss after mini-batch 14500/101301: 1.247\n",
            "Time remaining for current epoch: 0:19:58.095410\n",
            "Training loss after mini-batch 15000/101301: 1.241\n",
            "Time remaining for current epoch: 0:22:54.097232\n",
            "Training loss after mini-batch 15500/101301: 1.284\n",
            "Time remaining for current epoch: 0:24:29.928819\n",
            "Training loss after mini-batch 16000/101301: 1.247\n",
            "Time remaining for current epoch: 0:26:35.289511\n",
            "Training loss after mini-batch 16500/101301: 1.485\n",
            "Time remaining for current epoch: 0:26:50.075381\n",
            "Training loss after mini-batch 17000/101301: 1.442\n",
            "Time remaining for current epoch: 0:24:25.980309\n",
            "Training loss after mini-batch 17500/101301: 1.307\n",
            "Time remaining for current epoch: 0:24:56.278365\n",
            "Training loss after mini-batch 18000/101301: 1.021\n",
            "Time remaining for current epoch: 0:19:26.433101\n",
            "Training loss after mini-batch 18500/101301: 1.133\n",
            "Time remaining for current epoch: 0:18:26.901380\n",
            "Training loss after mini-batch 19000/101301: 1.508\n",
            "Time remaining for current epoch: 0:21:10.801616\n",
            "Training loss after mini-batch 19500/101301: 1.321\n",
            "Time remaining for current epoch: 0:21:35.406835\n",
            "Training loss after mini-batch 20000/101301: 1.275\n",
            "Time remaining for current epoch: 0:21:53.861248\n",
            "Training loss after mini-batch 20500/101301: 1.436\n",
            "Time remaining for current epoch: 0:24:40.890835\n",
            "Training loss after mini-batch 21000/101301: 1.424\n",
            "Time remaining for current epoch: 0:24:54.440983\n",
            "Training loss after mini-batch 21500/101301: 1.041\n",
            "Time remaining for current epoch: 0:23:47.888740\n",
            "Training loss after mini-batch 22000/101301: 1.023\n",
            "Time remaining for current epoch: 0:23:10.777430\n",
            "Training loss after mini-batch 22500/101301: 1.229\n",
            "Time remaining for current epoch: 0:24:04.487404\n",
            "Training loss after mini-batch 23000/101301: 1.444\n",
            "Time remaining for current epoch: 0:19:27.954322\n",
            "Training loss after mini-batch 23500/101301: 1.284\n",
            "Time remaining for current epoch: 0:18:21.243641\n",
            "Training loss after mini-batch 24000/101301: 1.207\n",
            "Time remaining for current epoch: 0:17:20.712822\n",
            "Training loss after mini-batch 24500/101301: 0.938\n",
            "Time remaining for current epoch: 0:18:02.678611\n",
            "Training loss after mini-batch 25000/101301: 1.358\n",
            "Time remaining for current epoch: 0:19:49.818064\n",
            "Training loss after mini-batch 25500/101301: 1.319\n",
            "Time remaining for current epoch: 0:18:45.522890\n",
            "Training loss after mini-batch 26000/101301: 1.196\n",
            "Time remaining for current epoch: 0:16:16.547851\n",
            "Training loss after mini-batch 26500/101301: 1.213\n",
            "Time remaining for current epoch: 0:17:03.467670\n",
            "Training loss after mini-batch 27000/101301: 1.491\n",
            "Time remaining for current epoch: 0:19:16.627859\n",
            "Training loss after mini-batch 27500/101301: 1.143\n",
            "Time remaining for current epoch: 0:17:26.688410\n",
            "Training loss after mini-batch 28000/101301: 1.047\n",
            "Time remaining for current epoch: 0:16:56.942253\n",
            "Training loss after mini-batch 28500/101301: 1.406\n",
            "Time remaining for current epoch: 0:20:18.809529\n",
            "Training loss after mini-batch 29000/101301: 1.538\n",
            "Time remaining for current epoch: 0:17:54.339467\n",
            "Training loss after mini-batch 29500/101301: 0.943\n",
            "Time remaining for current epoch: 0:15:31.911451\n",
            "Training loss after mini-batch 30000/101301: 0.865\n",
            "Time remaining for current epoch: 0:14:23.293012\n",
            "Training loss after mini-batch 30500/101301: 1.261\n",
            "Time remaining for current epoch: 0:19:13.911211\n",
            "Training loss after mini-batch 31000/101301: 1.243\n",
            "Time remaining for current epoch: 0:18:46.840167\n",
            "Training loss after mini-batch 31500/101301: 1.005\n",
            "Time remaining for current epoch: 0:14:49.763081\n",
            "Training loss after mini-batch 32000/101301: 0.802\n",
            "Time remaining for current epoch: 0:14:25.092631\n",
            "Training loss after mini-batch 32500/101301: 1.244\n",
            "Time remaining for current epoch: 0:14:44.623888\n",
            "Training loss after mini-batch 33000/101301: 1.133\n",
            "Time remaining for current epoch: 0:15:09.137048\n",
            "Training loss after mini-batch 33500/101301: 1.190\n",
            "Time remaining for current epoch: 0:16:10.270553\n",
            "Training loss after mini-batch 34000/101301: 1.269\n",
            "Time remaining for current epoch: 0:18:54.925608\n",
            "Training loss after mini-batch 34500/101301: 1.331\n",
            "Time remaining for current epoch: 0:18:47.222854\n",
            "Training loss after mini-batch 35000/101301: 1.201\n",
            "Time remaining for current epoch: 0:16:23.081578\n",
            "Training loss after mini-batch 35500/101301: 1.080\n",
            "Time remaining for current epoch: 0:14:07.759019\n",
            "Training loss after mini-batch 36000/101301: 1.308\n",
            "Time remaining for current epoch: 0:18:40.384021\n",
            "Training loss after mini-batch 36500/101301: 1.434\n",
            "Time remaining for current epoch: 0:18:05.919145\n",
            "Training loss after mini-batch 37000/101301: 1.530\n",
            "Time remaining for current epoch: 0:18:13.973395\n",
            "Training loss after mini-batch 37500/101301: 1.466\n",
            "Time remaining for current epoch: 0:18:00.410607\n",
            "Training loss after mini-batch 38000/101301: 1.353\n",
            "Time remaining for current epoch: 0:14:29.039604\n",
            "Training loss after mini-batch 38500/101301: 1.335\n",
            "Time remaining for current epoch: 0:15:35.366063\n",
            "Training loss after mini-batch 39000/101301: 1.438\n",
            "Time remaining for current epoch: 0:15:08.278703\n",
            "Training loss after mini-batch 39500/101301: 1.185\n",
            "Time remaining for current epoch: 0:13:57.152869\n",
            "Training loss after mini-batch 40000/101301: 1.036\n",
            "Time remaining for current epoch: 0:14:20.582087\n",
            "Training loss after mini-batch 40500/101301: 1.360\n",
            "Time remaining for current epoch: 0:17:01.802021\n",
            "Training loss after mini-batch 41000/101301: 1.247\n",
            "Time remaining for current epoch: 0:13:42.298432\n",
            "Training loss after mini-batch 41500/101301: 1.072\n",
            "Time remaining for current epoch: 0:13:28.354766\n",
            "Training loss after mini-batch 42000/101301: 1.065\n",
            "Time remaining for current epoch: 0:13:17.592896\n",
            "Training loss after mini-batch 42500/101301: 1.121\n",
            "Time remaining for current epoch: 0:13:18.220732\n",
            "Training loss after mini-batch 43000/101301: 1.167\n",
            "Time remaining for current epoch: 0:14:11.529947\n",
            "Training loss after mini-batch 43500/101301: 1.084\n",
            "Time remaining for current epoch: 0:15:52.194159\n",
            "Training loss after mini-batch 44000/101301: 1.318\n",
            "Time remaining for current epoch: 0:14:03.128840\n",
            "Training loss after mini-batch 44500/101301: 1.256\n",
            "Time remaining for current epoch: 0:16:26.742570\n",
            "Training loss after mini-batch 45000/101301: 1.325\n",
            "Time remaining for current epoch: 0:16:16.174782\n",
            "Training loss after mini-batch 45500/101301: 1.509\n",
            "Time remaining for current epoch: 0:13:59.398171\n",
            "Training loss after mini-batch 46000/101301: 1.202\n",
            "Time remaining for current epoch: 0:13:00.840700\n",
            "Training loss after mini-batch 46500/101301: 0.952\n",
            "Time remaining for current epoch: 0:12:26.914510\n",
            "Training loss after mini-batch 47000/101301: 1.205\n",
            "Time remaining for current epoch: 0:13:00.054826\n",
            "Training loss after mini-batch 47500/101301: 1.091\n",
            "Time remaining for current epoch: 0:12:10.575811\n",
            "Training loss after mini-batch 48000/101301: 1.384\n",
            "Time remaining for current epoch: 0:12:41.664070\n",
            "Training loss after mini-batch 48500/101301: 1.362\n",
            "Time remaining for current epoch: 0:13:05.566242\n",
            "Training loss after mini-batch 49000/101301: 0.942\n",
            "Time remaining for current epoch: 0:11:49.598836\n",
            "Training loss after mini-batch 49500/101301: 0.744\n",
            "Time remaining for current epoch: 0:11:17.415961\n",
            "Training loss after mini-batch 50000/101301: 0.728\n",
            "Time remaining for current epoch: 0:11:03.154575\n",
            "Training loss after mini-batch 50500/101301: 0.984\n",
            "Time remaining for current epoch: 0:11:57.241246\n",
            "Training loss after mini-batch 51000/101301: 1.316\n",
            "Time remaining for current epoch: 0:12:24.244170\n",
            "Training loss after mini-batch 51500/101301: 1.220\n",
            "Time remaining for current epoch: 0:11:23.391460\n",
            "Training loss after mini-batch 52000/101301: 1.045\n",
            "Time remaining for current epoch: 0:12:04.188813\n",
            "Training loss after mini-batch 52500/101301: 1.068\n",
            "Time remaining for current epoch: 0:11:22.402571\n",
            "Training loss after mini-batch 53000/101301: 1.190\n",
            "Time remaining for current epoch: 0:11:21.013049\n",
            "Training loss after mini-batch 53500/101301: 1.106\n",
            "Time remaining for current epoch: 0:11:36.322633\n",
            "Training loss after mini-batch 54000/101301: 0.995\n",
            "Time remaining for current epoch: 0:11:57.557662\n",
            "Training loss after mini-batch 54500/101301: 1.277\n",
            "Time remaining for current epoch: 0:12:13.176025\n",
            "Training loss after mini-batch 55000/101301: 1.349\n",
            "Time remaining for current epoch: 0:12:02.856863\n",
            "Training loss after mini-batch 55500/101301: 1.376\n",
            "Time remaining for current epoch: 0:12:32.646963\n",
            "Training loss after mini-batch 56000/101301: 1.296\n",
            "Time remaining for current epoch: 0:10:37.488353\n",
            "Training loss after mini-batch 56500/101301: 1.229\n",
            "Time remaining for current epoch: 0:10:23.475385\n",
            "Training loss after mini-batch 57000/101301: 1.398\n",
            "Time remaining for current epoch: 0:10:06.240340\n",
            "Training loss after mini-batch 57500/101301: 1.430\n",
            "Time remaining for current epoch: 0:09:33.816822\n",
            "Training loss after mini-batch 58000/101301: 1.425\n",
            "Time remaining for current epoch: 0:11:58.019857\n",
            "Training loss after mini-batch 58500/101301: 1.261\n",
            "Time remaining for current epoch: 0:11:36.314246\n",
            "Training loss after mini-batch 59000/101301: 1.194\n",
            "Time remaining for current epoch: 0:09:49.159149\n",
            "Training loss after mini-batch 59500/101301: 1.037\n",
            "Time remaining for current epoch: 0:09:45.910713\n",
            "Training loss after mini-batch 60000/101301: 1.022\n",
            "Time remaining for current epoch: 0:09:40.021620\n",
            "Training loss after mini-batch 60500/101301: 1.332\n",
            "Time remaining for current epoch: 0:10:39.859324\n",
            "Training loss after mini-batch 61000/101301: 1.287\n",
            "Time remaining for current epoch: 0:10:09.233392\n",
            "Training loss after mini-batch 61500/101301: 0.987\n",
            "Time remaining for current epoch: 0:09:01.870906\n",
            "Training loss after mini-batch 62000/101301: 1.200\n",
            "Time remaining for current epoch: 0:08:47.575810\n",
            "Training loss after mini-batch 62500/101301: 1.160\n",
            "Time remaining for current epoch: 0:13:16.741729\n",
            "Training loss after mini-batch 63000/101301: 1.192\n",
            "Time remaining for current epoch: 0:12:47.344092\n",
            "Training loss after mini-batch 63500/101301: 1.407\n",
            "Time remaining for current epoch: 0:09:12.900639\n",
            "Training loss after mini-batch 64000/101301: 1.225\n",
            "Time remaining for current epoch: 0:08:30.579955\n",
            "Training loss after mini-batch 64500/101301: 1.222\n",
            "Time remaining for current epoch: 0:09:06.705985\n",
            "Training loss after mini-batch 65000/101301: 0.998\n",
            "Time remaining for current epoch: 0:08:46.481479\n",
            "Training loss after mini-batch 65500/101301: 0.720\n",
            "Time remaining for current epoch: 0:08:29.682807\n",
            "Training loss after mini-batch 66000/101301: 1.291\n",
            "Time remaining for current epoch: 0:09:14.667084\n",
            "Training loss after mini-batch 66500/101301: 1.119\n",
            "Time remaining for current epoch: 0:09:13.703320\n",
            "Training loss after mini-batch 67000/101301: 0.960\n",
            "Time remaining for current epoch: 0:09:22.809139\n",
            "Training loss after mini-batch 67500/101301: 1.281\n",
            "Time remaining for current epoch: 0:11:10.127061\n",
            "Training loss after mini-batch 68000/101301: 1.374\n",
            "Time remaining for current epoch: 0:08:01.878944\n",
            "Training loss after mini-batch 68500/101301: 1.278\n",
            "Time remaining for current epoch: 0:08:19.355979\n",
            "Training loss after mini-batch 69000/101301: 1.172\n",
            "Time remaining for current epoch: 0:08:10.562011\n",
            "Training loss after mini-batch 69500/101301: 1.308\n",
            "Time remaining for current epoch: 0:08:06.543387\n",
            "Training loss after mini-batch 70000/101301: 1.241\n",
            "Time remaining for current epoch: 0:09:55.875978\n",
            "Training loss after mini-batch 70500/101301: 1.071\n",
            "Time remaining for current epoch: 0:09:33.372504\n",
            "Training loss after mini-batch 71000/101301: 0.934\n",
            "Time remaining for current epoch: 0:08:01.975758\n",
            "Training loss after mini-batch 71500/101301: 0.929\n",
            "Time remaining for current epoch: 0:06:57.974588\n",
            "Training loss after mini-batch 72000/101301: 1.151\n",
            "Time remaining for current epoch: 0:06:33.117414\n",
            "Training loss after mini-batch 72500/101301: 1.135\n",
            "Time remaining for current epoch: 0:06:15.141364\n",
            "Training loss after mini-batch 73000/101301: 0.964\n",
            "Time remaining for current epoch: 0:06:26.001710\n",
            "Training loss after mini-batch 73500/101301: 1.142\n",
            "Time remaining for current epoch: 0:07:05.106135\n",
            "Training loss after mini-batch 74000/101301: 1.259\n",
            "Time remaining for current epoch: 0:07:15.395644\n",
            "Training loss after mini-batch 74500/101301: 1.137\n",
            "Time remaining for current epoch: 0:07:07.412161\n",
            "Training loss after mini-batch 75000/101301: 1.416\n",
            "Time remaining for current epoch: 0:06:51.709390\n",
            "Training loss after mini-batch 75500/101301: 1.347\n",
            "Time remaining for current epoch: 0:06:45.393614\n",
            "Training loss after mini-batch 76000/101301: 1.090\n",
            "Time remaining for current epoch: 0:05:48.839429\n",
            "Training loss after mini-batch 76500/101301: 0.924\n",
            "Time remaining for current epoch: 0:05:35.335147\n",
            "Training loss after mini-batch 77000/101301: 1.203\n",
            "Time remaining for current epoch: 0:08:21.578164\n",
            "Training loss after mini-batch 77500/101301: 1.217\n",
            "Time remaining for current epoch: 0:07:45.434475\n",
            "Training loss after mini-batch 78000/101301: 1.150\n",
            "Time remaining for current epoch: 0:05:41.718375\n",
            "Training loss after mini-batch 78500/101301: 1.230\n",
            "Time remaining for current epoch: 0:05:23.528100\n",
            "Training loss after mini-batch 79000/101301: 1.311\n",
            "Time remaining for current epoch: 0:05:05.163439\n",
            "Training loss after mini-batch 79500/101301: 1.155\n",
            "Time remaining for current epoch: 0:04:56.315564\n",
            "Training loss after mini-batch 80000/101301: 1.051\n",
            "Time remaining for current epoch: 0:04:57.859669\n",
            "Training loss after mini-batch 80500/101301: 1.411\n",
            "Time remaining for current epoch: 0:05:30.471212\n",
            "Training loss after mini-batch 81000/101301: 1.095\n",
            "Time remaining for current epoch: 0:05:22.518794\n",
            "Training loss after mini-batch 81500/101301: 0.797\n",
            "Time remaining for current epoch: 0:04:31.405032\n",
            "Training loss after mini-batch 82000/101301: 0.978\n",
            "Time remaining for current epoch: 0:04:24.502745\n",
            "Training loss after mini-batch 82500/101301: 0.702\n",
            "Time remaining for current epoch: 0:03:59.979163\n",
            "Training loss after mini-batch 83000/101301: 0.760\n",
            "Time remaining for current epoch: 0:04:01.910680\n",
            "Training loss after mini-batch 83500/101301: 0.904\n",
            "Time remaining for current epoch: 0:04:19.752421\n",
            "Training loss after mini-batch 84000/101301: 0.909\n",
            "Time remaining for current epoch: 0:04:00.608952\n",
            "Training loss after mini-batch 84500/101301: 0.840\n",
            "Time remaining for current epoch: 0:03:44.440637\n",
            "Training loss after mini-batch 85000/101301: 0.900\n",
            "Time remaining for current epoch: 0:03:53.846889\n",
            "Training loss after mini-batch 85500/101301: 1.152\n",
            "Time remaining for current epoch: 0:04:23.309944\n",
            "Training loss after mini-batch 86000/101301: 1.294\n",
            "Time remaining for current epoch: 0:04:54.349860\n",
            "Training loss after mini-batch 86500/101301: 1.171\n",
            "Time remaining for current epoch: 0:04:53.079275\n",
            "Training loss after mini-batch 87000/101301: 1.153\n",
            "Time remaining for current epoch: 0:03:32.725998\n",
            "Training loss after mini-batch 87500/101301: 1.210\n",
            "Time remaining for current epoch: 0:03:48.725769\n",
            "Training loss after mini-batch 88000/101301: 1.143\n",
            "Time remaining for current epoch: 0:03:31.893966\n",
            "Training loss after mini-batch 88500/101301: 1.037\n",
            "Time remaining for current epoch: 0:03:21.625590\n",
            "Training loss after mini-batch 89000/101301: 1.011\n",
            "Time remaining for current epoch: 0:02:49.511853\n",
            "Training loss after mini-batch 89500/101301: 1.124\n",
            "Time remaining for current epoch: 0:02:55.939966\n",
            "Training loss after mini-batch 90000/101301: 1.205\n",
            "Time remaining for current epoch: 0:02:49.591148\n",
            "Training loss after mini-batch 90500/101301: 0.957\n",
            "Time remaining for current epoch: 0:02:26.607628\n",
            "Training loss after mini-batch 91000/101301: 0.914\n",
            "Time remaining for current epoch: 0:02:33.344298\n",
            "Training loss after mini-batch 91500/101301: 1.091\n",
            "Time remaining for current epoch: 0:02:30.893816\n",
            "Training loss after mini-batch 92000/101301: 1.136\n",
            "Time remaining for current epoch: 0:02:38.799184\n",
            "Training loss after mini-batch 92500/101301: 0.891\n",
            "Time remaining for current epoch: 0:01:57.808752\n",
            "Training loss after mini-batch 93000/101301: 0.788\n",
            "Time remaining for current epoch: 0:01:50.977042\n",
            "Training loss after mini-batch 93500/101301: 0.764\n",
            "Time remaining for current epoch: 0:01:45.515813\n",
            "Training loss after mini-batch 94000/101301: 0.708\n",
            "Time remaining for current epoch: 0:01:35.470548\n",
            "Training loss after mini-batch 94500/101301: 1.013\n",
            "Time remaining for current epoch: 0:01:41.939238\n",
            "Training loss after mini-batch 95000/101301: 0.809\n",
            "Time remaining for current epoch: 0:01:32.590822\n",
            "Training loss after mini-batch 95500/101301: 0.749\n",
            "Time remaining for current epoch: 0:01:17.774668\n",
            "Training loss after mini-batch 96000/101301: 0.823\n",
            "Time remaining for current epoch: 0:01:30.051209\n",
            "Training loss after mini-batch 96500/101301: 1.017\n",
            "Time remaining for current epoch: 0:01:36.339757\n",
            "Training loss after mini-batch 97000/101301: 1.146\n",
            "Time remaining for current epoch: 0:01:05.478894\n",
            "Training loss after mini-batch 97500/101301: 1.195\n",
            "Time remaining for current epoch: 0:00:59.830701\n",
            "Training loss after mini-batch 98000/101301: 1.107\n",
            "Time remaining for current epoch: 0:00:51.883739\n",
            "Training loss after mini-batch 98500/101301: 1.057\n",
            "Time remaining for current epoch: 0:00:47.282261\n",
            "Training loss after mini-batch 99000/101301: 1.044\n",
            "Time remaining for current epoch: 0:00:33.618108\n",
            "Training loss after mini-batch 99500/101301: 1.032\n",
            "Time remaining for current epoch: 0:00:24.342457\n",
            "Training loss after mini-batch 100000/101301: 0.982\n",
            "Time remaining for current epoch: 0:00:18.841233\n",
            "Training loss after mini-batch 100500/101301: 0.715\n",
            "Time remaining for current epoch: 0:00:11.389597\n",
            "Training loss after mini-batch 101000/101301: 0.620\n",
            "Time remaining for current epoch: 0:00:04.199877\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 3 complete. Training loss: 1.17914. Validation loss: 0.00003. Validation F1 score: 44.61118\n",
            "F1 score increased (39.740061--->44.611184) \t Saving The Model...\n",
            "Starting epoch 4\n",
            "Training loss after mini-batch   500/101301: 1.215\n",
            "Time remaining for current epoch: 0:27:53.648864\n",
            "Training loss after mini-batch  1000/101301: 0.923\n",
            "Time remaining for current epoch: 0:24:42.790048\n",
            "Training loss after mini-batch  1500/101301: 0.865\n",
            "Time remaining for current epoch: 0:23:58.612518\n",
            "Training loss after mini-batch  2000/101301: 1.110\n",
            "Time remaining for current epoch: 0:30:09.113246\n",
            "Training loss after mini-batch  2500/101301: 1.310\n",
            "Time remaining for current epoch: 0:24:36.766348\n",
            "Training loss after mini-batch  3000/101301: 1.138\n",
            "Time remaining for current epoch: 0:25:29.502009\n",
            "Training loss after mini-batch  3500/101301: 1.084\n",
            "Time remaining for current epoch: 0:22:12.802300\n",
            "Training loss after mini-batch  4000/101301: 1.058\n",
            "Time remaining for current epoch: 0:21:32.741574\n",
            "Training loss after mini-batch  4500/101301: 1.167\n",
            "Time remaining for current epoch: 0:26:19.245730\n",
            "Training loss after mini-batch  5000/101301: 1.042\n",
            "Time remaining for current epoch: 0:28:25.223552\n",
            "Training loss after mini-batch  5500/101301: 1.500\n",
            "Time remaining for current epoch: 0:27:04.197683\n",
            "Training loss after mini-batch  6000/101301: 1.213\n",
            "Time remaining for current epoch: 0:29:45.147237\n",
            "Training loss after mini-batch  6500/101301: 1.213\n",
            "Time remaining for current epoch: 0:28:11.770476\n",
            "Training loss after mini-batch  7000/101301: 1.010\n",
            "Time remaining for current epoch: 0:27:29.486257\n",
            "Training loss after mini-batch  7500/101301: 1.185\n",
            "Time remaining for current epoch: 0:25:23.535219\n",
            "Training loss after mini-batch  8000/101301: 0.996\n",
            "Time remaining for current epoch: 0:21:41.269895\n",
            "Training loss after mini-batch  8500/101301: 0.921\n",
            "Time remaining for current epoch: 0:23:43.171264\n",
            "Training loss after mini-batch  9000/101301: 1.080\n",
            "Time remaining for current epoch: 0:24:57.804483\n",
            "Training loss after mini-batch  9500/101301: 1.345\n",
            "Time remaining for current epoch: 0:23:04.593177\n",
            "Training loss after mini-batch 10000/101301: 1.386\n",
            "Time remaining for current epoch: 0:24:12.497153\n",
            "Training loss after mini-batch 10500/101301: 1.465\n",
            "Time remaining for current epoch: 0:22:35.119733\n",
            "Training loss after mini-batch 11000/101301: 1.236\n",
            "Time remaining for current epoch: 0:21:35.838244\n",
            "Training loss after mini-batch 11500/101301: 1.427\n",
            "Time remaining for current epoch: 0:20:45.228812\n",
            "Training loss after mini-batch 12000/101301: 1.056\n",
            "Time remaining for current epoch: 0:19:48.549124\n",
            "Training loss after mini-batch 12500/101301: 1.161\n",
            "Time remaining for current epoch: 0:19:54.562836\n",
            "Training loss after mini-batch 13000/101301: 1.030\n",
            "Time remaining for current epoch: 0:19:56.206946\n",
            "Training loss after mini-batch 13500/101301: 0.878\n",
            "Time remaining for current epoch: 0:20:06.352771\n",
            "Training loss after mini-batch 14000/101301: 0.952\n",
            "Time remaining for current epoch: 0:23:56.996693\n",
            "Training loss after mini-batch 14500/101301: 1.074\n",
            "Time remaining for current epoch: 0:25:22.402949\n",
            "Training loss after mini-batch 15000/101301: 1.016\n",
            "Time remaining for current epoch: 0:21:15.335592\n",
            "Training loss after mini-batch 15500/101301: 0.963\n",
            "Time remaining for current epoch: 0:20:12.986378\n",
            "Training loss after mini-batch 16000/101301: 1.119\n",
            "Time remaining for current epoch: 0:20:33.731370\n",
            "Training loss after mini-batch 16500/101301: 1.079\n",
            "Time remaining for current epoch: 0:19:51.937441\n",
            "Training loss after mini-batch 17000/101301: 0.886\n",
            "Time remaining for current epoch: 0:23:41.711865\n",
            "Training loss after mini-batch 17500/101301: 1.176\n",
            "Time remaining for current epoch: 0:21:02.865021\n",
            "Training loss after mini-batch 18000/101301: 1.035\n",
            "Time remaining for current epoch: 0:21:10.487257\n",
            "Training loss after mini-batch 18500/101301: 1.125\n",
            "Time remaining for current epoch: 0:20:17.079334\n",
            "Training loss after mini-batch 19000/101301: 1.064\n",
            "Time remaining for current epoch: 0:20:23.543310\n",
            "Training loss after mini-batch 19500/101301: 0.714\n",
            "Time remaining for current epoch: 0:19:28.852776\n",
            "Training loss after mini-batch 20000/101301: 0.965\n",
            "Time remaining for current epoch: 0:19:35.203972\n",
            "Training loss after mini-batch 20500/101301: 1.042\n",
            "Time remaining for current epoch: 0:19:56.620308\n",
            "Training loss after mini-batch 21000/101301: 0.950\n",
            "Time remaining for current epoch: 0:17:12.267365\n",
            "Training loss after mini-batch 21500/101301: 1.052\n",
            "Time remaining for current epoch: 0:17:06.122833\n",
            "Training loss after mini-batch 22000/101301: 1.151\n",
            "Time remaining for current epoch: 0:17:59.187927\n",
            "Training loss after mini-batch 22500/101301: 1.052\n",
            "Time remaining for current epoch: 0:17:27.800133\n",
            "Training loss after mini-batch 23000/101301: 1.224\n",
            "Time remaining for current epoch: 0:20:18.095925\n",
            "Training loss after mini-batch 23500/101301: 1.133\n",
            "Time remaining for current epoch: 0:23:16.340922\n",
            "Training loss after mini-batch 24000/101301: 1.102\n",
            "Time remaining for current epoch: 0:23:17.825925\n",
            "Training loss after mini-batch 24500/101301: 1.125\n",
            "Time remaining for current epoch: 0:21:28.925071\n",
            "Training loss after mini-batch 25000/101301: 1.219\n",
            "Time remaining for current epoch: 0:19:23.965433\n",
            "Training loss after mini-batch 25500/101301: 1.129\n",
            "Time remaining for current epoch: 0:20:54.757481\n",
            "Training loss after mini-batch 26000/101301: 1.143\n",
            "Time remaining for current epoch: 0:19:14.441451\n",
            "Training loss after mini-batch 26500/101301: 1.110\n",
            "Time remaining for current epoch: 0:16:50.762016\n",
            "Training loss after mini-batch 27000/101301: 0.761\n",
            "Time remaining for current epoch: 0:16:52.982762\n",
            "Training loss after mini-batch 27500/101301: 0.866\n",
            "Time remaining for current epoch: 0:16:41.338502\n",
            "Training loss after mini-batch 28000/101301: 0.969\n",
            "Time remaining for current epoch: 0:17:54.378704\n",
            "Training loss after mini-batch 28500/101301: 0.933\n",
            "Time remaining for current epoch: 0:21:58.463195\n",
            "Training loss after mini-batch 29000/101301: 1.204\n",
            "Time remaining for current epoch: 0:16:33.629473\n",
            "Training loss after mini-batch 29500/101301: 1.022\n",
            "Time remaining for current epoch: 0:16:32.442291\n",
            "Training loss after mini-batch 30000/101301: 1.076\n",
            "Time remaining for current epoch: 0:16:35.010889\n",
            "Training loss after mini-batch 30500/101301: 0.936\n",
            "Time remaining for current epoch: 0:16:35.941793\n",
            "Training loss after mini-batch 31000/101301: 1.065\n",
            "Time remaining for current epoch: 0:18:02.045752\n",
            "Training loss after mini-batch 31500/101301: 1.040\n",
            "Time remaining for current epoch: 0:19:15.302287\n",
            "Training loss after mini-batch 32000/101301: 1.050\n",
            "Time remaining for current epoch: 0:19:57.394434\n",
            "Training loss after mini-batch 32500/101301: 0.919\n",
            "Time remaining for current epoch: 0:15:10.554539\n",
            "Training loss after mini-batch 33000/101301: 0.949\n",
            "Time remaining for current epoch: 0:15:35.036167\n",
            "Training loss after mini-batch 33500/101301: 1.061\n",
            "Time remaining for current epoch: 0:14:38.520884\n",
            "Training loss after mini-batch 34000/101301: 0.929\n",
            "Time remaining for current epoch: 0:15:18.502980\n",
            "Training loss after mini-batch 34500/101301: 1.087\n",
            "Time remaining for current epoch: 0:15:53.364558\n",
            "Training loss after mini-batch 35000/101301: 0.926\n",
            "Time remaining for current epoch: 0:15:45.076818\n",
            "Training loss after mini-batch 35500/101301: 1.041\n",
            "Time remaining for current epoch: 0:17:25.648635\n",
            "Training loss after mini-batch 36000/101301: 1.043\n",
            "Time remaining for current epoch: 0:17:12.931612\n",
            "Training loss after mini-batch 36500/101301: 1.254\n",
            "Time remaining for current epoch: 0:14:42.608461\n",
            "Training loss after mini-batch 37000/101301: 1.100\n",
            "Time remaining for current epoch: 0:15:47.121997\n",
            "Training loss after mini-batch 37500/101301: 0.979\n",
            "Time remaining for current epoch: 0:14:39.251538\n",
            "Training loss after mini-batch 38000/101301: 1.058\n",
            "Time remaining for current epoch: 0:16:06.271072\n",
            "Training loss after mini-batch 38500/101301: 1.040\n",
            "Time remaining for current epoch: 0:14:55.177684\n",
            "Training loss after mini-batch 39000/101301: 1.084\n",
            "Time remaining for current epoch: 0:15:34.010895\n",
            "Training loss after mini-batch 39500/101301: 1.021\n",
            "Time remaining for current epoch: 0:13:56.759723\n",
            "Training loss after mini-batch 40000/101301: 0.968\n",
            "Time remaining for current epoch: 0:13:46.193257\n",
            "Training loss after mini-batch 40500/101301: 0.900\n",
            "Time remaining for current epoch: 0:12:44.428878\n",
            "Training loss after mini-batch 41000/101301: 0.869\n",
            "Time remaining for current epoch: 0:12:57.522506\n",
            "Training loss after mini-batch 41500/101301: 1.109\n",
            "Time remaining for current epoch: 0:14:25.044286\n",
            "Training loss after mini-batch 42000/101301: 1.061\n",
            "Time remaining for current epoch: 0:15:23.394257\n",
            "Training loss after mini-batch 42500/101301: 0.975\n",
            "Time remaining for current epoch: 0:18:07.024085\n",
            "Training loss after mini-batch 43000/101301: 1.133\n",
            "Time remaining for current epoch: 0:16:14.945219\n",
            "Training loss after mini-batch 43500/101301: 1.039\n",
            "Time remaining for current epoch: 0:14:03.499388\n",
            "Training loss after mini-batch 44000/101301: 1.015\n",
            "Time remaining for current epoch: 0:14:33.735356\n",
            "Training loss after mini-batch 44500/101301: 1.059\n",
            "Time remaining for current epoch: 0:14:44.607906\n",
            "Training loss after mini-batch 45000/101301: 1.005\n",
            "Time remaining for current epoch: 0:13:26.703417\n",
            "Training loss after mini-batch 45500/101301: 1.091\n",
            "Time remaining for current epoch: 0:12:44.129861\n",
            "Training loss after mini-batch 46000/101301: 1.122\n",
            "Time remaining for current epoch: 0:12:13.521083\n",
            "Training loss after mini-batch 46500/101301: 1.051\n",
            "Time remaining for current epoch: 0:12:43.076968\n",
            "Training loss after mini-batch 47000/101301: 0.959\n",
            "Time remaining for current epoch: 0:12:17.973462\n",
            "Training loss after mini-batch 47500/101301: 0.767\n",
            "Time remaining for current epoch: 0:12:40.802880\n",
            "Training loss after mini-batch 48000/101301: 0.855\n",
            "Time remaining for current epoch: 0:12:20.186264\n",
            "Training loss after mini-batch 48500/101301: 0.772\n",
            "Time remaining for current epoch: 0:11:27.626580\n",
            "Training loss after mini-batch 49000/101301: 0.895\n",
            "Time remaining for current epoch: 0:12:46.342979\n",
            "Training loss after mini-batch 49500/101301: 1.027\n",
            "Time remaining for current epoch: 0:12:40.702548\n",
            "Training loss after mini-batch 50000/101301: 1.208\n",
            "Time remaining for current epoch: 0:12:11.871235\n",
            "Training loss after mini-batch 50500/101301: 1.221\n",
            "Time remaining for current epoch: 0:12:39.258404\n",
            "Training loss after mini-batch 51000/101301: 1.082\n",
            "Time remaining for current epoch: 0:12:46.277817\n",
            "Training loss after mini-batch 51500/101301: 0.638\n",
            "Time remaining for current epoch: 0:12:11.370712\n",
            "Training loss after mini-batch 52000/101301: 0.774\n",
            "Time remaining for current epoch: 0:11:15.834244\n",
            "Training loss after mini-batch 52500/101301: 1.047\n",
            "Time remaining for current epoch: 0:13:10.314029\n",
            "Training loss after mini-batch 53000/101301: 1.025\n",
            "Time remaining for current epoch: 0:13:45.806970\n",
            "Training loss after mini-batch 53500/101301: 1.120\n",
            "Time remaining for current epoch: 0:15:59.393362\n",
            "Training loss after mini-batch 54000/101301: 1.138\n",
            "Time remaining for current epoch: 0:12:27.767210\n",
            "Training loss after mini-batch 54500/101301: 1.247\n",
            "Time remaining for current epoch: 0:12:14.670380\n",
            "Training loss after mini-batch 55000/101301: 1.166\n",
            "Time remaining for current epoch: 0:13:07.099073\n",
            "Training loss after mini-batch 55500/101301: 1.190\n",
            "Time remaining for current epoch: 0:11:11.621956\n",
            "Training loss after mini-batch 56000/101301: 1.097\n",
            "Time remaining for current epoch: 0:11:43.095324\n",
            "Training loss after mini-batch 56500/101301: 1.150\n",
            "Time remaining for current epoch: 0:11:02.765065\n",
            "Training loss after mini-batch 57000/101301: 1.412\n",
            "Time remaining for current epoch: 0:10:09.043416\n",
            "Training loss after mini-batch 57500/101301: 1.085\n",
            "Time remaining for current epoch: 0:12:41.502409\n",
            "Training loss after mini-batch 58000/101301: 1.346\n",
            "Time remaining for current epoch: 0:12:19.712705\n",
            "Training loss after mini-batch 58500/101301: 1.229\n",
            "Time remaining for current epoch: 0:12:02.785287\n",
            "Training loss after mini-batch 59000/101301: 0.703\n",
            "Time remaining for current epoch: 0:10:00.869787\n",
            "Training loss after mini-batch 59500/101301: 0.641\n",
            "Time remaining for current epoch: 0:09:32.362091\n",
            "Training loss after mini-batch 60000/101301: 0.953\n",
            "Time remaining for current epoch: 0:09:38.306305\n",
            "Training loss after mini-batch 60500/101301: 1.080\n",
            "Time remaining for current epoch: 0:10:16.070057\n",
            "Training loss after mini-batch 61000/101301: 0.989\n",
            "Time remaining for current epoch: 0:10:24.382895\n",
            "Training loss after mini-batch 61500/101301: 0.942\n",
            "Time remaining for current epoch: 0:12:00.653833\n",
            "Training loss after mini-batch 62000/101301: 1.050\n",
            "Time remaining for current epoch: 0:08:51.030037\n",
            "Training loss after mini-batch 62500/101301: 1.167\n",
            "Time remaining for current epoch: 0:09:46.283966\n",
            "Training loss after mini-batch 63000/101301: 1.175\n",
            "Time remaining for current epoch: 0:10:13.969184\n",
            "Training loss after mini-batch 63500/101301: 1.558\n",
            "Time remaining for current epoch: 0:21:59.497177\n",
            "Training loss after mini-batch 64000/101301: 1.710\n",
            "Time remaining for current epoch: 0:24:47.827231\n",
            "Training loss after mini-batch 64500/101301: 1.335\n",
            "Time remaining for current epoch: 0:22:51.458996\n",
            "Training loss after mini-batch 65000/101301: 1.513\n",
            "Time remaining for current epoch: 0:21:22.964413\n",
            "Training loss after mini-batch 65500/101301: 1.576\n",
            "Time remaining for current epoch: 0:20:21.049919\n",
            "Training loss after mini-batch 66000/101301: 1.527\n",
            "Time remaining for current epoch: 0:21:50.955431\n",
            "Training loss after mini-batch 66500/101301: 1.540\n",
            "Time remaining for current epoch: 0:22:07.297407\n",
            "Training loss after mini-batch 67000/101301: 1.509\n",
            "Time remaining for current epoch: 0:22:05.167769\n",
            "Training loss after mini-batch 67500/101301: 1.565\n",
            "Time remaining for current epoch: 0:20:02.813534\n",
            "Training loss after mini-batch 68000/101301: 1.335\n",
            "Time remaining for current epoch: 0:17:28.928956\n",
            "Training loss after mini-batch 68500/101301: 1.503\n",
            "Time remaining for current epoch: 0:19:15.145404\n",
            "Training loss after mini-batch 69000/101301: 1.655\n",
            "Time remaining for current epoch: 0:22:37.641503\n",
            "Training loss after mini-batch 69500/101301: 1.539\n",
            "Time remaining for current epoch: 0:22:19.492356\n",
            "Training loss after mini-batch 70000/101301: 1.539\n",
            "Time remaining for current epoch: 0:19:52.091377\n",
            "Training loss after mini-batch 70500/101301: 1.467\n",
            "Time remaining for current epoch: 0:17:53.809165\n",
            "Training loss after mini-batch 71000/101301: 1.504\n",
            "Time remaining for current epoch: 0:18:16.531510\n",
            "Training loss after mini-batch 71500/101301: 1.521\n",
            "Time remaining for current epoch: 0:18:02.120193\n",
            "Training loss after mini-batch 72000/101301: 1.473\n",
            "Time remaining for current epoch: 0:17:58.729869\n",
            "Training loss after mini-batch 72500/101301: 1.387\n",
            "Time remaining for current epoch: 0:17:25.309088\n",
            "Training loss after mini-batch 73000/101301: 1.296\n",
            "Time remaining for current epoch: 0:16:18.821664\n",
            "Training loss after mini-batch 73500/101301: 1.319\n",
            "Time remaining for current epoch: 0:16:03.007587\n",
            "Training loss after mini-batch 74000/101301: 1.503\n",
            "Time remaining for current epoch: 0:17:47.027705\n",
            "Training loss after mini-batch 74500/101301: 1.546\n",
            "Time remaining for current epoch: 0:16:41.569763\n",
            "Training loss after mini-batch 75000/101301: 1.635\n",
            "Time remaining for current epoch: 0:17:10.770918\n",
            "Training loss after mini-batch 75500/101301: 1.411\n",
            "Time remaining for current epoch: 0:14:59.676340\n",
            "Training loss after mini-batch 76000/101301: 1.458\n",
            "Time remaining for current epoch: 0:16:21.426501\n",
            "Training loss after mini-batch 76500/101301: 1.538\n",
            "Time remaining for current epoch: 0:14:49.195922\n",
            "Training loss after mini-batch 77000/101301: 1.333\n",
            "Time remaining for current epoch: 0:14:13.696775\n",
            "Training loss after mini-batch 77500/101301: 1.432\n",
            "Time remaining for current epoch: 0:13:40.089409\n",
            "Training loss after mini-batch 78000/101301: 1.508\n",
            "Time remaining for current epoch: 0:14:25.892456\n",
            "Training loss after mini-batch 78500/101301: 1.501\n",
            "Time remaining for current epoch: 0:13:21.132591\n",
            "Training loss after mini-batch 79000/101301: 1.360\n",
            "Time remaining for current epoch: 0:13:37.019450\n",
            "Training loss after mini-batch 79500/101301: 1.834\n",
            "Time remaining for current epoch: 0:14:02.976675\n",
            "Training loss after mini-batch 80000/101301: 1.409\n",
            "Time remaining for current epoch: 0:13:40.334627\n",
            "Training loss after mini-batch 80500/101301: 1.380\n",
            "Time remaining for current epoch: 0:12:34.645347\n",
            "Training loss after mini-batch 81000/101301: 1.485\n",
            "Time remaining for current epoch: 0:12:35.735158\n",
            "Training loss after mini-batch 81500/101301: 1.374\n",
            "Time remaining for current epoch: 0:11:51.216260\n",
            "Training loss after mini-batch 82000/101301: 1.367\n",
            "Time remaining for current epoch: 0:11:56.767608\n",
            "Training loss after mini-batch 82500/101301: 1.310\n",
            "Time remaining for current epoch: 0:11:46.712075\n",
            "Training loss after mini-batch 83000/101301: 1.256\n",
            "Time remaining for current epoch: 0:10:49.040858\n",
            "Training loss after mini-batch 83500/101301: 1.390\n",
            "Time remaining for current epoch: 0:11:38.544722\n",
            "Training loss after mini-batch 84000/101301: 1.416\n",
            "Time remaining for current epoch: 0:11:36.763895\n",
            "Training loss after mini-batch 84500/101301: 1.305\n",
            "Time remaining for current epoch: 0:10:59.107228\n",
            "Training loss after mini-batch 85000/101301: 1.327\n",
            "Time remaining for current epoch: 0:09:07.543822\n",
            "Training loss after mini-batch 85500/101301: 1.256\n",
            "Time remaining for current epoch: 0:09:05.484319\n",
            "Training loss after mini-batch 86000/101301: 1.314\n",
            "Time remaining for current epoch: 0:09:27.369105\n",
            "Training loss after mini-batch 86500/101301: 1.352\n",
            "Time remaining for current epoch: 0:09:43.690935\n",
            "Training loss after mini-batch 87000/101301: 1.486\n",
            "Time remaining for current epoch: 0:09:59.101293\n",
            "Training loss after mini-batch 87500/101301: 1.352\n",
            "Time remaining for current epoch: 0:08:42.063308\n",
            "Training loss after mini-batch 88000/101301: 1.314\n",
            "Time remaining for current epoch: 0:08:00.045111\n",
            "Training loss after mini-batch 88500/101301: 1.431\n",
            "Time remaining for current epoch: 0:07:34.692551\n",
            "Training loss after mini-batch 89000/101301: 1.296\n",
            "Time remaining for current epoch: 0:07:49.953528\n",
            "Training loss after mini-batch 89500/101301: 1.258\n",
            "Time remaining for current epoch: 0:07:10.085652\n",
            "Training loss after mini-batch 90000/101301: 1.186\n",
            "Time remaining for current epoch: 0:07:08.778359\n",
            "Training loss after mini-batch 90500/101301: 1.234\n",
            "Time remaining for current epoch: 0:06:36.790275\n",
            "Training loss after mini-batch 91000/101301: 1.136\n",
            "Time remaining for current epoch: 0:05:22.042482\n",
            "Training loss after mini-batch 91500/101301: 1.243\n",
            "Time remaining for current epoch: 0:06:11.788386\n",
            "Training loss after mini-batch 92000/101301: 1.571\n",
            "Time remaining for current epoch: 0:06:21.897117\n",
            "Training loss after mini-batch 92500/101301: 1.223\n",
            "Time remaining for current epoch: 0:05:19.302418\n",
            "Training loss after mini-batch 93000/101301: 1.287\n",
            "Time remaining for current epoch: 0:04:32.233041\n",
            "Training loss after mini-batch 93500/101301: 1.527\n",
            "Time remaining for current epoch: 0:04:41.071594\n",
            "Training loss after mini-batch 94000/101301: 1.553\n",
            "Time remaining for current epoch: 0:04:34.366100\n",
            "Training loss after mini-batch 94500/101301: 1.410\n",
            "Time remaining for current epoch: 0:04:03.063620\n",
            "Training loss after mini-batch 95000/101301: 1.239\n",
            "Time remaining for current epoch: 0:04:09.972502\n",
            "Training loss after mini-batch 95500/101301: 1.171\n",
            "Time remaining for current epoch: 0:03:34.787115\n",
            "Training loss after mini-batch 96000/101301: 1.363\n",
            "Time remaining for current epoch: 0:03:01.308139\n",
            "Training loss after mini-batch 96500/101301: 1.390\n",
            "Time remaining for current epoch: 0:02:42.126446\n",
            "Training loss after mini-batch 97000/101301: 1.403\n",
            "Time remaining for current epoch: 0:02:24.020288\n",
            "Training loss after mini-batch 97500/101301: 1.289\n",
            "Time remaining for current epoch: 0:02:08.249090\n",
            "Training loss after mini-batch 98000/101301: 1.241\n",
            "Time remaining for current epoch: 0:01:46.785612\n",
            "Training loss after mini-batch 98500/101301: 1.321\n",
            "Time remaining for current epoch: 0:01:40.683101\n",
            "Training loss after mini-batch 99000/101301: 1.354\n",
            "Time remaining for current epoch: 0:01:19.558958\n",
            "Training loss after mini-batch 99500/101301: 1.329\n",
            "Time remaining for current epoch: 0:01:04.407413\n",
            "Training loss after mini-batch 100000/101301: 1.285\n",
            "Time remaining for current epoch: 0:00:45.111421\n",
            "Training loss after mini-batch 100500/101301: 1.261\n",
            "Time remaining for current epoch: 0:00:26.374875\n",
            "Training loss after mini-batch 101000/101301: 1.399\n",
            "Time remaining for current epoch: 0:00:10.811956\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 4 complete. Training loss: 1.18979. Validation loss: 0.00004. Validation F1 score: 44.77938\n",
            "F1 score increased (44.611184--->44.779380) \t Saving The Model...\n",
            "Starting epoch 5\n",
            "Training loss after mini-batch   500/101301: 1.079\n",
            "Time remaining for current epoch: 0:26:17.759213\n",
            "Training loss after mini-batch  1000/101301: 1.245\n",
            "Time remaining for current epoch: 0:24:54.953186\n",
            "Training loss after mini-batch  1500/101301: 1.262\n",
            "Time remaining for current epoch: 0:30:01.029072\n",
            "Training loss after mini-batch  2000/101301: 1.178\n",
            "Time remaining for current epoch: 0:26:44.075379\n",
            "Training loss after mini-batch  2500/101301: 1.019\n",
            "Time remaining for current epoch: 0:24:34.356334\n",
            "Training loss after mini-batch  3000/101301: 1.005\n",
            "Time remaining for current epoch: 0:23:39.575145\n",
            "Training loss after mini-batch  3500/101301: 0.913\n",
            "Time remaining for current epoch: 0:21:41.529841\n",
            "Training loss after mini-batch  4000/101301: 1.293\n",
            "Time remaining for current epoch: 0:23:24.115863\n",
            "Training loss after mini-batch  4500/101301: 1.123\n",
            "Time remaining for current epoch: 0:23:43.676980\n",
            "Training loss after mini-batch  5000/101301: 1.074\n",
            "Time remaining for current epoch: 0:22:57.032040\n",
            "Training loss after mini-batch  5500/101301: 1.048\n",
            "Time remaining for current epoch: 0:23:09.213537\n",
            "Training loss after mini-batch  6000/101301: 1.062\n",
            "Time remaining for current epoch: 0:23:04.485656\n",
            "Training loss after mini-batch  6500/101301: 1.069\n",
            "Time remaining for current epoch: 0:23:50.756131\n",
            "Training loss after mini-batch  7000/101301: 1.208\n",
            "Time remaining for current epoch: 0:24:17.917610\n",
            "Training loss after mini-batch  7500/101301: 1.106\n",
            "Time remaining for current epoch: 0:29:46.232247\n",
            "Training loss after mini-batch  8000/101301: 1.148\n",
            "Time remaining for current epoch: 0:26:38.753587\n",
            "Training loss after mini-batch  8500/101301: 0.999\n",
            "Time remaining for current epoch: 0:24:57.210032\n",
            "Training loss after mini-batch  9000/101301: 0.924\n",
            "Time remaining for current epoch: 0:21:32.937566\n",
            "Training loss after mini-batch  9500/101301: 0.856\n",
            "Time remaining for current epoch: 0:21:15.857674\n",
            "Training loss after mini-batch 10000/101301: 1.211\n",
            "Time remaining for current epoch: 0:22:56.863218\n",
            "Training loss after mini-batch 10500/101301: 1.435\n",
            "Time remaining for current epoch: 0:26:13.598815\n",
            "Training loss after mini-batch 11000/101301: 1.240\n",
            "Time remaining for current epoch: 0:20:29.468332\n",
            "Training loss after mini-batch 11500/101301: 0.991\n",
            "Time remaining for current epoch: 0:20:02.137990\n",
            "Training loss after mini-batch 12000/101301: 0.995\n",
            "Time remaining for current epoch: 0:21:55.693119\n",
            "Training loss after mini-batch 12500/101301: 1.041\n",
            "Time remaining for current epoch: 0:21:59.216430\n",
            "Training loss after mini-batch 13000/101301: 0.757\n",
            "Time remaining for current epoch: 0:19:49.270662\n",
            "Training loss after mini-batch 13500/101301: 0.869\n",
            "Time remaining for current epoch: 0:20:37.193557\n",
            "Training loss after mini-batch 14000/101301: 0.801\n",
            "Time remaining for current epoch: 0:19:27.429870\n",
            "Training loss after mini-batch 14500/101301: 0.890\n",
            "Time remaining for current epoch: 0:19:57.961389\n",
            "Training loss after mini-batch 15000/101301: 0.898\n",
            "Time remaining for current epoch: 0:22:54.549117\n",
            "Training loss after mini-batch 15500/101301: 1.045\n",
            "Time remaining for current epoch: 0:24:29.874650\n",
            "Training loss after mini-batch 16000/101301: 1.060\n",
            "Time remaining for current epoch: 0:26:32.673641\n",
            "Training loss after mini-batch 16500/101301: 1.214\n",
            "Time remaining for current epoch: 0:25:47.801386\n",
            "Training loss after mini-batch 17000/101301: 1.204\n",
            "Time remaining for current epoch: 0:24:10.264478\n",
            "Training loss after mini-batch 17500/101301: 1.102\n",
            "Time remaining for current epoch: 0:24:50.109708\n",
            "Training loss after mini-batch 18000/101301: 0.932\n",
            "Time remaining for current epoch: 0:19:03.996965\n",
            "Training loss after mini-batch 18500/101301: 0.957\n",
            "Time remaining for current epoch: 0:18:45.608943\n",
            "Training loss after mini-batch 19000/101301: 1.150\n",
            "Time remaining for current epoch: 0:21:07.570918\n",
            "Training loss after mini-batch 19500/101301: 1.054\n",
            "Time remaining for current epoch: 0:21:25.108652\n",
            "Training loss after mini-batch 20000/101301: 1.068\n",
            "Time remaining for current epoch: 0:21:42.827366\n",
            "Training loss after mini-batch 20500/101301: 1.188\n",
            "Time remaining for current epoch: 0:24:36.149274\n",
            "Training loss after mini-batch 21000/101301: 1.137\n",
            "Time remaining for current epoch: 0:25:11.039264\n",
            "Training loss after mini-batch 21500/101301: 0.759\n",
            "Time remaining for current epoch: 0:24:15.520450\n",
            "Training loss after mini-batch 22000/101301: 0.904\n",
            "Time remaining for current epoch: 0:23:30.743649\n",
            "Training loss after mini-batch 22500/101301: 1.016\n",
            "Time remaining for current epoch: 0:24:06.316680\n",
            "Training loss after mini-batch 23000/101301: 1.116\n",
            "Time remaining for current epoch: 0:19:28.517138\n",
            "Training loss after mini-batch 23500/101301: 1.011\n",
            "Time remaining for current epoch: 0:18:20.852512\n",
            "Training loss after mini-batch 24000/101301: 0.949\n",
            "Time remaining for current epoch: 0:17:20.837630\n",
            "Training loss after mini-batch 24500/101301: 0.826\n",
            "Time remaining for current epoch: 0:17:54.500722\n",
            "Training loss after mini-batch 25000/101301: 1.208\n",
            "Time remaining for current epoch: 0:19:54.624278\n",
            "Training loss after mini-batch 25500/101301: 1.163\n",
            "Time remaining for current epoch: 0:18:52.743379\n",
            "Training loss after mini-batch 26000/101301: 0.978\n",
            "Time remaining for current epoch: 0:16:17.320124\n",
            "Training loss after mini-batch 26500/101301: 1.003\n",
            "Time remaining for current epoch: 0:17:02.045591\n",
            "Training loss after mini-batch 27000/101301: 1.079\n",
            "Time remaining for current epoch: 0:19:15.623079\n",
            "Training loss after mini-batch 27500/101301: 1.019\n",
            "Time remaining for current epoch: 0:16:29.865689\n",
            "Training loss after mini-batch 28000/101301: 0.969\n",
            "Time remaining for current epoch: 0:15:29.293474\n",
            "Training loss after mini-batch 28500/101301: 1.128\n",
            "Time remaining for current epoch: 0:18:30.529344\n",
            "Training loss after mini-batch 29000/101301: 1.198\n",
            "Time remaining for current epoch: 0:18:33.722142\n",
            "Training loss after mini-batch 29500/101301: 0.809\n",
            "Time remaining for current epoch: 0:16:06.581151\n",
            "Training loss after mini-batch 30000/101301: 0.835\n",
            "Time remaining for current epoch: 0:15:08.616048\n",
            "Training loss after mini-batch 30500/101301: 1.025\n",
            "Time remaining for current epoch: 0:20:00.380410\n",
            "Training loss after mini-batch 31000/101301: 1.083\n",
            "Time remaining for current epoch: 0:19:23.874443\n",
            "Training loss after mini-batch 31500/101301: 1.088\n",
            "Time remaining for current epoch: 0:15:32.894044\n",
            "Training loss after mini-batch 32000/101301: 0.865\n",
            "Time remaining for current epoch: 0:15:42.253222\n",
            "Training loss after mini-batch 32500/101301: 1.030\n",
            "Time remaining for current epoch: 0:16:01.979313\n",
            "Training loss after mini-batch 33000/101301: 1.047\n",
            "Time remaining for current epoch: 0:16:14.293466\n",
            "Training loss after mini-batch 33500/101301: 0.978\n",
            "Time remaining for current epoch: 0:17:01.330316\n",
            "Training loss after mini-batch 34000/101301: 1.047\n",
            "Time remaining for current epoch: 0:19:28.398802\n",
            "Training loss after mini-batch 34500/101301: 1.027\n",
            "Time remaining for current epoch: 0:19:24.568022\n",
            "Training loss after mini-batch 35000/101301: 0.911\n",
            "Time remaining for current epoch: 0:16:41.804863\n",
            "Training loss after mini-batch 35500/101301: 0.938\n",
            "Time remaining for current epoch: 0:14:24.436277\n",
            "Training loss after mini-batch 36000/101301: 1.010\n",
            "Time remaining for current epoch: 0:18:55.620065\n",
            "Training loss after mini-batch 36500/101301: 1.160\n",
            "Time remaining for current epoch: 0:18:20.766058\n",
            "Training loss after mini-batch 37000/101301: 1.252\n",
            "Time remaining for current epoch: 0:18:20.176904\n",
            "Training loss after mini-batch 37500/101301: 1.109\n",
            "Time remaining for current epoch: 0:17:13.742539\n",
            "Training loss after mini-batch 38000/101301: 1.153\n",
            "Time remaining for current epoch: 0:15:05.499605\n",
            "Training loss after mini-batch 38500/101301: 1.086\n",
            "Time remaining for current epoch: 0:15:47.502308\n",
            "Training loss after mini-batch 39000/101301: 1.243\n",
            "Time remaining for current epoch: 0:15:12.682414\n",
            "Training loss after mini-batch 39500/101301: 0.965\n",
            "Time remaining for current epoch: 0:14:00.889717\n",
            "Training loss after mini-batch 40000/101301: 0.865\n",
            "Time remaining for current epoch: 0:13:57.174373\n",
            "Training loss after mini-batch 40500/101301: 1.052\n",
            "Time remaining for current epoch: 0:16:42.139412\n",
            "Training loss after mini-batch 41000/101301: 1.038\n",
            "Time remaining for current epoch: 0:13:49.102261\n",
            "Training loss after mini-batch 41500/101301: 0.943\n",
            "Time remaining for current epoch: 0:13:34.495459\n",
            "Training loss after mini-batch 42000/101301: 0.879\n",
            "Time remaining for current epoch: 0:13:23.269799\n",
            "Training loss after mini-batch 42500/101301: 0.943\n",
            "Time remaining for current epoch: 0:13:29.446591\n",
            "Training loss after mini-batch 43000/101301: 0.946\n",
            "Time remaining for current epoch: 0:14:11.712315\n",
            "Training loss after mini-batch 43500/101301: 0.887\n",
            "Time remaining for current epoch: 0:15:52.915834\n",
            "Training loss after mini-batch 44000/101301: 1.007\n",
            "Time remaining for current epoch: 0:14:15.639553\n",
            "Training loss after mini-batch 44500/101301: 1.034\n",
            "Time remaining for current epoch: 0:16:35.777824\n",
            "Training loss after mini-batch 45000/101301: 1.089\n",
            "Time remaining for current epoch: 0:16:57.727037\n",
            "Training loss after mini-batch 45500/101301: 1.072\n",
            "Time remaining for current epoch: 0:14:31.100288\n",
            "Training loss after mini-batch 46000/101301: 0.926\n",
            "Time remaining for current epoch: 0:13:09.101759\n",
            "Training loss after mini-batch 46500/101301: 0.806\n",
            "Time remaining for current epoch: 0:12:48.466414\n",
            "Training loss after mini-batch 47000/101301: 0.933\n",
            "Time remaining for current epoch: 0:13:10.275775\n",
            "Training loss after mini-batch 47500/101301: 0.941\n",
            "Time remaining for current epoch: 0:12:25.713807\n",
            "Training loss after mini-batch 48000/101301: 1.160\n",
            "Time remaining for current epoch: 0:12:50.294165\n",
            "Training loss after mini-batch 48500/101301: 1.033\n",
            "Time remaining for current epoch: 0:13:18.766492\n",
            "Training loss after mini-batch 49000/101301: 1.060\n",
            "Time remaining for current epoch: 0:12:01.900252\n",
            "Training loss after mini-batch 49500/101301: 0.875\n",
            "Time remaining for current epoch: 0:11:39.637869\n",
            "Training loss after mini-batch 50000/101301: 0.657\n",
            "Time remaining for current epoch: 0:11:06.384469\n",
            "Training loss after mini-batch 50500/101301: 0.704\n",
            "Time remaining for current epoch: 0:12:12.860585\n",
            "Training loss after mini-batch 51000/101301: 1.121\n",
            "Time remaining for current epoch: 0:12:34.369673\n",
            "Training loss after mini-batch 51500/101301: 1.073\n",
            "Time remaining for current epoch: 0:11:33.439016\n",
            "Training loss after mini-batch 52000/101301: 0.842\n",
            "Time remaining for current epoch: 0:12:12.714823\n",
            "Training loss after mini-batch 52500/101301: 0.853\n",
            "Time remaining for current epoch: 0:11:05.056736\n",
            "Training loss after mini-batch 53000/101301: 0.930\n",
            "Time remaining for current epoch: 0:11:01.037162\n",
            "Training loss after mini-batch 53500/101301: 0.977\n",
            "Time remaining for current epoch: 0:11:37.666412\n",
            "Training loss after mini-batch 54000/101301: 0.887\n",
            "Time remaining for current epoch: 0:11:56.299957\n",
            "Training loss after mini-batch 54500/101301: 1.017\n",
            "Time remaining for current epoch: 0:12:23.843180\n",
            "Training loss after mini-batch 55000/101301: 1.159\n",
            "Time remaining for current epoch: 0:12:09.128638\n",
            "Training loss after mini-batch 55500/101301: 1.050\n",
            "Time remaining for current epoch: 0:12:36.166877\n",
            "Training loss after mini-batch 56000/101301: 1.029\n",
            "Time remaining for current epoch: 0:10:42.060830\n",
            "Training loss after mini-batch 56500/101301: 0.958\n",
            "Time remaining for current epoch: 0:10:19.367557\n",
            "Training loss after mini-batch 57000/101301: 1.065\n",
            "Time remaining for current epoch: 0:10:09.995913\n",
            "Training loss after mini-batch 57500/101301: 1.141\n",
            "Time remaining for current epoch: 0:09:59.862054\n",
            "Training loss after mini-batch 58000/101301: 1.185\n",
            "Time remaining for current epoch: 0:12:28.917578\n",
            "Training loss after mini-batch 58500/101301: 1.085\n",
            "Time remaining for current epoch: 0:11:37.813254\n",
            "Training loss after mini-batch 59000/101301: 1.002\n",
            "Time remaining for current epoch: 0:09:50.359527\n",
            "Training loss after mini-batch 59500/101301: 0.860\n",
            "Time remaining for current epoch: 0:09:54.797475\n",
            "Training loss after mini-batch 60000/101301: 0.801\n",
            "Time remaining for current epoch: 0:09:41.598743\n",
            "Training loss after mini-batch 60500/101301: 1.022\n",
            "Time remaining for current epoch: 0:10:57.981651\n",
            "Training loss after mini-batch 61000/101301: 1.000\n",
            "Time remaining for current epoch: 0:10:27.943192\n",
            "Training loss after mini-batch 61500/101301: 0.737\n",
            "Time remaining for current epoch: 0:09:11.721594\n",
            "Training loss after mini-batch 62000/101301: 0.993\n",
            "Time remaining for current epoch: 0:08:52.106566\n",
            "Training loss after mini-batch 62500/101301: 0.949\n",
            "Time remaining for current epoch: 0:13:29.625096\n",
            "Training loss after mini-batch 63000/101301: 0.991\n",
            "Time remaining for current epoch: 0:12:59.251607\n",
            "Training loss after mini-batch 63500/101301: 1.109\n",
            "Time remaining for current epoch: 0:09:25.314385\n",
            "Training loss after mini-batch 64000/101301: 0.991\n",
            "Time remaining for current epoch: 0:08:33.995231\n",
            "Training loss after mini-batch 64500/101301: 0.950\n",
            "Time remaining for current epoch: 0:08:53.707505\n",
            "Training loss after mini-batch 65000/101301: 0.894\n",
            "Time remaining for current epoch: 0:08:38.083556\n",
            "Training loss after mini-batch 65500/101301: 0.751\n",
            "Time remaining for current epoch: 0:08:36.612197\n",
            "Training loss after mini-batch 66000/101301: 1.012\n",
            "Time remaining for current epoch: 0:09:23.945372\n",
            "Training loss after mini-batch 66500/101301: 0.901\n",
            "Time remaining for current epoch: 0:09:27.177915\n",
            "Training loss after mini-batch 67000/101301: 0.705\n",
            "Time remaining for current epoch: 0:09:23.239188\n",
            "Training loss after mini-batch 67500/101301: 0.998\n",
            "Time remaining for current epoch: 0:11:05.448887\n",
            "Training loss after mini-batch 68000/101301: 1.033\n",
            "Time remaining for current epoch: 0:08:08.301888\n",
            "Training loss after mini-batch 68500/101301: 0.931\n",
            "Time remaining for current epoch: 0:08:31.591430\n",
            "Training loss after mini-batch 69000/101301: 0.958\n",
            "Time remaining for current epoch: 0:08:37.866407\n",
            "Training loss after mini-batch 69500/101301: 1.063\n",
            "Time remaining for current epoch: 0:08:28.080915\n",
            "Training loss after mini-batch 70000/101301: 1.006\n",
            "Time remaining for current epoch: 0:10:12.049482\n",
            "Training loss after mini-batch 70500/101301: 0.861\n",
            "Time remaining for current epoch: 0:09:46.184425\n",
            "Training loss after mini-batch 71000/101301: 0.769\n",
            "Time remaining for current epoch: 0:08:08.985329\n",
            "Training loss after mini-batch 71500/101301: 0.748\n",
            "Time remaining for current epoch: 0:07:02.213896\n",
            "Training loss after mini-batch 72000/101301: 0.815\n",
            "Time remaining for current epoch: 0:06:38.866770\n",
            "Training loss after mini-batch 72500/101301: 0.866\n",
            "Time remaining for current epoch: 0:06:20.220341\n",
            "Training loss after mini-batch 73000/101301: 0.771\n",
            "Time remaining for current epoch: 0:06:30.339073\n",
            "Training loss after mini-batch 73500/101301: 0.874\n",
            "Time remaining for current epoch: 0:07:10.362755\n",
            "Training loss after mini-batch 74000/101301: 0.948\n",
            "Time remaining for current epoch: 0:07:25.686008\n",
            "Training loss after mini-batch 74500/101301: 0.808\n",
            "Time remaining for current epoch: 0:07:14.235966\n",
            "Training loss after mini-batch 75000/101301: 1.058\n",
            "Time remaining for current epoch: 0:06:55.002834\n",
            "Training loss after mini-batch 75500/101301: 0.992\n",
            "Time remaining for current epoch: 0:06:51.493970\n",
            "Training loss after mini-batch 76000/101301: 0.816\n",
            "Time remaining for current epoch: 0:05:39.891885\n",
            "Training loss after mini-batch 76500/101301: 0.758\n",
            "Time remaining for current epoch: 0:05:28.581544\n",
            "Training loss after mini-batch 77000/101301: 0.841\n",
            "Time remaining for current epoch: 0:06:58.006222\n",
            "Training loss after mini-batch 77500/101301: 0.909\n",
            "Time remaining for current epoch: 0:07:44.432806\n",
            "Training loss after mini-batch 78000/101301: 0.942\n",
            "Time remaining for current epoch: 0:05:42.241871\n",
            "Training loss after mini-batch 78500/101301: 0.886\n",
            "Time remaining for current epoch: 0:05:23.455668\n",
            "Training loss after mini-batch 79000/101301: 1.028\n",
            "Time remaining for current epoch: 0:05:06.551669\n",
            "Training loss after mini-batch 79500/101301: 0.856\n",
            "Time remaining for current epoch: 0:04:56.120606\n",
            "Training loss after mini-batch 80000/101301: 0.787\n",
            "Time remaining for current epoch: 0:04:58.000660\n",
            "Training loss after mini-batch 80500/101301: 1.038\n",
            "Time remaining for current epoch: 0:05:41.633814\n",
            "Training loss after mini-batch 81000/101301: 0.849\n",
            "Time remaining for current epoch: 0:05:35.350913\n",
            "Training loss after mini-batch 81500/101301: 0.634\n",
            "Time remaining for current epoch: 0:04:33.178448\n",
            "Training loss after mini-batch 82000/101301: 0.723\n",
            "Time remaining for current epoch: 0:04:30.693867\n",
            "Training loss after mini-batch 82500/101301: 0.573\n",
            "Time remaining for current epoch: 0:04:03.083085\n",
            "Training loss after mini-batch 83000/101301: 0.680\n",
            "Time remaining for current epoch: 0:04:04.339310\n",
            "Training loss after mini-batch 83500/101301: 0.662\n",
            "Time remaining for current epoch: 0:04:24.823004\n",
            "Training loss after mini-batch 84000/101301: 0.598\n",
            "Time remaining for current epoch: 0:04:02.841650\n",
            "Training loss after mini-batch 84500/101301: 0.635\n",
            "Time remaining for current epoch: 0:03:48.357902\n",
            "Training loss after mini-batch 85000/101301: 0.694\n",
            "Time remaining for current epoch: 0:03:54.162656\n",
            "Training loss after mini-batch 85500/101301: 0.886\n",
            "Time remaining for current epoch: 0:04:25.000928\n",
            "Training loss after mini-batch 86000/101301: 0.951\n",
            "Time remaining for current epoch: 0:04:58.834370\n",
            "Training loss after mini-batch 86500/101301: 0.832\n",
            "Time remaining for current epoch: 0:04:57.899457\n",
            "Training loss after mini-batch 87000/101301: 0.813\n",
            "Time remaining for current epoch: 0:03:35.316118\n",
            "Training loss after mini-batch 87500/101301: 0.892\n",
            "Time remaining for current epoch: 0:03:56.399865\n",
            "Training loss after mini-batch 88000/101301: 0.817\n",
            "Time remaining for current epoch: 0:03:30.451448\n",
            "Training loss after mini-batch 88500/101301: 0.772\n",
            "Time remaining for current epoch: 0:03:19.791810\n",
            "Training loss after mini-batch 89000/101301: 0.749\n",
            "Time remaining for current epoch: 0:02:50.469103\n",
            "Training loss after mini-batch 89500/101301: 0.793\n",
            "Time remaining for current epoch: 0:03:00.538663\n",
            "Training loss after mini-batch 90000/101301: 0.852\n",
            "Time remaining for current epoch: 0:02:51.515040\n",
            "Training loss after mini-batch 90500/101301: 0.676\n",
            "Time remaining for current epoch: 0:02:27.305702\n",
            "Training loss after mini-batch 91000/101301: 0.705\n",
            "Time remaining for current epoch: 0:02:35.741064\n",
            "Training loss after mini-batch 91500/101301: 0.784\n",
            "Time remaining for current epoch: 0:02:31.807655\n",
            "Training loss after mini-batch 92000/101301: 0.808\n",
            "Time remaining for current epoch: 0:02:43.349637\n",
            "Training loss after mini-batch 92500/101301: 0.644\n",
            "Time remaining for current epoch: 0:02:01.044600\n",
            "Training loss after mini-batch 93000/101301: 0.603\n",
            "Time remaining for current epoch: 0:01:50.610463\n",
            "Training loss after mini-batch 93500/101301: 0.540\n",
            "Time remaining for current epoch: 0:01:41.016160\n",
            "Training loss after mini-batch 94000/101301: 0.539\n",
            "Time remaining for current epoch: 0:01:34.297842\n",
            "Training loss after mini-batch 94500/101301: 0.846\n",
            "Time remaining for current epoch: 0:01:41.595629\n",
            "Training loss after mini-batch 95000/101301: 0.740\n",
            "Time remaining for current epoch: 0:01:32.448296\n",
            "Training loss after mini-batch 95500/101301: 0.622\n",
            "Time remaining for current epoch: 0:01:17.453974\n",
            "Training loss after mini-batch 96000/101301: 0.685\n",
            "Time remaining for current epoch: 0:01:30.335326\n",
            "Training loss after mini-batch 96500/101301: 0.784\n",
            "Time remaining for current epoch: 0:01:36.207823\n",
            "Training loss after mini-batch 97000/101301: 0.700\n",
            "Time remaining for current epoch: 0:01:07.386629\n",
            "Training loss after mini-batch 97500/101301: 0.743\n",
            "Time remaining for current epoch: 0:00:58.297296\n",
            "Training loss after mini-batch 98000/101301: 0.828\n",
            "Time remaining for current epoch: 0:00:52.023052\n",
            "Training loss after mini-batch 98500/101301: 0.766\n",
            "Time remaining for current epoch: 0:00:47.128331\n",
            "Training loss after mini-batch 99000/101301: 0.710\n",
            "Time remaining for current epoch: 0:00:33.847567\n",
            "Training loss after mini-batch 99500/101301: 0.675\n",
            "Time remaining for current epoch: 0:00:24.311455\n",
            "Training loss after mini-batch 100000/101301: 0.690\n",
            "Time remaining for current epoch: 0:00:18.103705\n",
            "Training loss after mini-batch 100500/101301: 0.529\n",
            "Time remaining for current epoch: 0:00:11.094872\n",
            "Training loss after mini-batch 101000/101301: 0.465\n",
            "Time remaining for current epoch: 0:00:04.187452\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 5 complete. Training loss: 0.93758. Validation loss: 0.00004. Validation F1 score: 47.47816\n",
            "F1 score increased (44.779380--->47.478156) \t Saving The Model...\n",
            "Starting epoch 6\n",
            "Training loss after mini-batch   500/101301: 1.048\n",
            "Time remaining for current epoch: 0:28:07.193767\n",
            "Training loss after mini-batch  1000/101301: 0.747\n",
            "Time remaining for current epoch: 0:24:42.498876\n",
            "Training loss after mini-batch  1500/101301: 0.781\n",
            "Time remaining for current epoch: 0:23:52.496117\n",
            "Training loss after mini-batch  2000/101301: 0.961\n",
            "Time remaining for current epoch: 0:29:45.011510\n",
            "Training loss after mini-batch  2500/101301: 1.074\n",
            "Time remaining for current epoch: 0:25:08.258026\n",
            "Training loss after mini-batch  3000/101301: 0.918\n",
            "Time remaining for current epoch: 0:26:07.300153\n",
            "Training loss after mini-batch  3500/101301: 0.852\n",
            "Time remaining for current epoch: 0:22:16.801311\n",
            "Training loss after mini-batch  4000/101301: 0.900\n",
            "Time remaining for current epoch: 0:21:37.086582\n",
            "Training loss after mini-batch  4500/101301: 1.009\n",
            "Time remaining for current epoch: 0:26:48.648714\n",
            "Training loss after mini-batch  5000/101301: 0.877\n",
            "Time remaining for current epoch: 0:28:24.970395\n",
            "Training loss after mini-batch  5500/101301: 1.175\n",
            "Time remaining for current epoch: 0:27:00.931594\n",
            "Training loss after mini-batch  6000/101301: 1.057\n",
            "Time remaining for current epoch: 0:29:41.613039\n",
            "Training loss after mini-batch  6500/101301: 1.107\n",
            "Time remaining for current epoch: 0:28:22.339276\n",
            "Training loss after mini-batch  7000/101301: 0.844\n",
            "Time remaining for current epoch: 0:27:25.419377\n",
            "Training loss after mini-batch  7500/101301: 0.976\n",
            "Time remaining for current epoch: 0:25:21.331614\n",
            "Training loss after mini-batch  8000/101301: 0.820\n",
            "Time remaining for current epoch: 0:21:30.680665\n",
            "Training loss after mini-batch  8500/101301: 0.879\n",
            "Time remaining for current epoch: 0:23:40.600637\n",
            "Training loss after mini-batch  9000/101301: 0.887\n",
            "Time remaining for current epoch: 0:24:49.218119\n",
            "Training loss after mini-batch  9500/101301: 1.142\n",
            "Time remaining for current epoch: 0:22:23.214369\n",
            "Training loss after mini-batch 10000/101301: 1.232\n",
            "Time remaining for current epoch: 0:23:12.844568\n",
            "Training loss after mini-batch 10500/101301: 1.280\n",
            "Time remaining for current epoch: 0:22:28.609639\n",
            "Training loss after mini-batch 11000/101301: 1.078\n",
            "Time remaining for current epoch: 0:21:27.746664\n",
            "Training loss after mini-batch 11500/101301: 1.198\n",
            "Time remaining for current epoch: 0:20:33.679836\n",
            "Training loss after mini-batch 12000/101301: 0.935\n",
            "Time remaining for current epoch: 0:19:28.223739\n",
            "Training loss after mini-batch 12500/101301: 1.001\n",
            "Time remaining for current epoch: 0:19:47.530408\n",
            "Training loss after mini-batch 13000/101301: 0.860\n",
            "Time remaining for current epoch: 0:19:57.016966\n",
            "Training loss after mini-batch 13500/101301: 0.739\n",
            "Time remaining for current epoch: 0:20:30.488132\n",
            "Training loss after mini-batch 14000/101301: 0.838\n",
            "Time remaining for current epoch: 0:24:12.380370\n",
            "Training loss after mini-batch 14500/101301: 0.949\n",
            "Time remaining for current epoch: 0:26:00.193622\n",
            "Training loss after mini-batch 15000/101301: 0.848\n",
            "Time remaining for current epoch: 0:21:13.866112\n",
            "Training loss after mini-batch 15500/101301: 0.885\n",
            "Time remaining for current epoch: 0:20:14.000409\n",
            "Training loss after mini-batch 16000/101301: 0.990\n",
            "Time remaining for current epoch: 0:20:37.193233\n",
            "Training loss after mini-batch 16500/101301: 0.948\n",
            "Time remaining for current epoch: 0:20:03.116695\n",
            "Training loss after mini-batch 17000/101301: 0.883\n",
            "Time remaining for current epoch: 0:23:53.364056\n",
            "Training loss after mini-batch 17500/101301: 1.020\n",
            "Time remaining for current epoch: 0:20:46.475700\n",
            "Training loss after mini-batch 18000/101301: 0.870\n",
            "Time remaining for current epoch: 0:20:57.098258\n",
            "Training loss after mini-batch 18500/101301: 0.932\n",
            "Time remaining for current epoch: 0:20:24.229671\n",
            "Training loss after mini-batch 19000/101301: 0.864\n",
            "Time remaining for current epoch: 0:19:34.191366\n",
            "Training loss after mini-batch 19500/101301: 0.625\n",
            "Time remaining for current epoch: 0:19:07.786118\n",
            "Training loss after mini-batch 20000/101301: 0.804\n",
            "Time remaining for current epoch: 0:19:39.624068\n",
            "Training loss after mini-batch 20500/101301: 0.912\n",
            "Time remaining for current epoch: 0:19:56.223422\n",
            "Training loss after mini-batch 21000/101301: 0.820\n",
            "Time remaining for current epoch: 0:17:12.672747\n",
            "Training loss after mini-batch 21500/101301: 0.935\n",
            "Time remaining for current epoch: 0:16:55.154358\n",
            "Training loss after mini-batch 22000/101301: 0.969\n",
            "Time remaining for current epoch: 0:17:22.056876\n",
            "Training loss after mini-batch 22500/101301: 0.904\n",
            "Time remaining for current epoch: 0:16:51.340572\n",
            "Training loss after mini-batch 23000/101301: 1.060\n",
            "Time remaining for current epoch: 0:20:17.013643\n",
            "Training loss after mini-batch 23500/101301: 1.014\n",
            "Time remaining for current epoch: 0:23:10.871615\n",
            "Training loss after mini-batch 24000/101301: 0.956\n",
            "Time remaining for current epoch: 0:23:09.031721\n",
            "Training loss after mini-batch 24500/101301: 0.936\n",
            "Time remaining for current epoch: 0:21:00.757041\n",
            "Training loss after mini-batch 25000/101301: 1.043\n",
            "Time remaining for current epoch: 0:19:19.564891\n",
            "Training loss after mini-batch 25500/101301: 0.986\n",
            "Time remaining for current epoch: 0:20:56.250982\n",
            "Training loss after mini-batch 26000/101301: 0.986\n",
            "Time remaining for current epoch: 0:19:29.714741\n",
            "Training loss after mini-batch 26500/101301: 0.953\n",
            "Time remaining for current epoch: 0:17:25.908270\n",
            "Training loss after mini-batch 27000/101301: 0.647\n",
            "Time remaining for current epoch: 0:17:01.370443\n",
            "Training loss after mini-batch 27500/101301: 0.771\n",
            "Time remaining for current epoch: 0:16:49.060722\n",
            "Training loss after mini-batch 28000/101301: 0.852\n",
            "Time remaining for current epoch: 0:18:01.264374\n",
            "Training loss after mini-batch 28500/101301: 0.785\n",
            "Time remaining for current epoch: 0:22:10.244368\n",
            "Training loss after mini-batch 29000/101301: 1.042\n",
            "Time remaining for current epoch: 0:16:47.314437\n",
            "Training loss after mini-batch 29500/101301: 0.907\n",
            "Time remaining for current epoch: 0:16:32.964275\n",
            "Training loss after mini-batch 30000/101301: 0.885\n",
            "Time remaining for current epoch: 0:16:39.393628\n",
            "Training loss after mini-batch 30500/101301: 0.859\n",
            "Time remaining for current epoch: 0:16:41.686623\n",
            "Training loss after mini-batch 31000/101301: 0.931\n",
            "Time remaining for current epoch: 0:18:01.444634\n",
            "Training loss after mini-batch 31500/101301: 0.909\n",
            "Time remaining for current epoch: 0:19:22.528447\n",
            "Training loss after mini-batch 32000/101301: 0.907\n",
            "Time remaining for current epoch: 0:19:53.856572\n",
            "Training loss after mini-batch 32500/101301: 0.767\n",
            "Time remaining for current epoch: 0:15:17.898588\n",
            "Training loss after mini-batch 33000/101301: 0.806\n",
            "Time remaining for current epoch: 0:15:34.056345\n",
            "Training loss after mini-batch 33500/101301: 0.873\n",
            "Time remaining for current epoch: 0:14:41.204924\n",
            "Training loss after mini-batch 34000/101301: 0.792\n",
            "Time remaining for current epoch: 0:14:42.345162\n",
            "Training loss after mini-batch 34500/101301: 0.929\n",
            "Time remaining for current epoch: 0:15:25.655078\n",
            "Training loss after mini-batch 35000/101301: 0.783\n",
            "Time remaining for current epoch: 0:16:07.362677\n",
            "Training loss after mini-batch 35500/101301: 0.918\n",
            "Time remaining for current epoch: 0:16:58.433594\n",
            "Training loss after mini-batch 36000/101301: 0.899\n",
            "Time remaining for current epoch: 0:17:12.950108\n",
            "Training loss after mini-batch 36500/101301: 1.083\n",
            "Time remaining for current epoch: 0:14:45.694860\n",
            "Training loss after mini-batch 37000/101301: 0.922\n",
            "Time remaining for current epoch: 0:15:52.631031\n",
            "Training loss after mini-batch 37500/101301: 0.867\n",
            "Time remaining for current epoch: 0:14:34.966989\n",
            "Training loss after mini-batch 38000/101301: 0.931\n",
            "Time remaining for current epoch: 0:16:09.341416\n",
            "Training loss after mini-batch 38500/101301: 0.886\n",
            "Time remaining for current epoch: 0:15:16.895313\n",
            "Training loss after mini-batch 39000/101301: 0.946\n",
            "Time remaining for current epoch: 0:16:07.067513\n",
            "Training loss after mini-batch 39500/101301: 0.855\n",
            "Time remaining for current epoch: 0:13:59.935923\n",
            "Training loss after mini-batch 40000/101301: 0.819\n",
            "Time remaining for current epoch: 0:13:50.449437\n",
            "Training loss after mini-batch 40500/101301: 0.726\n",
            "Time remaining for current epoch: 0:12:48.775211\n",
            "Training loss after mini-batch 41000/101301: 0.769\n",
            "Time remaining for current epoch: 0:13:03.608584\n",
            "Training loss after mini-batch 41500/101301: 0.904\n",
            "Time remaining for current epoch: 0:14:29.076812\n",
            "Training loss after mini-batch 42000/101301: 0.933\n",
            "Time remaining for current epoch: 0:15:58.873167\n",
            "Training loss after mini-batch 42500/101301: 0.853\n",
            "Time remaining for current epoch: 0:18:33.118201\n",
            "Training loss after mini-batch 43000/101301: 0.920\n",
            "Time remaining for current epoch: 0:16:25.895922\n",
            "Training loss after mini-batch 43500/101301: 0.870\n",
            "Time remaining for current epoch: 0:14:10.893218\n",
            "Training loss after mini-batch 44000/101301: 0.915\n",
            "Time remaining for current epoch: 0:14:34.499396\n",
            "Training loss after mini-batch 44500/101301: 0.955\n",
            "Time remaining for current epoch: 0:15:43.163434\n",
            "Training loss after mini-batch 45000/101301: 0.899\n",
            "Time remaining for current epoch: 0:13:18.548901\n",
            "Training loss after mini-batch 45500/101301: 0.936\n",
            "Time remaining for current epoch: 0:13:10.243980\n",
            "Training loss after mini-batch 46000/101301: 0.948\n",
            "Time remaining for current epoch: 0:12:40.949868\n",
            "Training loss after mini-batch 46500/101301: 0.879\n",
            "Time remaining for current epoch: 0:13:02.863582\n",
            "Training loss after mini-batch 47000/101301: 0.799\n",
            "Time remaining for current epoch: 0:12:12.801699\n",
            "Training loss after mini-batch 47500/101301: 0.650\n",
            "Time remaining for current epoch: 0:12:28.482651\n",
            "Training loss after mini-batch 48000/101301: 0.725\n",
            "Time remaining for current epoch: 0:12:01.054371\n",
            "Training loss after mini-batch 48500/101301: 0.652\n",
            "Time remaining for current epoch: 0:11:37.291408\n",
            "Training loss after mini-batch 49000/101301: 0.796\n",
            "Time remaining for current epoch: 0:12:38.077426\n",
            "Training loss after mini-batch 49500/101301: 0.897\n",
            "Time remaining for current epoch: 0:11:37.310599\n",
            "Training loss after mini-batch 50000/101301: 1.037\n",
            "Time remaining for current epoch: 0:11:39.278723\n",
            "Training loss after mini-batch 50500/101301: 1.044\n",
            "Time remaining for current epoch: 0:13:28.014575\n",
            "Training loss after mini-batch 51000/101301: 0.903\n",
            "Time remaining for current epoch: 0:12:47.195210\n",
            "Training loss after mini-batch 51500/101301: 0.516\n",
            "Time remaining for current epoch: 0:12:12.915642\n",
            "Training loss after mini-batch 52000/101301: 0.633\n",
            "Time remaining for current epoch: 0:11:29.215968\n",
            "Training loss after mini-batch 52500/101301: 0.854\n",
            "Time remaining for current epoch: 0:12:30.920585\n",
            "Training loss after mini-batch 53000/101301: 0.889\n",
            "Time remaining for current epoch: 0:12:59.208060\n",
            "Training loss after mini-batch 53500/101301: 0.995\n",
            "Time remaining for current epoch: 0:15:25.343008\n",
            "Training loss after mini-batch 54000/101301: 0.969\n",
            "Time remaining for current epoch: 0:12:12.100820\n",
            "Training loss after mini-batch 54500/101301: 1.076\n",
            "Time remaining for current epoch: 0:12:04.549577\n",
            "Training loss after mini-batch 55000/101301: 0.994\n",
            "Time remaining for current epoch: 0:13:01.504564\n",
            "Training loss after mini-batch 55500/101301: 1.004\n",
            "Time remaining for current epoch: 0:11:06.955703\n",
            "Training loss after mini-batch 56000/101301: 0.974\n",
            "Time remaining for current epoch: 0:11:25.697847\n",
            "Training loss after mini-batch 56500/101301: 0.961\n",
            "Time remaining for current epoch: 0:10:59.443943\n",
            "Training loss after mini-batch 57000/101301: 1.205\n",
            "Time remaining for current epoch: 0:10:01.652424\n",
            "Training loss after mini-batch 57500/101301: 0.970\n",
            "Time remaining for current epoch: 0:12:37.385079\n",
            "Training loss after mini-batch 58000/101301: 1.192\n",
            "Time remaining for current epoch: 0:11:56.274645\n",
            "Training loss after mini-batch 58500/101301: 1.071\n",
            "Time remaining for current epoch: 0:11:35.768649\n",
            "Training loss after mini-batch 59000/101301: 0.614\n",
            "Time remaining for current epoch: 0:09:52.104251\n",
            "Training loss after mini-batch 59500/101301: 0.545\n",
            "Time remaining for current epoch: 0:09:21.223540\n",
            "Training loss after mini-batch 60000/101301: 0.804\n",
            "Time remaining for current epoch: 0:09:23.189068\n",
            "Training loss after mini-batch 60500/101301: 0.909\n",
            "Time remaining for current epoch: 0:10:11.326336\n",
            "Training loss after mini-batch 61000/101301: 0.861\n",
            "Time remaining for current epoch: 0:10:14.960312\n",
            "Training loss after mini-batch 61500/101301: 0.837\n",
            "Time remaining for current epoch: 0:11:57.564231\n",
            "Training loss after mini-batch 62000/101301: 0.913\n",
            "Time remaining for current epoch: 0:08:41.047669\n",
            "Training loss after mini-batch 62500/101301: 1.058\n",
            "Time remaining for current epoch: 0:09:49.654487\n",
            "Training loss after mini-batch 63000/101301: 1.064\n",
            "Time remaining for current epoch: 0:10:27.034357\n",
            "Training loss after mini-batch 63500/101301: 1.321\n",
            "Time remaining for current epoch: 0:21:52.730585\n",
            "Training loss after mini-batch 64000/101301: 1.473\n",
            "Time remaining for current epoch: 0:24:27.972852\n",
            "Training loss after mini-batch 64500/101301: 1.192\n",
            "Time remaining for current epoch: 0:22:36.109511\n",
            "Training loss after mini-batch 65000/101301: 1.315\n",
            "Time remaining for current epoch: 0:21:16.719328\n",
            "Training loss after mini-batch 65500/101301: 1.370\n",
            "Time remaining for current epoch: 0:20:12.107078\n",
            "Training loss after mini-batch 66000/101301: 1.340\n",
            "Time remaining for current epoch: 0:21:36.211371\n",
            "Training loss after mini-batch 66500/101301: 1.359\n",
            "Time remaining for current epoch: 0:21:42.522601\n",
            "Training loss after mini-batch 67000/101301: 1.278\n",
            "Time remaining for current epoch: 0:21:43.869651\n",
            "Training loss after mini-batch 67500/101301: 1.323\n",
            "Time remaining for current epoch: 0:19:49.884275\n",
            "Training loss after mini-batch 68000/101301: 1.202\n",
            "Time remaining for current epoch: 0:17:23.647565\n",
            "Training loss after mini-batch 68500/101301: 1.304\n",
            "Time remaining for current epoch: 0:18:51.458010\n",
            "Training loss after mini-batch 69000/101301: 1.477\n",
            "Time remaining for current epoch: 0:22:28.923344\n",
            "Training loss after mini-batch 69500/101301: 1.384\n",
            "Time remaining for current epoch: 0:21:59.112972\n",
            "Training loss after mini-batch 70000/101301: 1.395\n",
            "Time remaining for current epoch: 0:19:37.370973\n",
            "Training loss after mini-batch 70500/101301: 1.308\n",
            "Time remaining for current epoch: 0:17:44.442933\n",
            "Training loss after mini-batch 71000/101301: 1.350\n",
            "Time remaining for current epoch: 0:17:38.087096\n",
            "Training loss after mini-batch 71500/101301: 1.365\n",
            "Time remaining for current epoch: 0:17:44.944039\n",
            "Training loss after mini-batch 72000/101301: 1.318\n",
            "Time remaining for current epoch: 0:17:45.723236\n",
            "Training loss after mini-batch 72500/101301: 1.250\n",
            "Time remaining for current epoch: 0:17:13.571080\n",
            "Training loss after mini-batch 73000/101301: 1.138\n",
            "Time remaining for current epoch: 0:16:27.668115\n",
            "Training loss after mini-batch 73500/101301: 1.204\n",
            "Time remaining for current epoch: 0:15:52.346603\n",
            "Training loss after mini-batch 74000/101301: 1.338\n",
            "Time remaining for current epoch: 0:17:41.233061\n",
            "Training loss after mini-batch 74500/101301: 1.419\n",
            "Time remaining for current epoch: 0:16:30.509893\n",
            "Training loss after mini-batch 75000/101301: 1.527\n",
            "Time remaining for current epoch: 0:17:02.083313\n",
            "Training loss after mini-batch 75500/101301: 1.277\n",
            "Time remaining for current epoch: 0:14:56.884276\n",
            "Training loss after mini-batch 76000/101301: 1.268\n",
            "Time remaining for current epoch: 0:15:56.425447\n",
            "Training loss after mini-batch 76500/101301: 1.408\n",
            "Time remaining for current epoch: 0:14:45.768003\n",
            "Training loss after mini-batch 77000/101301: 1.201\n",
            "Time remaining for current epoch: 0:14:02.008992\n",
            "Training loss after mini-batch 77500/101301: 1.308\n",
            "Time remaining for current epoch: 0:13:38.290016\n",
            "Training loss after mini-batch 78000/101301: 1.346\n",
            "Time remaining for current epoch: 0:14:42.520220\n",
            "Training loss after mini-batch 78500/101301: 1.361\n",
            "Time remaining for current epoch: 0:13:14.650781\n",
            "Training loss after mini-batch 79000/101301: 1.221\n",
            "Time remaining for current epoch: 0:13:32.824072\n",
            "Training loss after mini-batch 79500/101301: 1.657\n",
            "Time remaining for current epoch: 0:13:47.888877\n",
            "Training loss after mini-batch 80000/101301: 1.247\n",
            "Time remaining for current epoch: 0:13:28.474903\n",
            "Training loss after mini-batch 80500/101301: 1.257\n",
            "Time remaining for current epoch: 0:12:27.669443\n",
            "Training loss after mini-batch 81000/101301: 1.331\n",
            "Time remaining for current epoch: 0:12:13.990762\n",
            "Training loss after mini-batch 81500/101301: 1.225\n",
            "Time remaining for current epoch: 0:11:40.466607\n",
            "Training loss after mini-batch 82000/101301: 1.253\n",
            "Time remaining for current epoch: 0:11:50.805320\n",
            "Training loss after mini-batch 82500/101301: 1.181\n",
            "Time remaining for current epoch: 0:11:40.940282\n",
            "Training loss after mini-batch 83000/101301: 1.120\n",
            "Time remaining for current epoch: 0:10:58.352581\n",
            "Training loss after mini-batch 83500/101301: 1.262\n",
            "Time remaining for current epoch: 0:11:32.218016\n",
            "Training loss after mini-batch 84000/101301: 1.296\n",
            "Time remaining for current epoch: 0:11:36.885331\n",
            "Training loss after mini-batch 84500/101301: 1.205\n",
            "Time remaining for current epoch: 0:10:58.636514\n",
            "Training loss after mini-batch 85000/101301: 1.174\n",
            "Time remaining for current epoch: 0:09:10.574477\n",
            "Training loss after mini-batch 85500/101301: 1.146\n",
            "Time remaining for current epoch: 0:09:01.988679\n",
            "Training loss after mini-batch 86000/101301: 1.199\n",
            "Time remaining for current epoch: 0:09:11.932551\n",
            "Training loss after mini-batch 86500/101301: 1.202\n",
            "Time remaining for current epoch: 0:09:44.353735\n",
            "Training loss after mini-batch 87000/101301: 1.347\n",
            "Time remaining for current epoch: 0:09:59.043841\n",
            "Training loss after mini-batch 87500/101301: 1.196\n",
            "Time remaining for current epoch: 0:08:47.061119\n",
            "Training loss after mini-batch 88000/101301: 1.168\n",
            "Time remaining for current epoch: 0:08:01.617020\n",
            "Training loss after mini-batch 88500/101301: 1.272\n",
            "Time remaining for current epoch: 0:07:34.089703\n",
            "Training loss after mini-batch 89000/101301: 1.138\n",
            "Time remaining for current epoch: 0:07:57.216944\n",
            "Training loss after mini-batch 89500/101301: 1.136\n",
            "Time remaining for current epoch: 0:07:14.439264\n",
            "Training loss after mini-batch 90000/101301: 1.068\n",
            "Time remaining for current epoch: 0:07:08.720371\n",
            "Training loss after mini-batch 90500/101301: 1.134\n",
            "Time remaining for current epoch: 0:06:28.291575\n",
            "Training loss after mini-batch 91000/101301: 1.008\n",
            "Time remaining for current epoch: 0:05:20.892994\n",
            "Training loss after mini-batch 91500/101301: 1.117\n",
            "Time remaining for current epoch: 0:06:09.030518\n",
            "Training loss after mini-batch 92000/101301: 1.405\n",
            "Time remaining for current epoch: 0:06:12.810974\n",
            "Training loss after mini-batch 92500/101301: 1.088\n",
            "Time remaining for current epoch: 0:05:22.660232\n",
            "Training loss after mini-batch 93000/101301: 1.132\n",
            "Time remaining for current epoch: 0:04:36.386233\n",
            "Training loss after mini-batch 93500/101301: 1.361\n",
            "Time remaining for current epoch: 0:04:40.590768\n",
            "Training loss after mini-batch 94000/101301: 1.432\n",
            "Time remaining for current epoch: 0:04:32.545031\n",
            "Training loss after mini-batch 94500/101301: 1.262\n",
            "Time remaining for current epoch: 0:04:03.357676\n",
            "Training loss after mini-batch 95000/101301: 1.102\n",
            "Time remaining for current epoch: 0:04:10.478562\n",
            "Training loss after mini-batch 95500/101301: 1.049\n",
            "Time remaining for current epoch: 0:03:31.047470\n",
            "Training loss after mini-batch 96000/101301: 1.225\n",
            "Time remaining for current epoch: 0:03:01.923216\n",
            "Training loss after mini-batch 96500/101301: 1.227\n",
            "Time remaining for current epoch: 0:02:42.025591\n",
            "Training loss after mini-batch 97000/101301: 1.268\n",
            "Time remaining for current epoch: 0:02:24.204387\n",
            "Training loss after mini-batch 97500/101301: 1.143\n",
            "Time remaining for current epoch: 0:02:09.770790\n",
            "Training loss after mini-batch 98000/101301: 1.100\n",
            "Time remaining for current epoch: 0:01:48.741532\n",
            "Training loss after mini-batch 98500/101301: 1.202\n",
            "Time remaining for current epoch: 0:01:41.330018\n",
            "Training loss after mini-batch 99000/101301: 1.205\n",
            "Time remaining for current epoch: 0:01:19.467323\n",
            "Training loss after mini-batch 99500/101301: 1.212\n",
            "Time remaining for current epoch: 0:01:04.908478\n",
            "Training loss after mini-batch 100000/101301: 1.130\n",
            "Time remaining for current epoch: 0:00:45.283980\n",
            "Training loss after mini-batch 100500/101301: 1.122\n",
            "Time remaining for current epoch: 0:00:26.308042\n",
            "Training loss after mini-batch 101000/101301: 1.238\n",
            "Time remaining for current epoch: 0:00:10.816040\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 6 complete. Training loss: 1.04074. Validation loss: 0.00004. Validation F1 score: 47.95216\n",
            "F1 score increased (47.478156--->47.952163) \t Saving The Model...\n",
            "Starting epoch 7\n",
            "Training loss after mini-batch   500/101301: 0.956\n",
            "Time remaining for current epoch: 0:26:23.958097\n",
            "Training loss after mini-batch  1000/101301: 1.126\n",
            "Time remaining for current epoch: 0:25:03.823848\n",
            "Training loss after mini-batch  1500/101301: 1.184\n",
            "Time remaining for current epoch: 0:30:07.001660\n",
            "Training loss after mini-batch  2000/101301: 1.085\n",
            "Time remaining for current epoch: 0:26:41.387391\n",
            "Training loss after mini-batch  2500/101301: 0.947\n",
            "Time remaining for current epoch: 0:25:11.026704\n",
            "Training loss after mini-batch  3000/101301: 0.928\n",
            "Time remaining for current epoch: 0:24:27.856516\n",
            "Training loss after mini-batch  3500/101301: 0.845\n",
            "Time remaining for current epoch: 0:21:38.942989\n",
            "Training loss after mini-batch  4000/101301: 1.195\n",
            "Time remaining for current epoch: 0:23:20.331746\n",
            "Training loss after mini-batch  4500/101301: 1.034\n",
            "Time remaining for current epoch: 0:23:05.263293\n",
            "Training loss after mini-batch  5000/101301: 1.006\n",
            "Time remaining for current epoch: 0:22:24.911992\n",
            "Training loss after mini-batch  5500/101301: 0.959\n",
            "Time remaining for current epoch: 0:23:07.498563\n",
            "Training loss after mini-batch  6000/101301: 0.985\n",
            "Time remaining for current epoch: 0:23:01.106919\n",
            "Training loss after mini-batch  6500/101301: 0.995\n",
            "Time remaining for current epoch: 0:23:43.404771\n",
            "Training loss after mini-batch  7000/101301: 1.111\n",
            "Time remaining for current epoch: 0:24:10.819468\n",
            "Training loss after mini-batch  7500/101301: 1.015\n",
            "Time remaining for current epoch: 0:29:50.608818\n",
            "Training loss after mini-batch  8000/101301: 1.066\n",
            "Time remaining for current epoch: 0:26:45.638586\n",
            "Training loss after mini-batch  8500/101301: 0.900\n",
            "Time remaining for current epoch: 0:25:01.918688\n",
            "Training loss after mini-batch  9000/101301: 0.837\n",
            "Time remaining for current epoch: 0:21:37.945446\n",
            "Training loss after mini-batch  9500/101301: 0.769\n",
            "Time remaining for current epoch: 0:20:19.287362\n",
            "Training loss after mini-batch 10000/101301: 1.151\n",
            "Time remaining for current epoch: 0:22:10.743055\n",
            "Training loss after mini-batch 10500/101301: 1.356\n",
            "Time remaining for current epoch: 0:26:06.909254\n",
            "Training loss after mini-batch 11000/101301: 1.175\n",
            "Time remaining for current epoch: 0:20:29.548206\n",
            "Training loss after mini-batch 11500/101301: 0.921\n",
            "Time remaining for current epoch: 0:19:59.806716\n",
            "Training loss after mini-batch 12000/101301: 0.944\n",
            "Time remaining for current epoch: 0:21:56.770615\n",
            "Training loss after mini-batch 12500/101301: 0.966\n",
            "Time remaining for current epoch: 0:21:54.302369\n",
            "Training loss after mini-batch 13000/101301: 0.692\n",
            "Time remaining for current epoch: 0:19:49.811630\n",
            "Training loss after mini-batch 13500/101301: 0.812\n",
            "Time remaining for current epoch: 0:20:32.932105\n",
            "Training loss after mini-batch 14000/101301: 0.738\n",
            "Time remaining for current epoch: 0:19:24.889499\n",
            "Training loss after mini-batch 14500/101301: 0.813\n",
            "Time remaining for current epoch: 0:20:09.031253\n",
            "Training loss after mini-batch 15000/101301: 0.829\n",
            "Time remaining for current epoch: 0:23:50.293835\n",
            "Training loss after mini-batch 15500/101301: 0.992\n",
            "Time remaining for current epoch: 0:24:43.929734\n",
            "Training loss after mini-batch 16000/101301: 1.013\n",
            "Time remaining for current epoch: 0:26:33.638485\n",
            "Training loss after mini-batch 16500/101301: 1.119\n",
            "Time remaining for current epoch: 0:25:53.820203\n",
            "Training loss after mini-batch 17000/101301: 1.146\n",
            "Time remaining for current epoch: 0:24:05.350371\n",
            "Training loss after mini-batch 17500/101301: 1.048\n",
            "Time remaining for current epoch: 0:24:48.309336\n",
            "Training loss after mini-batch 18000/101301: 0.833\n",
            "Time remaining for current epoch: 0:18:52.660390\n",
            "Training loss after mini-batch 18500/101301: 0.899\n",
            "Time remaining for current epoch: 0:19:04.911549\n",
            "Training loss after mini-batch 19000/101301: 1.067\n",
            "Time remaining for current epoch: 0:21:05.406209\n",
            "Training loss after mini-batch 19500/101301: 0.985\n",
            "Time remaining for current epoch: 0:21:22.672392\n",
            "Training loss after mini-batch 20000/101301: 1.018\n",
            "Time remaining for current epoch: 0:21:42.373129\n",
            "Training loss after mini-batch 20500/101301: 1.124\n",
            "Time remaining for current epoch: 0:24:26.920904\n",
            "Training loss after mini-batch 21000/101301: 1.063\n",
            "Time remaining for current epoch: 0:24:43.814757\n",
            "Training loss after mini-batch 21500/101301: 0.643\n",
            "Time remaining for current epoch: 0:23:34.820551\n",
            "Training loss after mini-batch 22000/101301: 0.830\n",
            "Time remaining for current epoch: 0:23:13.709010\n",
            "Training loss after mini-batch 22500/101301: 0.938\n",
            "Time remaining for current epoch: 0:24:05.082709\n",
            "Training loss after mini-batch 23000/101301: 1.038\n",
            "Time remaining for current epoch: 0:19:26.589325\n",
            "Training loss after mini-batch 23500/101301: 0.931\n",
            "Time remaining for current epoch: 0:18:21.325814\n",
            "Training loss after mini-batch 24000/101301: 0.876\n",
            "Time remaining for current epoch: 0:17:16.854282\n",
            "Training loss after mini-batch 24500/101301: 0.771\n",
            "Time remaining for current epoch: 0:17:52.053046\n",
            "Training loss after mini-batch 25000/101301: 1.132\n",
            "Time remaining for current epoch: 0:20:25.128968\n",
            "Training loss after mini-batch 25500/101301: 1.085\n",
            "Time remaining for current epoch: 0:19:05.129854\n",
            "Training loss after mini-batch 26000/101301: 0.904\n",
            "Time remaining for current epoch: 0:16:51.722180\n",
            "Training loss after mini-batch 26500/101301: 0.937\n",
            "Time remaining for current epoch: 0:17:38.758093\n",
            "Training loss after mini-batch 27000/101301: 1.017\n",
            "Time remaining for current epoch: 0:19:30.028067\n",
            "Training loss after mini-batch 27500/101301: 0.935\n",
            "Time remaining for current epoch: 0:17:15.112171\n",
            "Training loss after mini-batch 28000/101301: 0.934\n",
            "Time remaining for current epoch: 0:16:23.615434\n",
            "Training loss after mini-batch 28500/101301: 1.067\n",
            "Time remaining for current epoch: 0:19:53.543234\n",
            "Training loss after mini-batch 29000/101301: 1.123\n",
            "Time remaining for current epoch: 0:17:48.297927\n",
            "Training loss after mini-batch 29500/101301: 0.722\n",
            "Time remaining for current epoch: 0:15:33.197441\n",
            "Training loss after mini-batch 30000/101301: 0.767\n",
            "Time remaining for current epoch: 0:14:21.153492\n",
            "Training loss after mini-batch 30500/101301: 0.949\n",
            "Time remaining for current epoch: 0:19:23.461091\n",
            "Training loss after mini-batch 31000/101301: 1.014\n",
            "Time remaining for current epoch: 0:18:59.884867\n",
            "Training loss after mini-batch 31500/101301: 0.934\n",
            "Time remaining for current epoch: 0:14:50.889335\n",
            "Training loss after mini-batch 32000/101301: 0.795\n",
            "Time remaining for current epoch: 0:14:17.154292\n",
            "Training loss after mini-batch 32500/101301: 0.924\n",
            "Time remaining for current epoch: 0:14:45.538839\n",
            "Training loss after mini-batch 33000/101301: 0.969\n",
            "Time remaining for current epoch: 0:15:05.795557\n",
            "Training loss after mini-batch 33500/101301: 0.934\n",
            "Time remaining for current epoch: 0:16:05.712276\n",
            "Training loss after mini-batch 34000/101301: 0.991\n",
            "Time remaining for current epoch: 0:18:56.800753\n",
            "Training loss after mini-batch 34500/101301: 0.964\n",
            "Time remaining for current epoch: 0:18:51.819589\n",
            "Training loss after mini-batch 35000/101301: 0.852\n",
            "Time remaining for current epoch: 0:16:25.214185\n",
            "Training loss after mini-batch 35500/101301: 0.874\n",
            "Time remaining for current epoch: 0:14:11.281580\n",
            "Training loss after mini-batch 36000/101301: 0.953\n",
            "Time remaining for current epoch: 0:18:36.322574\n",
            "Training loss after mini-batch 36500/101301: 1.093\n",
            "Time remaining for current epoch: 0:18:06.116902\n",
            "Training loss after mini-batch 37000/101301: 1.175\n",
            "Time remaining for current epoch: 0:18:02.655645\n",
            "Training loss after mini-batch 37500/101301: 1.026\n",
            "Time remaining for current epoch: 0:16:57.743552\n",
            "Training loss after mini-batch 38000/101301: 1.042\n",
            "Time remaining for current epoch: 0:14:39.992117\n",
            "Training loss after mini-batch 38500/101301: 1.020\n",
            "Time remaining for current epoch: 0:16:34.147312\n",
            "Training loss after mini-batch 39000/101301: 1.186\n",
            "Time remaining for current epoch: 0:15:13.748347\n",
            "Training loss after mini-batch 39500/101301: 0.870\n",
            "Time remaining for current epoch: 0:13:54.228482\n",
            "Training loss after mini-batch 40000/101301: 0.801\n",
            "Time remaining for current epoch: 0:13:43.844988\n",
            "Training loss after mini-batch 40500/101301: 1.000\n",
            "Time remaining for current epoch: 0:16:31.222292\n",
            "Training loss after mini-batch 41000/101301: 0.953\n",
            "Time remaining for current epoch: 0:13:42.151271\n",
            "Training loss after mini-batch 41500/101301: 0.863\n",
            "Time remaining for current epoch: 0:13:26.217000\n",
            "Training loss after mini-batch 42000/101301: 0.819\n",
            "Time remaining for current epoch: 0:13:12.809431\n",
            "Training loss after mini-batch 42500/101301: 0.878\n",
            "Time remaining for current epoch: 0:13:17.896663\n",
            "Training loss after mini-batch 43000/101301: 0.852\n",
            "Time remaining for current epoch: 0:14:05.101817\n",
            "Training loss after mini-batch 43500/101301: 0.806\n",
            "Time remaining for current epoch: 0:15:45.498025\n",
            "Training loss after mini-batch 44000/101301: 0.935\n",
            "Time remaining for current epoch: 0:14:03.619620\n",
            "Training loss after mini-batch 44500/101301: 0.989\n",
            "Time remaining for current epoch: 0:16:29.373590\n",
            "Training loss after mini-batch 45000/101301: 1.049\n",
            "Time remaining for current epoch: 0:16:21.560118\n",
            "Training loss after mini-batch 45500/101301: 1.005\n",
            "Time remaining for current epoch: 0:13:59.979237\n",
            "Training loss after mini-batch 46000/101301: 0.880\n",
            "Time remaining for current epoch: 0:12:53.625748\n",
            "Training loss after mini-batch 46500/101301: 0.757\n",
            "Time remaining for current epoch: 0:12:27.969974\n",
            "Training loss after mini-batch 47000/101301: 0.871\n",
            "Time remaining for current epoch: 0:13:05.018618\n",
            "Training loss after mini-batch 47500/101301: 0.883\n",
            "Time remaining for current epoch: 0:12:10.727633\n",
            "Training loss after mini-batch 48000/101301: 1.095\n",
            "Time remaining for current epoch: 0:12:37.898425\n",
            "Training loss after mini-batch 48500/101301: 0.969\n",
            "Time remaining for current epoch: 0:13:08.477036\n",
            "Training loss after mini-batch 49000/101301: 0.863\n",
            "Time remaining for current epoch: 0:11:51.095878\n",
            "Training loss after mini-batch 49500/101301: 0.840\n",
            "Time remaining for current epoch: 0:11:19.314354\n",
            "Training loss after mini-batch 50000/101301: 0.588\n",
            "Time remaining for current epoch: 0:10:58.696237\n",
            "Training loss after mini-batch 50500/101301: 0.651\n",
            "Time remaining for current epoch: 0:12:09.846417\n",
            "Training loss after mini-batch 51000/101301: 1.023\n",
            "Time remaining for current epoch: 0:13:00.274345\n",
            "Training loss after mini-batch 51500/101301: 1.006\n",
            "Time remaining for current epoch: 0:11:30.937392\n",
            "Training loss after mini-batch 52000/101301: 0.777\n",
            "Time remaining for current epoch: 0:12:10.406002\n",
            "Training loss after mini-batch 52500/101301: 0.804\n",
            "Time remaining for current epoch: 0:11:04.086139\n",
            "Training loss after mini-batch 53000/101301: 0.867\n",
            "Time remaining for current epoch: 0:11:01.760772\n",
            "Training loss after mini-batch 53500/101301: 0.888\n",
            "Time remaining for current epoch: 0:11:42.197354\n",
            "Training loss after mini-batch 54000/101301: 0.829\n",
            "Time remaining for current epoch: 0:12:22.173263\n",
            "Training loss after mini-batch 54500/101301: 0.962\n",
            "Time remaining for current epoch: 0:12:21.545433\n",
            "Training loss after mini-batch 55000/101301: 1.101\n",
            "Time remaining for current epoch: 0:12:05.544710\n",
            "Training loss after mini-batch 55500/101301: 1.005\n",
            "Time remaining for current epoch: 0:12:40.050550\n",
            "Training loss after mini-batch 56000/101301: 0.965\n",
            "Time remaining for current epoch: 0:10:38.297448\n",
            "Training loss after mini-batch 56500/101301: 0.897\n",
            "Time remaining for current epoch: 0:10:18.524090\n",
            "Training loss after mini-batch 57000/101301: 1.002\n",
            "Time remaining for current epoch: 0:10:06.022780\n",
            "Training loss after mini-batch 57500/101301: 1.076\n",
            "Time remaining for current epoch: 0:09:35.543150\n",
            "Training loss after mini-batch 58000/101301: 1.140\n",
            "Time remaining for current epoch: 0:11:59.919532\n",
            "Training loss after mini-batch 58500/101301: 1.044\n",
            "Time remaining for current epoch: 0:11:20.777487\n",
            "Training loss after mini-batch 59000/101301: 0.957\n",
            "Time remaining for current epoch: 0:10:07.302663\n",
            "Training loss after mini-batch 59500/101301: 0.815\n",
            "Time remaining for current epoch: 0:09:44.631200\n",
            "Training loss after mini-batch 60000/101301: 0.764\n",
            "Time remaining for current epoch: 0:09:36.487342\n",
            "Training loss after mini-batch 60500/101301: 0.971\n",
            "Time remaining for current epoch: 0:10:21.950909\n",
            "Training loss after mini-batch 61000/101301: 0.936\n",
            "Time remaining for current epoch: 0:10:07.212704\n",
            "Training loss after mini-batch 61500/101301: 0.689\n",
            "Time remaining for current epoch: 0:09:04.420339\n",
            "Training loss after mini-batch 62000/101301: 0.935\n",
            "Time remaining for current epoch: 0:08:47.668592\n",
            "Training loss after mini-batch 62500/101301: 0.891\n",
            "Time remaining for current epoch: 0:13:22.882870\n",
            "Training loss after mini-batch 63000/101301: 0.939\n",
            "Time remaining for current epoch: 0:13:17.282345\n",
            "Training loss after mini-batch 63500/101301: 1.049\n",
            "Time remaining for current epoch: 0:09:20.332513\n",
            "Training loss after mini-batch 64000/101301: 0.926\n",
            "Time remaining for current epoch: 0:08:32.131152\n",
            "Training loss after mini-batch 64500/101301: 0.913\n",
            "Time remaining for current epoch: 0:08:48.286516\n",
            "Training loss after mini-batch 65000/101301: 0.792\n",
            "Time remaining for current epoch: 0:08:36.438982\n",
            "Training loss after mini-batch 65500/101301: 0.702\n",
            "Time remaining for current epoch: 0:08:31.326409\n",
            "Training loss after mini-batch 66000/101301: 0.960\n",
            "Time remaining for current epoch: 0:09:16.527583\n",
            "Training loss after mini-batch 66500/101301: 0.857\n",
            "Time remaining for current epoch: 0:09:13.899101\n",
            "Training loss after mini-batch 67000/101301: 0.659\n",
            "Time remaining for current epoch: 0:09:19.463554\n",
            "Training loss after mini-batch 67500/101301: 0.947\n",
            "Time remaining for current epoch: 0:10:57.749941\n",
            "Training loss after mini-batch 68000/101301: 0.970\n",
            "Time remaining for current epoch: 0:08:02.958393\n",
            "Training loss after mini-batch 68500/101301: 0.881\n",
            "Time remaining for current epoch: 0:08:23.058439\n",
            "Training loss after mini-batch 69000/101301: 0.901\n",
            "Time remaining for current epoch: 0:08:10.410714\n",
            "Training loss after mini-batch 69500/101301: 1.007\n",
            "Time remaining for current epoch: 0:08:07.763596\n",
            "Training loss after mini-batch 70000/101301: 0.961\n",
            "Time remaining for current epoch: 0:09:55.313736\n",
            "Training loss after mini-batch 70500/101301: 0.811\n",
            "Time remaining for current epoch: 0:09:35.330187\n",
            "Training loss after mini-batch 71000/101301: 0.720\n",
            "Time remaining for current epoch: 0:08:02.412728\n",
            "Training loss after mini-batch 71500/101301: 0.691\n",
            "Time remaining for current epoch: 0:06:57.243571\n",
            "Training loss after mini-batch 72000/101301: 0.754\n",
            "Time remaining for current epoch: 0:06:33.928170\n",
            "Training loss after mini-batch 72500/101301: 0.817\n",
            "Time remaining for current epoch: 0:06:13.950405\n",
            "Training loss after mini-batch 73000/101301: 0.728\n",
            "Time remaining for current epoch: 0:06:28.474959\n",
            "Training loss after mini-batch 73500/101301: 0.836\n",
            "Time remaining for current epoch: 0:07:03.829039\n",
            "Training loss after mini-batch 74000/101301: 0.896\n",
            "Time remaining for current epoch: 0:07:18.458237\n",
            "Training loss after mini-batch 74500/101301: 0.767\n",
            "Time remaining for current epoch: 0:07:25.505078\n",
            "Training loss after mini-batch 75000/101301: 1.003\n",
            "Time remaining for current epoch: 0:06:57.285714\n",
            "Training loss after mini-batch 75500/101301: 0.939\n",
            "Time remaining for current epoch: 0:06:51.875002\n",
            "Training loss after mini-batch 76000/101301: 0.754\n",
            "Time remaining for current epoch: 0:05:36.698496\n",
            "Training loss after mini-batch 76500/101301: 0.717\n",
            "Time remaining for current epoch: 0:05:22.720099\n",
            "Training loss after mini-batch 77000/101301: 0.787\n",
            "Time remaining for current epoch: 0:06:54.922931\n",
            "Training loss after mini-batch 77500/101301: 0.857\n",
            "Time remaining for current epoch: 0:07:23.899740\n",
            "Training loss after mini-batch 78000/101301: 0.883\n",
            "Time remaining for current epoch: 0:05:42.462764\n",
            "Training loss after mini-batch 78500/101301: 0.825\n",
            "Time remaining for current epoch: 0:05:26.702460\n",
            "Training loss after mini-batch 79000/101301: 0.957\n",
            "Time remaining for current epoch: 0:05:03.156750\n",
            "Training loss after mini-batch 79500/101301: 0.807\n",
            "Time remaining for current epoch: 0:04:55.382722\n",
            "Training loss after mini-batch 80000/101301: 0.751\n",
            "Time remaining for current epoch: 0:04:56.655046\n",
            "Training loss after mini-batch 80500/101301: 0.983\n",
            "Time remaining for current epoch: 0:05:27.787720\n",
            "Training loss after mini-batch 81000/101301: 0.807\n",
            "Time remaining for current epoch: 0:05:22.934688\n",
            "Training loss after mini-batch 81500/101301: 0.596\n",
            "Time remaining for current epoch: 0:04:39.999195\n",
            "Training loss after mini-batch 82000/101301: 0.653\n",
            "Time remaining for current epoch: 0:04:29.231460\n",
            "Training loss after mini-batch 82500/101301: 0.514\n",
            "Time remaining for current epoch: 0:04:01.672242\n",
            "Training loss after mini-batch 83000/101301: 0.623\n",
            "Time remaining for current epoch: 0:04:02.231575\n",
            "Training loss after mini-batch 83500/101301: 0.610\n",
            "Time remaining for current epoch: 0:04:22.003902\n",
            "Training loss after mini-batch 84000/101301: 0.535\n",
            "Time remaining for current epoch: 0:04:01.040430\n",
            "Training loss after mini-batch 84500/101301: 0.567\n",
            "Time remaining for current epoch: 0:03:43.829059\n",
            "Training loss after mini-batch 85000/101301: 0.637\n",
            "Time remaining for current epoch: 0:03:53.160703\n",
            "Training loss after mini-batch 85500/101301: 0.833\n",
            "Time remaining for current epoch: 0:04:23.610563\n",
            "Training loss after mini-batch 86000/101301: 0.884\n",
            "Time remaining for current epoch: 0:04:53.284420\n",
            "Training loss after mini-batch 86500/101301: 0.779\n",
            "Time remaining for current epoch: 0:05:04.784542\n",
            "Training loss after mini-batch 87000/101301: 0.762\n",
            "Time remaining for current epoch: 0:03:37.405584\n",
            "Training loss after mini-batch 87500/101301: 0.844\n",
            "Time remaining for current epoch: 0:03:54.265722\n",
            "Training loss after mini-batch 88000/101301: 0.769\n",
            "Time remaining for current epoch: 0:03:25.650008\n",
            "Training loss after mini-batch 88500/101301: 0.725\n",
            "Time remaining for current epoch: 0:03:17.440058\n",
            "Training loss after mini-batch 89000/101301: 0.709\n",
            "Time remaining for current epoch: 0:02:48.059074\n",
            "Training loss after mini-batch 89500/101301: 0.742\n",
            "Time remaining for current epoch: 0:02:55.701650\n",
            "Training loss after mini-batch 90000/101301: 0.797\n",
            "Time remaining for current epoch: 0:02:51.439140\n",
            "Training loss after mini-batch 90500/101301: 0.627\n",
            "Time remaining for current epoch: 0:02:25.973907\n",
            "Training loss after mini-batch 91000/101301: 0.664\n",
            "Time remaining for current epoch: 0:02:36.478910\n",
            "Training loss after mini-batch 91500/101301: 0.725\n",
            "Time remaining for current epoch: 0:02:29.984235\n",
            "Training loss after mini-batch 92000/101301: 0.758\n",
            "Time remaining for current epoch: 0:02:37.596262\n",
            "Training loss after mini-batch 92500/101301: 0.601\n",
            "Time remaining for current epoch: 0:01:57.099263\n",
            "Training loss after mini-batch 93000/101301: 0.557\n",
            "Time remaining for current epoch: 0:01:48.016935\n",
            "Training loss after mini-batch 93500/101301: 0.496\n",
            "Time remaining for current epoch: 0:01:40.097275\n",
            "Training loss after mini-batch 94000/101301: 0.496\n",
            "Time remaining for current epoch: 0:01:33.656901\n",
            "Training loss after mini-batch 94500/101301: 0.731\n",
            "Time remaining for current epoch: 0:01:40.786271\n",
            "Training loss after mini-batch 95000/101301: 0.664\n",
            "Time remaining for current epoch: 0:01:31.393022\n",
            "Training loss after mini-batch 95500/101301: 0.549\n",
            "Time remaining for current epoch: 0:01:16.944110\n",
            "Training loss after mini-batch 96000/101301: 0.641\n",
            "Time remaining for current epoch: 0:01:29.266136\n",
            "Training loss after mini-batch 96500/101301: 0.739\n",
            "Time remaining for current epoch: 0:01:35.202914\n",
            "Training loss after mini-batch 97000/101301: 0.655\n",
            "Time remaining for current epoch: 0:01:05.223038\n",
            "Training loss after mini-batch 97500/101301: 0.694\n",
            "Time remaining for current epoch: 0:00:57.917584\n",
            "Training loss after mini-batch 98000/101301: 0.787\n",
            "Time remaining for current epoch: 0:00:52.091795\n",
            "Training loss after mini-batch 98500/101301: 0.724\n",
            "Time remaining for current epoch: 0:00:50.653421\n",
            "Training loss after mini-batch 99000/101301: 0.665\n",
            "Time remaining for current epoch: 0:00:34.126540\n",
            "Training loss after mini-batch 99500/101301: 0.625\n",
            "Time remaining for current epoch: 0:00:24.191767\n",
            "Training loss after mini-batch 100000/101301: 0.646\n",
            "Time remaining for current epoch: 0:00:18.155320\n",
            "Training loss after mini-batch 100500/101301: 0.485\n",
            "Time remaining for current epoch: 0:00:11.110077\n",
            "Training loss after mini-batch 101000/101301: 0.438\n",
            "Time remaining for current epoch: 0:00:04.216564\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 7 complete. Training loss: 0.87365. Validation loss: 0.00004. Validation F1 score: 48.10507\n",
            "F1 score increased (47.952163--->48.105068) \t Saving The Model...\n",
            "Starting epoch 8\n",
            "Training loss after mini-batch   500/101301: 1.033\n",
            "Time remaining for current epoch: 0:27:52.404251\n",
            "Training loss after mini-batch  1000/101301: 0.734\n",
            "Time remaining for current epoch: 0:24:47.235352\n",
            "Training loss after mini-batch  1500/101301: 0.770\n",
            "Time remaining for current epoch: 0:23:47.999544\n",
            "Training loss after mini-batch  2000/101301: 0.950\n",
            "Time remaining for current epoch: 0:29:54.142468\n",
            "Training loss after mini-batch  2500/101301: 1.058\n",
            "Time remaining for current epoch: 0:24:35.297820\n",
            "Training loss after mini-batch  3000/101301: 0.904\n",
            "Time remaining for current epoch: 0:25:17.582436\n",
            "Training loss after mini-batch  3500/101301: 0.834\n",
            "Time remaining for current epoch: 0:22:12.285349\n",
            "Training loss after mini-batch  4000/101301: 0.885\n",
            "Time remaining for current epoch: 0:21:30.677987\n",
            "Training loss after mini-batch  4500/101301: 0.995\n",
            "Time remaining for current epoch: 0:26:23.182342\n",
            "Training loss after mini-batch  5000/101301: 0.865\n",
            "Time remaining for current epoch: 0:28:15.141931\n",
            "Training loss after mini-batch  5500/101301: 1.156\n",
            "Time remaining for current epoch: 0:26:59.124298\n",
            "Training loss after mini-batch  6000/101301: 1.046\n",
            "Time remaining for current epoch: 0:29:31.561126\n",
            "Training loss after mini-batch  6500/101301: 1.095\n",
            "Time remaining for current epoch: 0:27:51.884955\n",
            "Training loss after mini-batch  7000/101301: 0.832\n",
            "Time remaining for current epoch: 0:28:07.384148\n",
            "Training loss after mini-batch  7500/101301: 0.963\n",
            "Time remaining for current epoch: 0:25:42.733778\n",
            "Training loss after mini-batch  8000/101301: 0.806\n",
            "Time remaining for current epoch: 0:22:19.611869\n",
            "Training loss after mini-batch  8500/101301: 0.871\n",
            "Time remaining for current epoch: 0:24:19.172698\n",
            "Training loss after mini-batch  9000/101301: 0.878\n",
            "Time remaining for current epoch: 0:24:53.458332\n",
            "Training loss after mini-batch  9500/101301: 1.124\n",
            "Time remaining for current epoch: 0:22:28.993867\n",
            "Training loss after mini-batch 10000/101301: 1.217\n",
            "Time remaining for current epoch: 0:23:12.953712\n",
            "Training loss after mini-batch 10500/101301: 1.261\n",
            "Time remaining for current epoch: 0:22:27.312842\n",
            "Training loss after mini-batch 11000/101301: 1.065\n",
            "Time remaining for current epoch: 0:21:30.482065\n",
            "Training loss after mini-batch 11500/101301: 1.183\n",
            "Time remaining for current epoch: 0:20:26.432603\n",
            "Training loss after mini-batch 12000/101301: 0.924\n",
            "Time remaining for current epoch: 0:19:31.191835\n",
            "Training loss after mini-batch 12500/101301: 0.986\n",
            "Time remaining for current epoch: 0:19:43.784268\n",
            "Training loss after mini-batch 13000/101301: 0.848\n",
            "Time remaining for current epoch: 0:19:40.912865\n",
            "Training loss after mini-batch 13500/101301: 0.728\n",
            "Time remaining for current epoch: 0:19:44.555921\n",
            "Training loss after mini-batch 14000/101301: 0.829\n",
            "Time remaining for current epoch: 0:23:42.763873\n",
            "Training loss after mini-batch 14500/101301: 0.938\n",
            "Time remaining for current epoch: 0:25:05.728762\n",
            "Training loss after mini-batch 15000/101301: 0.827\n",
            "Time remaining for current epoch: 0:20:30.518661\n",
            "Training loss after mini-batch 15500/101301: 0.867\n",
            "Time remaining for current epoch: 0:20:02.796151\n",
            "Training loss after mini-batch 16000/101301: 0.977\n",
            "Time remaining for current epoch: 0:21:16.269160\n",
            "Training loss after mini-batch 16500/101301: 0.933\n",
            "Time remaining for current epoch: 0:19:52.879445\n",
            "Training loss after mini-batch 17000/101301: 0.866\n",
            "Time remaining for current epoch: 0:23:26.983735\n",
            "Training loss after mini-batch 17500/101301: 1.007\n",
            "Time remaining for current epoch: 0:20:39.794403\n",
            "Training loss after mini-batch 18000/101301: 0.859\n",
            "Time remaining for current epoch: 0:20:54.082440\n",
            "Training loss after mini-batch 18500/101301: 0.918\n",
            "Time remaining for current epoch: 0:20:19.250166\n",
            "Training loss after mini-batch 19000/101301: 0.847\n",
            "Time remaining for current epoch: 0:19:28.171623\n",
            "Training loss after mini-batch 19500/101301: 0.614\n",
            "Time remaining for current epoch: 0:19:00.679698\n",
            "Training loss after mini-batch 20000/101301: 0.790\n",
            "Time remaining for current epoch: 0:19:52.658578\n",
            "Training loss after mini-batch 20500/101301: 0.901\n",
            "Time remaining for current epoch: 0:20:33.508011\n",
            "Training loss after mini-batch 21000/101301: 0.810\n",
            "Time remaining for current epoch: 0:17:24.603454\n",
            "Training loss after mini-batch 21500/101301: 0.929\n",
            "Time remaining for current epoch: 0:17:02.668465\n",
            "Training loss after mini-batch 22000/101301: 0.957\n",
            "Time remaining for current epoch: 0:17:21.047213\n",
            "Training loss after mini-batch 22500/101301: 0.894\n",
            "Time remaining for current epoch: 0:16:52.972841\n",
            "Training loss after mini-batch 23000/101301: 1.048\n",
            "Time remaining for current epoch: 0:20:21.109828\n",
            "Training loss after mini-batch 23500/101301: 1.006\n",
            "Time remaining for current epoch: 0:23:02.177195\n",
            "Training loss after mini-batch 24000/101301: 0.944\n",
            "Time remaining for current epoch: 0:22:59.240270\n",
            "Training loss after mini-batch 24500/101301: 0.927\n",
            "Time remaining for current epoch: 0:20:53.842046\n",
            "Training loss after mini-batch 25000/101301: 1.030\n",
            "Time remaining for current epoch: 0:19:10.985052\n",
            "Training loss after mini-batch 25500/101301: 0.974\n",
            "Time remaining for current epoch: 0:20:46.935436\n",
            "Training loss after mini-batch 26000/101301: 0.976\n",
            "Time remaining for current epoch: 0:19:12.328113\n",
            "Training loss after mini-batch 26500/101301: 0.943\n",
            "Time remaining for current epoch: 0:16:42.366578\n",
            "Training loss after mini-batch 27000/101301: 0.633\n",
            "Time remaining for current epoch: 0:16:36.999320\n",
            "Training loss after mini-batch 27500/101301: 0.759\n",
            "Time remaining for current epoch: 0:16:33.024896\n",
            "Training loss after mini-batch 28000/101301: 0.843\n",
            "Time remaining for current epoch: 0:17:46.204015\n",
            "Training loss after mini-batch 28500/101301: 0.773\n",
            "Time remaining for current epoch: 0:21:52.111742\n",
            "Training loss after mini-batch 29000/101301: 1.030\n",
            "Time remaining for current epoch: 0:16:31.410369\n",
            "Training loss after mini-batch 29500/101301: 0.899\n",
            "Time remaining for current epoch: 0:16:20.928636\n",
            "Training loss after mini-batch 30000/101301: 0.872\n",
            "Time remaining for current epoch: 0:16:29.721160\n",
            "Training loss after mini-batch 30500/101301: 0.850\n",
            "Time remaining for current epoch: 0:16:24.986124\n",
            "Training loss after mini-batch 31000/101301: 0.917\n",
            "Time remaining for current epoch: 0:17:54.920289\n",
            "Training loss after mini-batch 31500/101301: 0.896\n",
            "Time remaining for current epoch: 0:19:05.339641\n",
            "Training loss after mini-batch 32000/101301: 0.896\n",
            "Time remaining for current epoch: 0:19:54.976675\n",
            "Training loss after mini-batch 32500/101301: 0.757\n",
            "Time remaining for current epoch: 0:15:45.209227\n",
            "Training loss after mini-batch 33000/101301: 0.797\n",
            "Time remaining for current epoch: 0:15:47.959265\n",
            "Training loss after mini-batch 33500/101301: 0.860\n",
            "Time remaining for current epoch: 0:14:29.664653\n",
            "Training loss after mini-batch 34000/101301: 0.783\n",
            "Time remaining for current epoch: 0:14:34.898495\n",
            "Training loss after mini-batch 34500/101301: 0.918\n",
            "Time remaining for current epoch: 0:15:19.288324\n",
            "Training loss after mini-batch 35000/101301: 0.775\n",
            "Time remaining for current epoch: 0:15:39.851717\n",
            "Training loss after mini-batch 35500/101301: 0.906\n",
            "Time remaining for current epoch: 0:17:15.192023\n",
            "Training loss after mini-batch 36000/101301: 0.881\n",
            "Time remaining for current epoch: 0:17:38.417708\n",
            "Training loss after mini-batch 36500/101301: 1.068\n",
            "Time remaining for current epoch: 0:15:09.790897\n",
            "Training loss after mini-batch 37000/101301: 0.909\n",
            "Time remaining for current epoch: 0:15:48.369751\n",
            "Training loss after mini-batch 37500/101301: 0.855\n",
            "Time remaining for current epoch: 0:14:37.548932\n",
            "Training loss after mini-batch 38000/101301: 0.918\n",
            "Time remaining for current epoch: 0:16:07.126646\n",
            "Training loss after mini-batch 38500/101301: 0.875\n",
            "Time remaining for current epoch: 0:14:56.703215\n",
            "Training loss after mini-batch 39000/101301: 0.937\n",
            "Time remaining for current epoch: 0:15:27.216746\n",
            "Training loss after mini-batch 39500/101301: 0.842\n",
            "Time remaining for current epoch: 0:13:48.751695\n",
            "Training loss after mini-batch 40000/101301: 0.808\n",
            "Time remaining for current epoch: 0:13:47.616758\n",
            "Training loss after mini-batch 40500/101301: 0.709\n",
            "Time remaining for current epoch: 0:12:42.019570\n",
            "Training loss after mini-batch 41000/101301: 0.757\n",
            "Time remaining for current epoch: 0:12:58.113626\n",
            "Training loss after mini-batch 41500/101301: 0.891\n",
            "Time remaining for current epoch: 0:14:25.611827\n",
            "Training loss after mini-batch 42000/101301: 0.924\n",
            "Time remaining for current epoch: 0:15:23.371098\n",
            "Training loss after mini-batch 42500/101301: 0.843\n",
            "Time remaining for current epoch: 0:18:09.077905\n",
            "Training loss after mini-batch 43000/101301: 0.907\n",
            "Time remaining for current epoch: 0:16:13.630358\n",
            "Training loss after mini-batch 43500/101301: 0.858\n",
            "Time remaining for current epoch: 0:13:56.726274\n",
            "Training loss after mini-batch 44000/101301: 0.907\n",
            "Time remaining for current epoch: 0:14:31.121286\n",
            "Training loss after mini-batch 44500/101301: 0.947\n",
            "Time remaining for current epoch: 0:15:49.125834\n",
            "Training loss after mini-batch 45000/101301: 0.890\n",
            "Time remaining for current epoch: 0:13:40.648918\n",
            "Training loss after mini-batch 45500/101301: 0.928\n",
            "Time remaining for current epoch: 0:13:19.222634\n",
            "Training loss after mini-batch 46000/101301: 0.937\n",
            "Time remaining for current epoch: 0:12:32.962604\n",
            "Training loss after mini-batch 46500/101301: 0.871\n",
            "Time remaining for current epoch: 0:12:58.311979\n",
            "Training loss after mini-batch 47000/101301: 0.789\n",
            "Time remaining for current epoch: 0:12:07.817528\n",
            "Training loss after mini-batch 47500/101301: 0.640\n",
            "Time remaining for current epoch: 0:12:27.429439\n",
            "Training loss after mini-batch 48000/101301: 0.713\n",
            "Time remaining for current epoch: 0:12:00.039921\n",
            "Training loss after mini-batch 48500/101301: 0.645\n",
            "Time remaining for current epoch: 0:11:34.990287\n",
            "Training loss after mini-batch 49000/101301: 0.783\n",
            "Time remaining for current epoch: 0:12:34.649004\n",
            "Training loss after mini-batch 49500/101301: 0.882\n",
            "Time remaining for current epoch: 0:11:33.457966\n",
            "Training loss after mini-batch 50000/101301: 1.026\n",
            "Time remaining for current epoch: 0:11:51.694255\n",
            "Training loss after mini-batch 50500/101301: 1.032\n",
            "Time remaining for current epoch: 0:13:19.361736\n",
            "Training loss after mini-batch 51000/101301: 0.892\n",
            "Time remaining for current epoch: 0:12:13.065853\n",
            "Training loss after mini-batch 51500/101301: 0.505\n",
            "Time remaining for current epoch: 0:11:41.552690\n",
            "Training loss after mini-batch 52000/101301: 0.623\n",
            "Time remaining for current epoch: 0:11:20.262173\n",
            "Training loss after mini-batch 52500/101301: 0.843\n",
            "Time remaining for current epoch: 0:12:25.049042\n",
            "Training loss after mini-batch 53000/101301: 0.882\n",
            "Time remaining for current epoch: 0:13:00.999743\n",
            "Training loss after mini-batch 53500/101301: 0.988\n",
            "Time remaining for current epoch: 0:15:19.691411\n",
            "Training loss after mini-batch 54000/101301: 0.958\n",
            "Time remaining for current epoch: 0:12:06.154836\n",
            "Training loss after mini-batch 54500/101301: 1.066\n",
            "Time remaining for current epoch: 0:11:58.582603\n",
            "Training loss after mini-batch 55000/101301: 0.982\n",
            "Time remaining for current epoch: 0:12:57.743217\n",
            "Training loss after mini-batch 55500/101301: 0.995\n",
            "Time remaining for current epoch: 0:10:45.434266\n",
            "Training loss after mini-batch 56000/101301: 0.964\n",
            "Time remaining for current epoch: 0:11:19.010461\n",
            "Training loss after mini-batch 56500/101301: 0.949\n",
            "Time remaining for current epoch: 0:10:53.012079\n",
            "Training loss after mini-batch 57000/101301: 1.189\n",
            "Time remaining for current epoch: 0:10:41.982572\n",
            "Training loss after mini-batch 57500/101301: 0.961\n",
            "Time remaining for current epoch: 0:12:57.305905\n",
            "Training loss after mini-batch 58000/101301: 1.182\n",
            "Time remaining for current epoch: 0:11:59.643227\n",
            "Training loss after mini-batch 58500/101301: 1.061\n",
            "Time remaining for current epoch: 0:11:41.179287\n",
            "Training loss after mini-batch 59000/101301: 0.607\n",
            "Time remaining for current epoch: 0:09:51.626811\n",
            "Training loss after mini-batch 59500/101301: 0.537\n",
            "Time remaining for current epoch: 0:09:22.636738\n",
            "Training loss after mini-batch 60000/101301: 0.797\n",
            "Time remaining for current epoch: 0:09:27.907124\n",
            "Training loss after mini-batch 60500/101301: 0.899\n",
            "Time remaining for current epoch: 0:10:10.607711\n",
            "Training loss after mini-batch 61000/101301: 0.853\n",
            "Time remaining for current epoch: 0:10:15.278892\n",
            "Training loss after mini-batch 61500/101301: 0.833\n",
            "Time remaining for current epoch: 0:11:53.571495\n",
            "Training loss after mini-batch 62000/101301: 0.905\n",
            "Time remaining for current epoch: 0:08:42.675347\n",
            "Training loss after mini-batch 62500/101301: 1.049\n",
            "Time remaining for current epoch: 0:09:40.415412\n",
            "Training loss after mini-batch 63000/101301: 1.056\n",
            "Time remaining for current epoch: 0:10:09.047561\n",
            "Training loss after mini-batch 63500/101301: 1.302\n",
            "Time remaining for current epoch: 0:22:05.451006\n",
            "Training loss after mini-batch 64000/101301: 1.453\n",
            "Time remaining for current epoch: 0:24:24.326903\n",
            "Training loss after mini-batch 64500/101301: 1.180\n",
            "Time remaining for current epoch: 0:22:31.723350\n",
            "Training loss after mini-batch 65000/101301: 1.299\n",
            "Time remaining for current epoch: 0:21:01.184509\n",
            "Training loss after mini-batch 65500/101301: 1.352\n",
            "Time remaining for current epoch: 0:20:20.167148\n",
            "Training loss after mini-batch 66000/101301: 1.323\n",
            "Time remaining for current epoch: 0:21:30.828855\n",
            "Training loss after mini-batch 66500/101301: 1.348\n",
            "Time remaining for current epoch: 0:21:40.877613\n",
            "Training loss after mini-batch 67000/101301: 1.265\n",
            "Time remaining for current epoch: 0:21:45.612153\n",
            "Training loss after mini-batch 67500/101301: 1.301\n",
            "Time remaining for current epoch: 0:19:38.967376\n",
            "Training loss after mini-batch 68000/101301: 1.191\n",
            "Time remaining for current epoch: 0:17:09.093209\n",
            "Training loss after mini-batch 68500/101301: 1.288\n",
            "Time remaining for current epoch: 0:18:22.091278\n",
            "Training loss after mini-batch 69000/101301: 1.462\n",
            "Time remaining for current epoch: 0:22:30.029277\n",
            "Training loss after mini-batch 69500/101301: 1.369\n",
            "Time remaining for current epoch: 0:21:50.376549\n",
            "Training loss after mini-batch 70000/101301: 1.378\n",
            "Time remaining for current epoch: 0:19:29.247229\n",
            "Training loss after mini-batch 70500/101301: 1.295\n",
            "Time remaining for current epoch: 0:18:05.659759\n",
            "Training loss after mini-batch 71000/101301: 1.335\n",
            "Time remaining for current epoch: 0:17:43.535174\n",
            "Training loss after mini-batch 71500/101301: 1.346\n",
            "Time remaining for current epoch: 0:17:41.673144\n",
            "Training loss after mini-batch 72000/101301: 1.304\n",
            "Time remaining for current epoch: 0:17:44.641455\n",
            "Training loss after mini-batch 72500/101301: 1.236\n",
            "Time remaining for current epoch: 0:17:11.455753\n",
            "Training loss after mini-batch 73000/101301: 1.125\n",
            "Time remaining for current epoch: 0:16:07.146843\n",
            "Training loss after mini-batch 73500/101301: 1.190\n",
            "Time remaining for current epoch: 0:15:49.899775\n",
            "Training loss after mini-batch 74000/101301: 1.324\n",
            "Time remaining for current epoch: 0:17:30.747230\n",
            "Training loss after mini-batch 74500/101301: 1.405\n",
            "Time remaining for current epoch: 0:16:29.351770\n",
            "Training loss after mini-batch 75000/101301: 1.516\n",
            "Time remaining for current epoch: 0:17:07.637790\n",
            "Training loss after mini-batch 75500/101301: 1.265\n",
            "Time remaining for current epoch: 0:15:04.773544\n",
            "Training loss after mini-batch 76000/101301: 1.253\n",
            "Time remaining for current epoch: 0:15:56.655698\n",
            "Training loss after mini-batch 76500/101301: 1.392\n",
            "Time remaining for current epoch: 0:14:20.127271\n",
            "Training loss after mini-batch 77000/101301: 1.189\n",
            "Time remaining for current epoch: 0:14:11.420839\n",
            "Training loss after mini-batch 77500/101301: 1.295\n",
            "Time remaining for current epoch: 0:13:26.794693\n",
            "Training loss after mini-batch 78000/101301: 1.337\n",
            "Time remaining for current epoch: 0:14:18.590838\n",
            "Training loss after mini-batch 78500/101301: 1.348\n",
            "Time remaining for current epoch: 0:13:12.345688\n",
            "Training loss after mini-batch 79000/101301: 1.209\n",
            "Time remaining for current epoch: 0:13:27.311051\n",
            "Training loss after mini-batch 79500/101301: 1.646\n",
            "Time remaining for current epoch: 0:13:45.948822\n",
            "Training loss after mini-batch 80000/101301: 1.237\n",
            "Time remaining for current epoch: 0:13:22.595573\n",
            "Training loss after mini-batch 80500/101301: 1.246\n",
            "Time remaining for current epoch: 0:12:37.557881\n",
            "Training loss after mini-batch 81000/101301: 1.320\n",
            "Time remaining for current epoch: 0:12:42.370422\n",
            "Training loss after mini-batch 81500/101301: 1.214\n",
            "Time remaining for current epoch: 0:11:47.660533\n",
            "Training loss after mini-batch 82000/101301: 1.244\n",
            "Time remaining for current epoch: 0:11:51.584179\n",
            "Training loss after mini-batch 82500/101301: 1.174\n",
            "Time remaining for current epoch: 0:11:47.277095\n",
            "Training loss after mini-batch 83000/101301: 1.108\n",
            "Time remaining for current epoch: 0:10:39.261484\n",
            "Training loss after mini-batch 83500/101301: 1.252\n",
            "Time remaining for current epoch: 0:11:27.428179\n",
            "Training loss after mini-batch 84000/101301: 1.287\n",
            "Time remaining for current epoch: 0:11:31.808800\n",
            "Training loss after mini-batch 84500/101301: 1.194\n",
            "Time remaining for current epoch: 0:10:45.389545\n",
            "Training loss after mini-batch 85000/101301: 1.159\n",
            "Time remaining for current epoch: 0:09:04.775519\n",
            "Training loss after mini-batch 85500/101301: 1.136\n",
            "Time remaining for current epoch: 0:09:16.995836\n",
            "Training loss after mini-batch 86000/101301: 1.188\n",
            "Time remaining for current epoch: 0:09:11.882018\n",
            "Training loss after mini-batch 86500/101301: 1.192\n",
            "Time remaining for current epoch: 0:09:46.654824\n",
            "Training loss after mini-batch 87000/101301: 1.337\n",
            "Time remaining for current epoch: 0:10:01.868394\n",
            "Training loss after mini-batch 87500/101301: 1.184\n",
            "Time remaining for current epoch: 0:08:40.529777\n",
            "Training loss after mini-batch 88000/101301: 1.156\n",
            "Time remaining for current epoch: 0:07:53.151728\n",
            "Training loss after mini-batch 88500/101301: 1.261\n",
            "Time remaining for current epoch: 0:07:33.444743\n",
            "Training loss after mini-batch 89000/101301: 1.127\n",
            "Time remaining for current epoch: 0:07:46.096103\n",
            "Training loss after mini-batch 89500/101301: 1.125\n",
            "Time remaining for current epoch: 0:07:07.509146\n",
            "Training loss after mini-batch 90000/101301: 1.057\n",
            "Time remaining for current epoch: 0:07:10.551308\n",
            "Training loss after mini-batch 90500/101301: 1.124\n",
            "Time remaining for current epoch: 0:06:32.683241\n",
            "Training loss after mini-batch 91000/101301: 0.998\n",
            "Time remaining for current epoch: 0:05:19.954674\n",
            "Training loss after mini-batch 91500/101301: 1.108\n",
            "Time remaining for current epoch: 0:06:08.903914\n",
            "Training loss after mini-batch 92000/101301: 1.394\n",
            "Time remaining for current epoch: 0:06:12.231218\n",
            "Training loss after mini-batch 92500/101301: 1.080\n",
            "Time remaining for current epoch: 0:05:13.628845\n",
            "Training loss after mini-batch 93000/101301: 1.121\n",
            "Time remaining for current epoch: 0:04:33.394757\n",
            "Training loss after mini-batch 93500/101301: 1.348\n",
            "Time remaining for current epoch: 0:04:44.168939\n",
            "Training loss after mini-batch 94000/101301: 1.423\n",
            "Time remaining for current epoch: 0:04:31.012270\n",
            "Training loss after mini-batch 94500/101301: 1.253\n",
            "Time remaining for current epoch: 0:04:03.139502\n",
            "Training loss after mini-batch 95000/101301: 1.093\n",
            "Time remaining for current epoch: 0:04:10.487296\n",
            "Training loss after mini-batch 95500/101301: 1.040\n",
            "Time remaining for current epoch: 0:03:33.708965\n",
            "Training loss after mini-batch 96000/101301: 1.213\n",
            "Time remaining for current epoch: 0:03:01.129708\n",
            "Training loss after mini-batch 96500/101301: 1.216\n",
            "Time remaining for current epoch: 0:02:41.745287\n",
            "Training loss after mini-batch 97000/101301: 1.259\n",
            "Time remaining for current epoch: 0:02:23.279981\n",
            "Training loss after mini-batch 97500/101301: 1.133\n",
            "Time remaining for current epoch: 0:02:07.344233\n",
            "Training loss after mini-batch 98000/101301: 1.090\n",
            "Time remaining for current epoch: 0:01:47.035933\n",
            "Training loss after mini-batch 98500/101301: 1.194\n",
            "Time remaining for current epoch: 0:01:41.729114\n",
            "Training loss after mini-batch 99000/101301: 1.196\n",
            "Time remaining for current epoch: 0:01:19.082972\n",
            "Training loss after mini-batch 99500/101301: 1.203\n",
            "Time remaining for current epoch: 0:01:04.225643\n",
            "Training loss after mini-batch 100000/101301: 1.120\n",
            "Time remaining for current epoch: 0:00:44.804661\n",
            "Training loss after mini-batch 100500/101301: 1.113\n",
            "Time remaining for current epoch: 0:00:26.652840\n",
            "Training loss after mini-batch 101000/101301: 1.227\n",
            "Time remaining for current epoch: 0:00:10.523460\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 8 complete. Training loss: 1.02901. Validation loss: 0.00004. Validation F1 score: 48.10507\n",
            "Starting epoch 9\n",
            "Training loss after mini-batch   500/101301: 0.952\n",
            "Time remaining for current epoch: 0:25:37.920902\n",
            "Training loss after mini-batch  1000/101301: 1.123\n",
            "Time remaining for current epoch: 0:25:19.490858\n",
            "Training loss after mini-batch  1500/101301: 1.182\n",
            "Time remaining for current epoch: 0:30:00.748964\n",
            "Training loss after mini-batch  2000/101301: 1.082\n",
            "Time remaining for current epoch: 0:26:46.373483\n",
            "Training loss after mini-batch  2500/101301: 0.945\n",
            "Time remaining for current epoch: 0:24:27.212508\n",
            "Training loss after mini-batch  3000/101301: 0.926\n",
            "Time remaining for current epoch: 0:23:34.497144\n",
            "Training loss after mini-batch  3500/101301: 0.843\n",
            "Time remaining for current epoch: 0:21:32.148667\n",
            "Training loss after mini-batch  4000/101301: 1.192\n",
            "Time remaining for current epoch: 0:23:18.141449\n",
            "Training loss after mini-batch  4500/101301: 1.031\n",
            "Time remaining for current epoch: 0:22:54.647850\n",
            "Training loss after mini-batch  5000/101301: 1.003\n",
            "Time remaining for current epoch: 0:22:18.891251\n",
            "Training loss after mini-batch  5500/101301: 0.957\n",
            "Time remaining for current epoch: 0:23:04.071265\n",
            "Training loss after mini-batch  6000/101301: 0.983\n",
            "Time remaining for current epoch: 0:23:41.902606\n",
            "Training loss after mini-batch  6500/101301: 0.993\n",
            "Time remaining for current epoch: 0:23:54.511190\n",
            "Training loss after mini-batch  7000/101301: 1.108\n",
            "Time remaining for current epoch: 0:24:33.087145\n",
            "Training loss after mini-batch  7500/101301: 1.013\n",
            "Time remaining for current epoch: 0:30:10.371215\n",
            "Training loss after mini-batch  8000/101301: 1.064\n",
            "Time remaining for current epoch: 0:27:05.547273\n",
            "Training loss after mini-batch  8500/101301: 0.897\n",
            "Time remaining for current epoch: 0:25:15.911463\n",
            "Training loss after mini-batch  9000/101301: 0.834\n",
            "Time remaining for current epoch: 0:22:32.338219\n",
            "Training loss after mini-batch  9500/101301: 0.766\n",
            "Time remaining for current epoch: 0:21:39.407367\n",
            "Training loss after mini-batch 10000/101301: 1.149\n",
            "Time remaining for current epoch: 0:23:12.449655\n",
            "Training loss after mini-batch 10500/101301: 1.354\n",
            "Time remaining for current epoch: 0:26:30.341055\n",
            "Training loss after mini-batch 11000/101301: 1.172\n",
            "Time remaining for current epoch: 0:20:51.927151\n",
            "Training loss after mini-batch 11500/101301: 0.919\n",
            "Time remaining for current epoch: 0:20:14.485782\n",
            "Training loss after mini-batch 12000/101301: 0.942\n",
            "Time remaining for current epoch: 0:22:22.050632\n",
            "Training loss after mini-batch 12500/101301: 0.963\n",
            "Time remaining for current epoch: 0:22:18.004378\n",
            "Training loss after mini-batch 13000/101301: 0.690\n",
            "Time remaining for current epoch: 0:20:34.005919\n",
            "Training loss after mini-batch 13500/101301: 0.811\n",
            "Time remaining for current epoch: 0:20:51.576512\n",
            "Training loss after mini-batch 14000/101301: 0.736\n",
            "Time remaining for current epoch: 0:19:47.186690\n",
            "Training loss after mini-batch 14500/101301: 0.810\n",
            "Time remaining for current epoch: 0:20:46.171566\n",
            "Training loss after mini-batch 15000/101301: 0.826\n",
            "Time remaining for current epoch: 0:23:04.851689\n",
            "Training loss after mini-batch 15500/101301: 0.990\n",
            "Time remaining for current epoch: 0:25:55.331653\n",
            "Training loss after mini-batch 16000/101301: 1.011\n",
            "Time remaining for current epoch: 0:25:33.536918\n",
            "Training loss after mini-batch 16500/101301: 1.116\n",
            "Time remaining for current epoch: 0:28:05.979513\n",
            "Training loss after mini-batch 17000/101301: 1.144\n",
            "Time remaining for current epoch: 0:24:57.241695\n",
            "Training loss after mini-batch 17500/101301: 1.046\n",
            "Time remaining for current epoch: 0:25:22.513210\n",
            "Training loss after mini-batch 18000/101301: 0.830\n",
            "Time remaining for current epoch: 0:21:48.519849\n",
            "Training loss after mini-batch 18500/101301: 0.897\n",
            "Time remaining for current epoch: 0:21:58.205903\n",
            "Training loss after mini-batch 19000/101301: 1.065\n",
            "Time remaining for current epoch: 0:25:04.209486\n",
            "Training loss after mini-batch 19500/101301: 0.982\n",
            "Time remaining for current epoch: 0:25:48.350857\n",
            "Training loss after mini-batch 20000/101301: 1.016\n",
            "Time remaining for current epoch: 0:27:54.105688\n",
            "Training loss after mini-batch 20500/101301: 1.122\n",
            "Time remaining for current epoch: 0:30:44.856890\n",
            "Training loss after mini-batch 21000/101301: 1.061\n",
            "Time remaining for current epoch: 0:30:52.397393\n",
            "Training loss after mini-batch 21500/101301: 0.639\n",
            "Time remaining for current epoch: 0:27:22.205677\n",
            "Training loss after mini-batch 22000/101301: 0.826\n",
            "Time remaining for current epoch: 0:26:17.727917\n",
            "Training loss after mini-batch 22500/101301: 0.936\n",
            "Time remaining for current epoch: 0:28:14.317993\n",
            "Training loss after mini-batch 23000/101301: 1.035\n",
            "Time remaining for current epoch: 0:22:08.987195\n",
            "Training loss after mini-batch 23500/101301: 0.928\n",
            "Time remaining for current epoch: 0:19:13.254459\n",
            "Training loss after mini-batch 24000/101301: 0.874\n",
            "Time remaining for current epoch: 0:18:02.379501\n",
            "Training loss after mini-batch 24500/101301: 0.770\n",
            "Time remaining for current epoch: 0:20:26.719342\n",
            "Training loss after mini-batch 25000/101301: 1.130\n",
            "Time remaining for current epoch: 0:21:29.346468\n",
            "Training loss after mini-batch 25500/101301: 1.082\n",
            "Time remaining for current epoch: 0:23:12.895315\n",
            "Training loss after mini-batch 26000/101301: 0.901\n",
            "Time remaining for current epoch: 0:18:44.822188\n",
            "Training loss after mini-batch 26500/101301: 0.935\n",
            "Time remaining for current epoch: 0:18:49.061906\n",
            "Training loss after mini-batch 27000/101301: 1.015\n",
            "Time remaining for current epoch: 0:21:10.364171\n",
            "Training loss after mini-batch 27500/101301: 0.932\n",
            "Time remaining for current epoch: 0:19:48.907973\n",
            "Training loss after mini-batch 28000/101301: 0.932\n",
            "Time remaining for current epoch: 0:17:52.614259\n",
            "Training loss after mini-batch 28500/101301: 1.064\n",
            "Time remaining for current epoch: 0:23:25.854756\n",
            "Training loss after mini-batch 29000/101301: 1.121\n",
            "Time remaining for current epoch: 0:21:30.772630\n",
            "Training loss after mini-batch 29500/101301: 0.719\n",
            "Time remaining for current epoch: 0:17:48.410152\n",
            "Training loss after mini-batch 30000/101301: 0.764\n",
            "Time remaining for current epoch: 0:16:22.301260\n",
            "Training loss after mini-batch 30500/101301: 0.947\n",
            "Time remaining for current epoch: 0:21:53.342629\n",
            "Training loss after mini-batch 31000/101301: 1.011\n",
            "Time remaining for current epoch: 0:22:10.930699\n",
            "Training loss after mini-batch 31500/101301: 0.928\n",
            "Time remaining for current epoch: 0:16:13.379617\n",
            "Training loss after mini-batch 32000/101301: 0.792\n",
            "Time remaining for current epoch: 0:15:45.664752\n",
            "Training loss after mini-batch 32500/101301: 0.921\n",
            "Time remaining for current epoch: 0:16:51.345456\n",
            "Training loss after mini-batch 33000/101301: 0.966\n",
            "Time remaining for current epoch: 0:17:54.097867\n",
            "Training loss after mini-batch 33500/101301: 0.932\n",
            "Time remaining for current epoch: 0:18:50.435201\n",
            "Training loss after mini-batch 34000/101301: 0.989\n",
            "Time remaining for current epoch: 0:22:24.349150\n",
            "Training loss after mini-batch 34500/101301: 0.962\n",
            "Time remaining for current epoch: 0:22:24.812279\n",
            "Training loss after mini-batch 35000/101301: 0.850\n",
            "Time remaining for current epoch: 0:19:27.902090\n",
            "Training loss after mini-batch 35500/101301: 0.872\n",
            "Time remaining for current epoch: 0:16:10.017640\n",
            "Training loss after mini-batch 36000/101301: 0.951\n",
            "Time remaining for current epoch: 0:21:40.184843\n",
            "Training loss after mini-batch 36500/101301: 1.091\n",
            "Time remaining for current epoch: 0:21:23.127204\n",
            "Training loss after mini-batch 37000/101301: 1.172\n",
            "Time remaining for current epoch: 0:22:10.233346\n",
            "Training loss after mini-batch 37500/101301: 1.023\n",
            "Time remaining for current epoch: 0:19:25.913905\n",
            "Training loss after mini-batch 38000/101301: 1.038\n",
            "Time remaining for current epoch: 0:16:00.551093\n",
            "Training loss after mini-batch 38500/101301: 1.017\n",
            "Time remaining for current epoch: 0:18:09.986281\n",
            "Training loss after mini-batch 39000/101301: 1.183\n",
            "Time remaining for current epoch: 0:17:55.359925\n",
            "Training loss after mini-batch 39500/101301: 0.867\n",
            "Time remaining for current epoch: 0:16:14.259192\n",
            "Training loss after mini-batch 40000/101301: 0.799\n",
            "Time remaining for current epoch: 0:15:27.347689\n",
            "Training loss after mini-batch 40500/101301: 0.998\n",
            "Time remaining for current epoch: 0:19:05.644102\n",
            "Training loss after mini-batch 41000/101301: 0.950\n",
            "Time remaining for current epoch: 0:15:00.464429\n",
            "Training loss after mini-batch 41500/101301: 0.860\n",
            "Time remaining for current epoch: 0:15:44.936510\n",
            "Training loss after mini-batch 42000/101301: 0.817\n",
            "Time remaining for current epoch: 0:14:43.348426\n",
            "Training loss after mini-batch 42500/101301: 0.876\n",
            "Time remaining for current epoch: 0:14:14.907713\n",
            "Training loss after mini-batch 43000/101301: 0.849\n",
            "Time remaining for current epoch: 0:16:41.113382\n",
            "Training loss after mini-batch 43500/101301: 0.803\n",
            "Time remaining for current epoch: 0:18:07.059723\n",
            "Training loss after mini-batch 44000/101301: 0.933\n",
            "Time remaining for current epoch: 0:16:52.132679\n",
            "Training loss after mini-batch 44500/101301: 0.988\n",
            "Time remaining for current epoch: 0:19:10.851164\n",
            "Training loss after mini-batch 45000/101301: 1.048\n",
            "Time remaining for current epoch: 0:18:33.854765\n",
            "Training loss after mini-batch 45500/101301: 1.003\n",
            "Time remaining for current epoch: 0:14:09.103994\n",
            "Training loss after mini-batch 46000/101301: 0.879\n",
            "Time remaining for current epoch: 0:13:07.274427\n",
            "Training loss after mini-batch 46500/101301: 0.755\n",
            "Time remaining for current epoch: 0:12:37.294849\n",
            "Training loss after mini-batch 47000/101301: 0.869\n",
            "Time remaining for current epoch: 0:13:10.939769\n",
            "Training loss after mini-batch 47500/101301: 0.881\n",
            "Time remaining for current epoch: 0:12:20.544847\n",
            "Training loss after mini-batch 48000/101301: 1.093\n",
            "Time remaining for current epoch: 0:14:29.146349\n",
            "Training loss after mini-batch 48500/101301: 0.966\n",
            "Time remaining for current epoch: 0:14:37.672711\n",
            "Training loss after mini-batch 49000/101301: 0.855\n",
            "Time remaining for current epoch: 0:13:42.658744\n",
            "Training loss after mini-batch 49500/101301: 0.838\n",
            "Time remaining for current epoch: 0:12:48.459710\n",
            "Training loss after mini-batch 50000/101301: 0.585\n",
            "Time remaining for current epoch: 0:12:33.346033\n",
            "Training loss after mini-batch 50500/101301: 0.650\n",
            "Time remaining for current epoch: 0:13:33.675436\n",
            "Training loss after mini-batch 51000/101301: 1.019\n",
            "Time remaining for current epoch: 0:14:31.799579\n",
            "Training loss after mini-batch 51500/101301: 1.004\n",
            "Time remaining for current epoch: 0:13:31.689343\n",
            "Training loss after mini-batch 52000/101301: 0.775\n",
            "Time remaining for current epoch: 0:13:24.661961\n",
            "Training loss after mini-batch 52500/101301: 0.802\n",
            "Time remaining for current epoch: 0:12:40.561027\n",
            "Training loss after mini-batch 53000/101301: 0.865\n",
            "Time remaining for current epoch: 0:11:05.896415\n",
            "Training loss after mini-batch 53500/101301: 0.885\n",
            "Time remaining for current epoch: 0:12:45.140690\n",
            "Training loss after mini-batch 54000/101301: 0.827\n",
            "Time remaining for current epoch: 0:14:06.673537\n",
            "Training loss after mini-batch 54500/101301: 0.960\n",
            "Time remaining for current epoch: 0:14:19.381318\n",
            "Training loss after mini-batch 55000/101301: 1.099\n",
            "Time remaining for current epoch: 0:14:12.880342\n",
            "Training loss after mini-batch 55500/101301: 1.003\n",
            "Time remaining for current epoch: 0:14:57.965821\n",
            "Training loss after mini-batch 56000/101301: 0.963\n",
            "Time remaining for current epoch: 0:12:10.761709\n",
            "Training loss after mini-batch 56500/101301: 0.895\n",
            "Time remaining for current epoch: 0:11:55.757474\n",
            "Training loss after mini-batch 57000/101301: 1.000\n",
            "Time remaining for current epoch: 0:11:25.951518\n",
            "Training loss after mini-batch 57500/101301: 1.074\n",
            "Time remaining for current epoch: 0:10:45.074840\n",
            "Training loss after mini-batch 58000/101301: 1.138\n",
            "Time remaining for current epoch: 0:14:08.213987\n",
            "Training loss after mini-batch 58500/101301: 1.042\n",
            "Time remaining for current epoch: 0:13:22.711636\n",
            "Training loss after mini-batch 59000/101301: 0.955\n",
            "Time remaining for current epoch: 0:11:25.542304\n",
            "Training loss after mini-batch 59500/101301: 0.813\n",
            "Time remaining for current epoch: 0:11:24.784838\n",
            "Training loss after mini-batch 60000/101301: 0.763\n",
            "Time remaining for current epoch: 0:11:25.107519\n",
            "Training loss after mini-batch 60500/101301: 0.969\n",
            "Time remaining for current epoch: 0:12:13.581533\n",
            "Training loss after mini-batch 61000/101301: 0.934\n",
            "Time remaining for current epoch: 0:11:37.150963\n",
            "Training loss after mini-batch 61500/101301: 0.688\n",
            "Time remaining for current epoch: 0:10:21.193086\n",
            "Training loss after mini-batch 62000/101301: 0.933\n",
            "Time remaining for current epoch: 0:10:27.134613\n",
            "Training loss after mini-batch 62500/101301: 0.889\n",
            "Time remaining for current epoch: 0:15:30.699679\n",
            "Training loss after mini-batch 63000/101301: 0.938\n",
            "Time remaining for current epoch: 0:15:16.013798\n",
            "Training loss after mini-batch 63500/101301: 1.047\n",
            "Time remaining for current epoch: 0:10:51.230804\n",
            "Training loss after mini-batch 64000/101301: 0.924\n",
            "Time remaining for current epoch: 0:09:30.261641\n",
            "Training loss after mini-batch 64500/101301: 0.912\n",
            "Time remaining for current epoch: 0:09:53.888712\n",
            "Training loss after mini-batch 65000/101301: 0.788\n",
            "Time remaining for current epoch: 0:09:40.629402\n",
            "Training loss after mini-batch 65500/101301: 0.699\n",
            "Time remaining for current epoch: 0:09:55.510516\n",
            "Training loss after mini-batch 66000/101301: 0.958\n",
            "Time remaining for current epoch: 0:10:41.786465\n",
            "Training loss after mini-batch 66500/101301: 0.856\n",
            "Time remaining for current epoch: 0:10:49.623689\n",
            "Training loss after mini-batch 67000/101301: 0.658\n",
            "Time remaining for current epoch: 0:10:46.571393\n",
            "Training loss after mini-batch 67500/101301: 0.946\n",
            "Time remaining for current epoch: 0:12:38.350705\n",
            "Training loss after mini-batch 68000/101301: 0.968\n",
            "Time remaining for current epoch: 0:08:08.036944\n",
            "Training loss after mini-batch 68500/101301: 0.879\n",
            "Time remaining for current epoch: 0:08:29.119990\n",
            "Training loss after mini-batch 69000/101301: 0.899\n",
            "Time remaining for current epoch: 0:08:21.053500\n",
            "Training loss after mini-batch 69500/101301: 1.005\n",
            "Time remaining for current epoch: 0:08:24.508725\n",
            "Training loss after mini-batch 70000/101301: 0.959\n",
            "Time remaining for current epoch: 0:10:00.828924\n",
            "Training loss after mini-batch 70500/101301: 0.809\n",
            "Time remaining for current epoch: 0:09:41.282074\n",
            "Training loss after mini-batch 71000/101301: 0.718\n",
            "Time remaining for current epoch: 0:08:07.665576\n",
            "Training loss after mini-batch 71500/101301: 0.689\n",
            "Time remaining for current epoch: 0:07:31.386079\n",
            "Training loss after mini-batch 72000/101301: 0.752\n",
            "Time remaining for current epoch: 0:07:28.294100\n",
            "Training loss after mini-batch 72500/101301: 0.816\n",
            "Time remaining for current epoch: 0:07:25.491271\n",
            "Training loss after mini-batch 73000/101301: 0.727\n",
            "Time remaining for current epoch: 0:07:10.517198\n",
            "Training loss after mini-batch 73500/101301: 0.834\n",
            "Time remaining for current epoch: 0:08:18.174780\n",
            "Training loss after mini-batch 74000/101301: 0.894\n",
            "Time remaining for current epoch: 0:08:36.575669\n",
            "Training loss after mini-batch 74500/101301: 0.766\n",
            "Time remaining for current epoch: 0:08:26.943035\n",
            "Training loss after mini-batch 75000/101301: 1.001\n",
            "Time remaining for current epoch: 0:08:04.133884\n",
            "Training loss after mini-batch 75500/101301: 0.938\n",
            "Time remaining for current epoch: 0:08:05.171807\n",
            "Training loss after mini-batch 76000/101301: 0.753\n",
            "Time remaining for current epoch: 0:05:47.304589\n",
            "Training loss after mini-batch 76500/101301: 0.715\n",
            "Time remaining for current epoch: 0:05:33.772217\n",
            "Training loss after mini-batch 77000/101301: 0.786\n",
            "Time remaining for current epoch: 0:06:59.195437\n",
            "Training loss after mini-batch 77500/101301: 0.856\n",
            "Time remaining for current epoch: 0:07:27.575009\n",
            "Training loss after mini-batch 78000/101301: 0.881\n",
            "Time remaining for current epoch: 0:05:45.600849\n",
            "Training loss after mini-batch 78500/101301: 0.823\n",
            "Time remaining for current epoch: 0:05:23.906262\n",
            "Training loss after mini-batch 79000/101301: 0.955\n",
            "Time remaining for current epoch: 0:05:08.660805\n",
            "Training loss after mini-batch 79500/101301: 0.806\n",
            "Time remaining for current epoch: 0:05:17.380530\n",
            "Training loss after mini-batch 80000/101301: 0.750\n",
            "Time remaining for current epoch: 0:05:17.646102\n",
            "Training loss after mini-batch 80500/101301: 0.981\n",
            "Time remaining for current epoch: 0:06:07.096103\n",
            "Training loss after mini-batch 81000/101301: 0.805\n",
            "Time remaining for current epoch: 0:05:44.535361\n",
            "Training loss after mini-batch 81500/101301: 0.595\n",
            "Time remaining for current epoch: 0:04:39.315162\n",
            "Training loss after mini-batch 82000/101301: 0.651\n",
            "Time remaining for current epoch: 0:04:42.378611\n",
            "Training loss after mini-batch 82500/101301: 0.512\n",
            "Time remaining for current epoch: 0:04:05.374794\n",
            "Training loss after mini-batch 83000/101301: 0.621\n",
            "Time remaining for current epoch: 0:04:10.697779\n",
            "Training loss after mini-batch 83500/101301: 0.609\n",
            "Time remaining for current epoch: 0:04:28.942903\n",
            "Training loss after mini-batch 84000/101301: 0.533\n",
            "Time remaining for current epoch: 0:04:06.789795\n",
            "Training loss after mini-batch 84500/101301: 0.565\n",
            "Time remaining for current epoch: 0:04:05.640937\n",
            "Training loss after mini-batch 85000/101301: 0.636\n",
            "Time remaining for current epoch: 0:04:14.002707\n",
            "Training loss after mini-batch 85500/101301: 0.832\n",
            "Time remaining for current epoch: 0:04:44.126543\n",
            "Training loss after mini-batch 86000/101301: 0.882\n",
            "Time remaining for current epoch: 0:05:18.998299\n",
            "Training loss after mini-batch 86500/101301: 0.777\n",
            "Time remaining for current epoch: 0:05:37.097632\n",
            "Training loss after mini-batch 87000/101301: 0.760\n",
            "Time remaining for current epoch: 0:04:03.531365\n",
            "Training loss after mini-batch 87500/101301: 0.843\n",
            "Time remaining for current epoch: 0:04:17.560898\n",
            "Training loss after mini-batch 88000/101301: 0.767\n",
            "Time remaining for current epoch: 0:03:34.099140\n",
            "Training loss after mini-batch 88500/101301: 0.723\n",
            "Time remaining for current epoch: 0:03:35.618083\n",
            "Training loss after mini-batch 89000/101301: 0.708\n",
            "Time remaining for current epoch: 0:03:14.879580\n",
            "Training loss after mini-batch 89500/101301: 0.740\n",
            "Time remaining for current epoch: 0:03:07.555455\n",
            "Training loss after mini-batch 90000/101301: 0.795\n",
            "Time remaining for current epoch: 0:02:51.185104\n",
            "Training loss after mini-batch 90500/101301: 0.626\n",
            "Time remaining for current epoch: 0:02:32.867638\n",
            "Training loss after mini-batch 91000/101301: 0.663\n",
            "Time remaining for current epoch: 0:02:46.372188\n",
            "Training loss after mini-batch 91500/101301: 0.723\n",
            "Time remaining for current epoch: 0:02:36.330388\n",
            "Training loss after mini-batch 92000/101301: 0.756\n",
            "Time remaining for current epoch: 0:02:28.584237\n",
            "Training loss after mini-batch 92500/101301: 0.600\n",
            "Time remaining for current epoch: 0:01:56.710335\n",
            "Training loss after mini-batch 93000/101301: 0.555\n",
            "Time remaining for current epoch: 0:01:42.633527\n",
            "Training loss after mini-batch 93500/101301: 0.495\n",
            "Time remaining for current epoch: 0:01:43.160959\n",
            "Training loss after mini-batch 94000/101301: 0.494\n",
            "Time remaining for current epoch: 0:01:33.423091\n",
            "Training loss after mini-batch 94500/101301: 0.728\n",
            "Time remaining for current epoch: 0:01:40.056613\n",
            "Training loss after mini-batch 95000/101301: 0.661\n",
            "Time remaining for current epoch: 0:01:34.955975\n",
            "Training loss after mini-batch 95500/101301: 0.546\n",
            "Time remaining for current epoch: 0:01:15.399465\n",
            "Training loss after mini-batch 96000/101301: 0.639\n",
            "Time remaining for current epoch: 0:01:28.402191\n",
            "Training loss after mini-batch 96500/101301: 0.738\n",
            "Time remaining for current epoch: 0:01:40.051538\n",
            "Training loss after mini-batch 97000/101301: 0.654\n",
            "Time remaining for current epoch: 0:01:11.190646\n",
            "Training loss after mini-batch 97500/101301: 0.693\n",
            "Time remaining for current epoch: 0:01:04.148335\n",
            "Training loss after mini-batch 98000/101301: 0.786\n",
            "Time remaining for current epoch: 0:00:52.923743\n",
            "Training loss after mini-batch 98500/101301: 0.722\n",
            "Time remaining for current epoch: 0:00:47.906750\n",
            "Training loss after mini-batch 99000/101301: 0.664\n",
            "Time remaining for current epoch: 0:00:34.042695\n",
            "Training loss after mini-batch 99500/101301: 0.623\n",
            "Time remaining for current epoch: 0:00:24.799029\n",
            "Training loss after mini-batch 100000/101301: 0.645\n",
            "Time remaining for current epoch: 0:00:18.362248\n",
            "Training loss after mini-batch 100500/101301: 0.484\n",
            "Time remaining for current epoch: 0:00:11.215335\n",
            "Training loss after mini-batch 101000/101301: 0.437\n",
            "Time remaining for current epoch: 0:00:04.227226\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 9 complete. Training loss: 0.87150. Validation loss: 0.00004. Validation F1 score: 48.10507\n",
            "Starting epoch 10\n",
            "Training loss after mini-batch   500/101301: 1.033\n",
            "Time remaining for current epoch: 0:30:32.941833\n",
            "Training loss after mini-batch  1000/101301: 0.733\n",
            "Time remaining for current epoch: 0:25:35.571716\n",
            "Training loss after mini-batch  1500/101301: 0.769\n",
            "Time remaining for current epoch: 0:24:54.786939\n",
            "Training loss after mini-batch  2000/101301: 0.950\n",
            "Time remaining for current epoch: 0:30:31.891064\n",
            "Training loss after mini-batch  2500/101301: 1.057\n",
            "Time remaining for current epoch: 0:25:09.915473\n",
            "Training loss after mini-batch  3000/101301: 0.904\n",
            "Time remaining for current epoch: 0:26:08.388135\n",
            "Training loss after mini-batch  3500/101301: 0.834\n",
            "Time remaining for current epoch: 0:22:44.138370\n",
            "Training loss after mini-batch  4000/101301: 0.884\n",
            "Time remaining for current epoch: 0:21:56.710451\n",
            "Training loss after mini-batch  4500/101301: 0.995\n",
            "Time remaining for current epoch: 0:27:43.005852\n",
            "Training loss after mini-batch  5000/101301: 0.865\n",
            "Time remaining for current epoch: 0:29:55.791407\n",
            "Training loss after mini-batch  5500/101301: 1.155\n",
            "Time remaining for current epoch: 0:28:38.325644\n",
            "Training loss after mini-batch  6000/101301: 1.046\n",
            "Time remaining for current epoch: 0:33:03.575713\n",
            "Training loss after mini-batch  6500/101301: 1.095\n",
            "Time remaining for current epoch: 0:30:48.105026\n",
            "Training loss after mini-batch  7000/101301: 0.832\n",
            "Time remaining for current epoch: 0:31:17.005082\n",
            "Training loss after mini-batch  7500/101301: 0.963\n",
            "Time remaining for current epoch: 0:31:33.293698\n",
            "Training loss after mini-batch  8000/101301: 0.806\n",
            "Time remaining for current epoch: 0:26:32.469283\n",
            "Training loss after mini-batch  8500/101301: 0.871\n",
            "Time remaining for current epoch: 0:25:57.858723\n",
            "Training loss after mini-batch  9000/101301: 0.878\n",
            "Time remaining for current epoch: 0:29:17.450384\n",
            "Training loss after mini-batch  9500/101301: 1.124\n",
            "Time remaining for current epoch: 0:26:09.912655\n",
            "Training loss after mini-batch 10000/101301: 1.217\n",
            "Time remaining for current epoch: 0:26:35.272512\n",
            "Training loss after mini-batch 10500/101301: 1.260\n",
            "Time remaining for current epoch: 0:25:57.783311\n",
            "Training loss after mini-batch 11000/101301: 1.065\n",
            "Time remaining for current epoch: 0:24:59.513384\n",
            "Training loss after mini-batch 11500/101301: 1.183\n",
            "Time remaining for current epoch: 0:23:02.076131\n",
            "Training loss after mini-batch 12000/101301: 0.924\n",
            "Time remaining for current epoch: 0:21:55.890572\n",
            "Training loss after mini-batch 12500/101301: 0.985\n",
            "Time remaining for current epoch: 0:22:13.885021\n",
            "Training loss after mini-batch 13000/101301: 0.848\n",
            "Time remaining for current epoch: 0:22:58.046033\n",
            "Training loss after mini-batch 13500/101301: 0.728\n",
            "Time remaining for current epoch: 0:22:48.550996\n",
            "Training loss after mini-batch 14000/101301: 0.829\n",
            "Time remaining for current epoch: 0:26:59.371207\n",
            "Training loss after mini-batch 14500/101301: 0.937\n",
            "Time remaining for current epoch: 0:29:38.540550\n",
            "Training loss after mini-batch 15000/101301: 0.826\n",
            "Time remaining for current epoch: 0:23:21.380034\n",
            "Training loss after mini-batch 15500/101301: 0.867\n",
            "Time remaining for current epoch: 0:22:50.262859\n",
            "Training loss after mini-batch 16000/101301: 0.976\n",
            "Time remaining for current epoch: 0:24:31.965204\n",
            "Training loss after mini-batch 16500/101301: 0.933\n",
            "Time remaining for current epoch: 0:22:23.483718\n",
            "Training loss after mini-batch 17000/101301: 0.865\n",
            "Time remaining for current epoch: 0:27:37.326936\n",
            "Training loss after mini-batch 17500/101301: 1.007\n",
            "Time remaining for current epoch: 0:23:45.108821\n",
            "Training loss after mini-batch 18000/101301: 0.859\n",
            "Time remaining for current epoch: 0:23:22.895464\n",
            "Training loss after mini-batch 18500/101301: 0.917\n",
            "Time remaining for current epoch: 0:21:53.496538\n",
            "Training loss after mini-batch 19000/101301: 0.847\n",
            "Time remaining for current epoch: 0:21:05.028602\n",
            "Training loss after mini-batch 19500/101301: 0.614\n",
            "Time remaining for current epoch: 0:22:18.807565\n",
            "Training loss after mini-batch 20000/101301: 0.790\n",
            "Time remaining for current epoch: 0:22:53.567755\n",
            "Training loss after mini-batch 20500/101301: 0.901\n",
            "Time remaining for current epoch: 0:23:12.482377\n",
            "Training loss after mini-batch 21000/101301: 0.810\n",
            "Time remaining for current epoch: 0:19:18.694481\n",
            "Training loss after mini-batch 21500/101301: 0.928\n",
            "Time remaining for current epoch: 0:19:05.513904\n",
            "Training loss after mini-batch 22000/101301: 0.957\n",
            "Time remaining for current epoch: 0:19:26.996812\n",
            "Training loss after mini-batch 22500/101301: 0.893\n",
            "Time remaining for current epoch: 0:18:53.372380\n",
            "Training loss after mini-batch 23000/101301: 1.047\n",
            "Time remaining for current epoch: 0:23:06.586792\n",
            "Training loss after mini-batch 23500/101301: 1.005\n",
            "Time remaining for current epoch: 0:27:18.169824\n",
            "Training loss after mini-batch 24000/101301: 0.944\n",
            "Time remaining for current epoch: 0:27:42.034125\n",
            "Training loss after mini-batch 24500/101301: 0.927\n",
            "Time remaining for current epoch: 0:22:40.322266\n",
            "Training loss after mini-batch 25000/101301: 1.030\n",
            "Time remaining for current epoch: 0:19:10.084424\n",
            "Training loss after mini-batch 25500/101301: 0.973\n",
            "Time remaining for current epoch: 0:21:22.480781\n",
            "Training loss after mini-batch 26000/101301: 0.976\n",
            "Time remaining for current epoch: 0:20:36.704362\n",
            "Training loss after mini-batch 26500/101301: 0.943\n",
            "Time remaining for current epoch: 0:17:27.012690\n",
            "Training loss after mini-batch 27000/101301: 0.633\n",
            "Time remaining for current epoch: 0:17:29.690748\n",
            "Training loss after mini-batch 27500/101301: 0.758\n",
            "Time remaining for current epoch: 0:17:04.342510\n",
            "Training loss after mini-batch 28000/101301: 0.843\n",
            "Time remaining for current epoch: 0:18:06.747255\n",
            "Training loss after mini-batch 28500/101301: 0.773\n",
            "Time remaining for current epoch: 0:22:01.018682\n",
            "Training loss after mini-batch 29000/101301: 1.029\n",
            "Time remaining for current epoch: 0:16:48.917286\n",
            "Training loss after mini-batch 29500/101301: 0.899\n",
            "Time remaining for current epoch: 0:17:33.166539\n",
            "Training loss after mini-batch 30000/101301: 0.872\n",
            "Time remaining for current epoch: 0:16:30.976334\n",
            "Training loss after mini-batch 30500/101301: 0.850\n",
            "Time remaining for current epoch: 0:16:54.642672\n",
            "Training loss after mini-batch 31000/101301: 0.917\n",
            "Time remaining for current epoch: 0:18:22.873956\n",
            "Training loss after mini-batch 31500/101301: 0.896\n",
            "Time remaining for current epoch: 0:19:35.105596\n",
            "Training loss after mini-batch 32000/101301: 0.895\n",
            "Time remaining for current epoch: 0:20:17.041975\n",
            "Training loss after mini-batch 32500/101301: 0.757\n",
            "Time remaining for current epoch: 0:15:25.148480\n",
            "Training loss after mini-batch 33000/101301: 0.797\n",
            "Time remaining for current epoch: 0:15:38.151730\n",
            "Training loss after mini-batch 33500/101301: 0.860\n",
            "Time remaining for current epoch: 0:14:39.702353\n",
            "Training loss after mini-batch 34000/101301: 0.783\n",
            "Time remaining for current epoch: 0:14:44.133564\n",
            "Training loss after mini-batch 34500/101301: 0.918\n",
            "Time remaining for current epoch: 0:15:43.694468\n",
            "Training loss after mini-batch 35000/101301: 0.775\n",
            "Time remaining for current epoch: 0:15:48.925159\n",
            "Training loss after mini-batch 35500/101301: 0.906\n",
            "Time remaining for current epoch: 0:17:00.508482\n",
            "Training loss after mini-batch 36000/101301: 0.881\n",
            "Time remaining for current epoch: 0:17:19.851615\n",
            "Training loss after mini-batch 36500/101301: 1.068\n",
            "Time remaining for current epoch: 0:14:46.837154\n",
            "Training loss after mini-batch 37000/101301: 0.909\n",
            "Time remaining for current epoch: 0:15:55.727098\n",
            "Training loss after mini-batch 37500/101301: 0.855\n",
            "Time remaining for current epoch: 0:14:49.667749\n",
            "Training loss after mini-batch 38000/101301: 0.918\n",
            "Time remaining for current epoch: 0:16:13.719856\n",
            "Training loss after mini-batch 38500/101301: 0.874\n",
            "Time remaining for current epoch: 0:15:05.487621\n",
            "Training loss after mini-batch 39000/101301: 0.937\n",
            "Time remaining for current epoch: 0:15:39.766294\n",
            "Training loss after mini-batch 39500/101301: 0.842\n",
            "Time remaining for current epoch: 0:14:17.109668\n",
            "Training loss after mini-batch 40000/101301: 0.808\n",
            "Time remaining for current epoch: 0:13:54.682407\n",
            "Training loss after mini-batch 40500/101301: 0.709\n",
            "Time remaining for current epoch: 0:13:23.288454\n",
            "Training loss after mini-batch 41000/101301: 0.757\n",
            "Time remaining for current epoch: 0:13:46.147928\n",
            "Training loss after mini-batch 41500/101301: 0.891\n",
            "Time remaining for current epoch: 0:15:21.577799\n",
            "Training loss after mini-batch 42000/101301: 0.923\n",
            "Time remaining for current epoch: 0:16:04.537544\n",
            "Training loss after mini-batch 42500/101301: 0.843\n",
            "Time remaining for current epoch: 0:18:55.209877\n",
            "Training loss after mini-batch 43000/101301: 0.907\n",
            "Time remaining for current epoch: 0:16:48.484379\n",
            "Training loss after mini-batch 43500/101301: 0.858\n",
            "Time remaining for current epoch: 0:15:19.393573\n",
            "Training loss after mini-batch 44000/101301: 0.907\n",
            "Time remaining for current epoch: 0:17:11.800990\n",
            "Training loss after mini-batch 44500/101301: 0.947\n",
            "Time remaining for current epoch: 0:17:33.240100\n",
            "Training loss after mini-batch 45000/101301: 0.890\n",
            "Time remaining for current epoch: 0:13:47.679778\n",
            "Training loss after mini-batch 45500/101301: 0.928\n",
            "Time remaining for current epoch: 0:13:51.802548\n",
            "Training loss after mini-batch 46000/101301: 0.936\n",
            "Time remaining for current epoch: 0:13:39.650460\n",
            "Training loss after mini-batch 46500/101301: 0.870\n",
            "Time remaining for current epoch: 0:13:46.514157\n",
            "Training loss after mini-batch 47000/101301: 0.789\n",
            "Time remaining for current epoch: 0:12:26.655399\n",
            "Training loss after mini-batch 47500/101301: 0.640\n",
            "Time remaining for current epoch: 0:12:38.663592\n",
            "Training loss after mini-batch 48000/101301: 0.713\n",
            "Time remaining for current epoch: 0:12:30.646507\n",
            "Training loss after mini-batch 48500/101301: 0.644\n",
            "Time remaining for current epoch: 0:12:23.202264\n",
            "Training loss after mini-batch 49000/101301: 0.783\n",
            "Time remaining for current epoch: 0:13:26.085990\n",
            "Training loss after mini-batch 49500/101301: 0.881\n",
            "Time remaining for current epoch: 0:12:16.909285\n",
            "Training loss after mini-batch 50000/101301: 1.025\n",
            "Time remaining for current epoch: 0:12:53.679155\n",
            "Training loss after mini-batch 50500/101301: 1.032\n",
            "Time remaining for current epoch: 0:15:29.043364\n",
            "Training loss after mini-batch 51000/101301: 0.892\n",
            "Time remaining for current epoch: 0:12:53.189902\n",
            "Training loss after mini-batch 51500/101301: 0.505\n",
            "Time remaining for current epoch: 0:11:53.583829\n",
            "Training loss after mini-batch 52000/101301: 0.623\n",
            "Time remaining for current epoch: 0:11:35.013411\n",
            "Training loss after mini-batch 52500/101301: 0.843\n",
            "Time remaining for current epoch: 0:12:32.801532\n",
            "Training loss after mini-batch 53000/101301: 0.882\n",
            "Time remaining for current epoch: 0:13:06.387505\n",
            "Training loss after mini-batch 53500/101301: 0.988\n",
            "Time remaining for current epoch: 0:15:26.618453\n",
            "Training loss after mini-batch 54000/101301: 0.957\n",
            "Time remaining for current epoch: 0:12:24.229681\n",
            "Training loss after mini-batch 54500/101301: 1.065\n",
            "Time remaining for current epoch: 0:12:11.849178\n",
            "Training loss after mini-batch 55000/101301: 0.982\n",
            "Time remaining for current epoch: 0:13:01.556823\n",
            "Training loss after mini-batch 55500/101301: 0.995\n",
            "Time remaining for current epoch: 0:10:52.590913\n",
            "Training loss after mini-batch 56000/101301: 0.964\n",
            "Time remaining for current epoch: 0:11:25.394371\n",
            "Training loss after mini-batch 56500/101301: 0.949\n",
            "Time remaining for current epoch: 0:11:01.819036\n",
            "Training loss after mini-batch 57000/101301: 1.189\n",
            "Time remaining for current epoch: 0:10:07.484860\n",
            "Training loss after mini-batch 57500/101301: 0.961\n",
            "Time remaining for current epoch: 0:12:39.579649\n",
            "Training loss after mini-batch 58000/101301: 1.182\n",
            "Time remaining for current epoch: 0:12:08.275772\n",
            "Training loss after mini-batch 58500/101301: 1.061\n",
            "Time remaining for current epoch: 0:11:40.546830\n",
            "Training loss after mini-batch 59000/101301: 0.607\n",
            "Time remaining for current epoch: 0:09:57.137907\n",
            "Training loss after mini-batch 59500/101301: 0.537\n",
            "Time remaining for current epoch: 0:09:29.488416\n",
            "Training loss after mini-batch 60000/101301: 0.796\n",
            "Time remaining for current epoch: 0:09:29.756180\n",
            "Training loss after mini-batch 60500/101301: 0.898\n",
            "Time remaining for current epoch: 0:10:14.391014\n",
            "Training loss after mini-batch 61000/101301: 0.853\n",
            "Time remaining for current epoch: 0:10:23.094221\n",
            "Training loss after mini-batch 61500/101301: 0.833\n",
            "Time remaining for current epoch: 0:12:04.485934\n",
            "Training loss after mini-batch 62000/101301: 0.904\n",
            "Time remaining for current epoch: 0:08:54.214949\n",
            "Training loss after mini-batch 62500/101301: 1.049\n",
            "Time remaining for current epoch: 0:09:46.043998\n",
            "Training loss after mini-batch 63000/101301: 1.056\n",
            "Time remaining for current epoch: 0:10:09.821561\n",
            "Training loss after mini-batch 63500/101301: 1.302\n",
            "Time remaining for current epoch: 0:23:50.608957\n",
            "Training loss after mini-batch 64000/101301: 1.453\n",
            "Time remaining for current epoch: 0:27:43.578743\n",
            "Training loss after mini-batch 64500/101301: 1.180\n",
            "Time remaining for current epoch: 0:24:43.037332\n",
            "Training loss after mini-batch 65000/101301: 1.299\n",
            "Time remaining for current epoch: 0:23:17.013057\n",
            "Training loss after mini-batch 65500/101301: 1.352\n",
            "Time remaining for current epoch: 0:22:09.843184\n",
            "Training loss after mini-batch 66000/101301: 1.322\n",
            "Time remaining for current epoch: 0:23:32.324407\n",
            "Training loss after mini-batch 66500/101301: 1.348\n",
            "Time remaining for current epoch: 0:23:36.687644\n",
            "Training loss after mini-batch 67000/101301: 1.265\n",
            "Time remaining for current epoch: 0:23:49.396062\n",
            "Training loss after mini-batch 67500/101301: 1.300\n",
            "Time remaining for current epoch: 0:21:51.704275\n",
            "Training loss after mini-batch 68000/101301: 1.191\n",
            "Time remaining for current epoch: 0:17:48.168234\n",
            "Training loss after mini-batch 68500/101301: 1.288\n",
            "Time remaining for current epoch: 0:18:31.390289\n",
            "Training loss after mini-batch 69000/101301: 1.461\n",
            "Time remaining for current epoch: 0:22:33.006529\n",
            "Training loss after mini-batch 69500/101301: 1.368\n",
            "Time remaining for current epoch: 0:21:55.232378\n",
            "Training loss after mini-batch 70000/101301: 1.378\n",
            "Time remaining for current epoch: 0:20:12.600972\n",
            "Training loss after mini-batch 70500/101301: 1.294\n",
            "Time remaining for current epoch: 0:18:43.637209\n",
            "Training loss after mini-batch 71000/101301: 1.335\n",
            "Time remaining for current epoch: 0:18:07.264975\n",
            "Training loss after mini-batch 71500/101301: 1.346\n",
            "Time remaining for current epoch: 0:17:59.118013\n",
            "Training loss after mini-batch 72000/101301: 1.303\n",
            "Time remaining for current epoch: 0:18:31.452956\n",
            "Training loss after mini-batch 72500/101301: 1.236\n",
            "Time remaining for current epoch: 0:17:29.140565\n",
            "Training loss after mini-batch 73000/101301: 1.125\n",
            "Time remaining for current epoch: 0:16:28.513521\n",
            "Training loss after mini-batch 73500/101301: 1.190\n",
            "Time remaining for current epoch: 0:18:10.776613\n",
            "Training loss after mini-batch 74000/101301: 1.323\n",
            "Time remaining for current epoch: 0:20:59.379121\n",
            "Training loss after mini-batch 74500/101301: 1.404\n",
            "Time remaining for current epoch: 0:18:53.225507\n",
            "Training loss after mini-batch 75000/101301: 1.515\n",
            "Time remaining for current epoch: 0:20:01.659532\n",
            "Training loss after mini-batch 75500/101301: 1.265\n",
            "Time remaining for current epoch: 0:17:09.437819\n",
            "Training loss after mini-batch 76000/101301: 1.253\n",
            "Time remaining for current epoch: 0:18:21.283587\n",
            "Training loss after mini-batch 76500/101301: 1.391\n",
            "Time remaining for current epoch: 0:16:57.254438\n",
            "Training loss after mini-batch 77000/101301: 1.189\n",
            "Time remaining for current epoch: 0:16:28.279238\n",
            "Training loss after mini-batch 77500/101301: 1.294\n",
            "Time remaining for current epoch: 0:15:19.024879\n",
            "Training loss after mini-batch 78000/101301: 1.337\n",
            "Time remaining for current epoch: 0:16:04.494718\n",
            "Training loss after mini-batch 78500/101301: 1.348\n",
            "Time remaining for current epoch: 0:15:15.255045\n",
            "Training loss after mini-batch 79000/101301: 1.209\n",
            "Time remaining for current epoch: 0:15:35.593440\n",
            "Training loss after mini-batch 79500/101301: 1.645\n",
            "Time remaining for current epoch: 0:16:08.131728\n",
            "Training loss after mini-batch 80000/101301: 1.237\n",
            "Time remaining for current epoch: 0:15:39.810945\n",
            "Training loss after mini-batch 80500/101301: 1.245\n",
            "Time remaining for current epoch: 0:14:32.441610\n",
            "Training loss after mini-batch 81000/101301: 1.320\n",
            "Time remaining for current epoch: 0:14:11.024785\n",
            "Training loss after mini-batch 81500/101301: 1.214\n",
            "Time remaining for current epoch: 0:13:27.546364\n",
            "Training loss after mini-batch 82000/101301: 1.243\n",
            "Time remaining for current epoch: 0:13:41.771860\n",
            "Training loss after mini-batch 82500/101301: 1.173\n",
            "Time remaining for current epoch: 0:13:28.156666\n",
            "Training loss after mini-batch 83000/101301: 1.108\n",
            "Time remaining for current epoch: 0:12:19.099442\n",
            "Training loss after mini-batch 83500/101301: 1.252\n",
            "Time remaining for current epoch: 0:13:11.815278\n",
            "Training loss after mini-batch 84000/101301: 1.286\n",
            "Time remaining for current epoch: 0:13:24.527206\n",
            "Training loss after mini-batch 84500/101301: 1.193\n",
            "Time remaining for current epoch: 0:12:30.803027\n",
            "Training loss after mini-batch 85000/101301: 1.159\n",
            "Time remaining for current epoch: 0:10:33.102293\n",
            "Training loss after mini-batch 85500/101301: 1.135\n",
            "Time remaining for current epoch: 0:10:22.380141\n",
            "Training loss after mini-batch 86000/101301: 1.187\n",
            "Time remaining for current epoch: 0:10:40.166949\n",
            "Training loss after mini-batch 86500/101301: 1.192\n",
            "Time remaining for current epoch: 0:11:21.807812\n",
            "Training loss after mini-batch 87000/101301: 1.337\n",
            "Time remaining for current epoch: 0:11:27.520245\n",
            "Training loss after mini-batch 87500/101301: 1.184\n",
            "Time remaining for current epoch: 0:09:56.390257\n",
            "Training loss after mini-batch 88000/101301: 1.156\n",
            "Time remaining for current epoch: 0:09:12.874858\n",
            "Training loss after mini-batch 88500/101301: 1.261\n",
            "Time remaining for current epoch: 0:08:40.320102\n",
            "Training loss after mini-batch 89000/101301: 1.126\n",
            "Time remaining for current epoch: 0:09:01.070620\n",
            "Training loss after mini-batch 89500/101301: 1.124\n",
            "Time remaining for current epoch: 0:08:17.020338\n",
            "Training loss after mini-batch 90000/101301: 1.057\n",
            "Time remaining for current epoch: 0:08:15.567187\n",
            "Training loss after mini-batch 90500/101301: 1.124\n",
            "Time remaining for current epoch: 0:07:24.341344\n",
            "Training loss after mini-batch 91000/101301: 0.998\n",
            "Time remaining for current epoch: 0:06:11.540357\n",
            "Training loss after mini-batch 91500/101301: 1.108\n",
            "Time remaining for current epoch: 0:07:12.629701\n",
            "Training loss after mini-batch 92000/101301: 1.394\n",
            "Time remaining for current epoch: 0:07:11.325701\n",
            "Training loss after mini-batch 92500/101301: 1.080\n",
            "Time remaining for current epoch: 0:06:01.260627\n",
            "Training loss after mini-batch 93000/101301: 1.121\n",
            "Time remaining for current epoch: 0:05:14.920743\n",
            "Training loss after mini-batch 93500/101301: 1.347\n",
            "Time remaining for current epoch: 0:05:28.822447\n",
            "Training loss after mini-batch 94000/101301: 1.422\n",
            "Time remaining for current epoch: 0:05:14.562659\n",
            "Training loss after mini-batch 94500/101301: 1.252\n",
            "Time remaining for current epoch: 0:04:41.253477\n",
            "Training loss after mini-batch 95000/101301: 1.092\n",
            "Time remaining for current epoch: 0:04:48.693917\n",
            "Training loss after mini-batch 95500/101301: 1.040\n",
            "Time remaining for current epoch: 0:04:01.477092\n",
            "Training loss after mini-batch 96000/101301: 1.213\n",
            "Time remaining for current epoch: 0:03:27.155433\n",
            "Training loss after mini-batch 96500/101301: 1.216\n",
            "Time remaining for current epoch: 0:03:17.477001\n",
            "Training loss after mini-batch 97000/101301: 1.258\n",
            "Time remaining for current epoch: 0:02:49.568150\n",
            "Training loss after mini-batch 97500/101301: 1.133\n",
            "Time remaining for current epoch: 0:02:31.405567\n",
            "Training loss after mini-batch 98000/101301: 1.090\n",
            "Time remaining for current epoch: 0:02:03.984842\n",
            "Training loss after mini-batch 98500/101301: 1.194\n",
            "Time remaining for current epoch: 0:01:56.772475\n",
            "Training loss after mini-batch 99000/101301: 1.195\n",
            "Time remaining for current epoch: 0:01:27.905794\n",
            "Training loss after mini-batch 99500/101301: 1.203\n",
            "Time remaining for current epoch: 0:01:10.980602\n",
            "Training loss after mini-batch 100000/101301: 1.120\n",
            "Time remaining for current epoch: 0:00:46.863923\n",
            "Training loss after mini-batch 100500/101301: 1.113\n",
            "Time remaining for current epoch: 0:00:30.581445\n",
            "Training loss after mini-batch 101000/101301: 1.226\n",
            "Time remaining for current epoch: 0:00:13.120640\n",
            "Training done. Validating...\n",
            "100/436 complete.\n",
            "200/436 complete.\n",
            "300/436 complete.\n",
            "400/436 complete.\n",
            "Epoch 10 complete. Training loss: 1.02873. Validation loss: 0.00004. Validation F1 score: 48.10507\n",
            "Training process has finished.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the MLP\n",
        "mlp = MLP_Classifier().to(dev)\n",
        "\n",
        "# Define the loss function, optimizer, and scheduler (to reduce learning rate)\n",
        "loss_function = nn.CrossEntropyLoss().to(dev)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=(lambda epoch: 1/epoch))\n",
        "\n",
        "min_valid_loss = np.inf\n",
        "max_f1_score = 0\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(10):\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Each epoch, the model trains on half of the training dataset. \n",
        "  # The first epoch will train on the first half, then the second epoch will train on the second half, and so on, alternating every epoch.\n",
        "  halfway_idx = int(len(training) / 2)\n",
        "  if epoch % 2 == 0:\n",
        "    training_subset = training[:halfway_idx]\n",
        "  else:\n",
        "    training_subset = training[halfway_idx:]\n",
        "\n",
        "  # Training loop:\n",
        "  train_loss = 0.0\n",
        "  batch_loss = 0.0\n",
        "  batchStart = time.time()\n",
        "  mlp.train().cuda()\n",
        "  for i, data in enumerate(training_subset):\n",
        "    feature_vec = getFeatureVec(data).to(dev)\n",
        "    label = data['label'].to(dev)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = torch.reshape(mlp(feature_vec.to(dev), data), (1, -1))\n",
        "    loss = loss_function(outputs, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    if i % 500 == 499:\n",
        "        print('Training loss after mini-batch {:5d}/{:d}: {:.3f}'.format(i + 1, len(training_subset), batch_loss / 500))\n",
        "        batchTime = time.time() - batchStart\n",
        "        remainingTime = ((len(training_subset) - (i + 1)) / 500) * batchTime\n",
        "        print(\"Time remaining for current epoch: {}\".format(str(datetime.timedelta(seconds=remainingTime))))\n",
        "        batch_loss = 0.0\n",
        "        batchStart = time.time()\n",
        "\n",
        "  print(\"Training done. Validating...\")\n",
        "  \n",
        "  # Validation loop:\n",
        "  valid_loss, f1 = evaluate(mlp, validation)\n",
        "  \n",
        "  # Decrease learning rate (per epoch)\n",
        "  scheduler.step()\n",
        "\n",
        "  # Epoch complete\n",
        "  print(\"Epoch {} complete. Training loss: {:.5f}. Validation loss: {:.5f}. Validation F1 score: {:.5f}\".format(epoch+1, (train_loss / len(training_subset)), (valid_loss / len(validation)), f1) )\n",
        "  if f1 > max_f1_score:\n",
        "    print(f'F1 score increased ({max_f1_score:.6f}--->{f1:.6f}) \\t Saving The Model...')\n",
        "    max_f1_score = f1\n",
        "    # Saving State Dict\n",
        "    torch.save(model.state_dict(), 'saved_model.pth')\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/2192 complete.\n",
            "200/2192 complete.\n",
            "300/2192 complete.\n",
            "400/2192 complete.\n",
            "500/2192 complete.\n",
            "600/2192 complete.\n",
            "700/2192 complete.\n",
            "800/2192 complete.\n",
            "900/2192 complete.\n",
            "1000/2192 complete.\n",
            "1100/2192 complete.\n",
            "1200/2192 complete.\n",
            "1300/2192 complete.\n",
            "1400/2192 complete.\n",
            "1500/2192 complete.\n",
            "1600/2192 complete.\n",
            "1700/2192 complete.\n",
            "1800/2192 complete.\n",
            "1900/2192 complete.\n",
            "2000/2192 complete.\n",
            "2100/2192 complete.\n",
            "100/2192 complete.\n",
            "200/2192 complete.\n",
            "300/2192 complete.\n",
            "400/2192 complete.\n",
            "500/2192 complete.\n",
            "600/2192 complete.\n",
            "700/2192 complete.\n",
            "800/2192 complete.\n",
            "900/2192 complete.\n",
            "1000/2192 complete.\n",
            "1100/2192 complete.\n",
            "1200/2192 complete.\n",
            "1300/2192 complete.\n",
            "1400/2192 complete.\n",
            "1500/2192 complete.\n",
            "1600/2192 complete.\n",
            "1700/2192 complete.\n",
            "1800/2192 complete.\n",
            "1900/2192 complete.\n",
            "2000/2192 complete.\n",
            "2100/2192 complete.\n",
            "100/1509 complete.\n",
            "200/1509 complete.\n",
            "300/1509 complete.\n",
            "400/1509 complete.\n",
            "500/1509 complete.\n",
            "600/1509 complete.\n",
            "700/1509 complete.\n",
            "800/1509 complete.\n",
            "900/1509 complete.\n",
            "1000/1509 complete.\n",
            "1100/1509 complete.\n",
            "1200/1509 complete.\n",
            "1300/1509 complete.\n",
            "1400/1509 complete.\n",
            "1500/1509 complete.\n",
            "100/977 complete.\n",
            "200/977 complete.\n",
            "300/977 complete.\n",
            "400/977 complete.\n",
            "500/977 complete.\n",
            "600/977 complete.\n",
            "700/977 complete.\n",
            "800/977 complete.\n",
            "900/977 complete.\n"
          ]
        }
      ],
      "source": [
        "testing_f1_scores = {}\n",
        "for name, dataset in testing.items():\n",
        "    f1 = evaluate(mlp, dataset)\n",
        "    testing_f1_scores[name] = f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SE2 F1 score: 53.132858214738135\n",
            "SE3 F1 score: 53.132858214738135\n",
            "SE13 F1 score: 49.13122993639893\n",
            "SE15 F1 score: 48.29437875097964\n"
          ]
        }
      ],
      "source": [
        "for name, score in testing_f1_scores.items():\n",
        "    print(name, \"F1 score:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sense Predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictor function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predictSense(sentence, polyseme):\n",
        "    idx = 0\n",
        "    polyseme_text = \"\"\n",
        "    split_sentence = list(map(lambda x: re.sub('\\W+', \"\", x), sentence.lower().split()))\n",
        "    \n",
        "    if isinstance(polyseme, str):\n",
        "        try:\n",
        "            idx = split_sentence.index(polyseme.lower())\n",
        "        except ValueError:\n",
        "            print(\"Given string not found in sentence.\")\n",
        "            return\n",
        "        polyseme_text = polyseme.lower()\n",
        "    elif isinstance(polyseme, int):\n",
        "        idx = polyseme\n",
        "        polyseme_text = split_sentence[polyseme].lower()\n",
        "    \n",
        "    data = {\n",
        "        'sentence': sentence,\n",
        "        'polyseme': polyseme_text,\n",
        "        'idx': idx,\n",
        "        'senses': [lemma for sense in wn.synsets(polyseme_text) for lemma in sense.lemmas()]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        feature_vec = getFeatureVec(data).to(dev)\n",
        "        output = torch.reshape(mlp(feature_vec.to(dev), data), (1, -1))\n",
        "        pred = torch.argmax(output)\n",
        "        predicted_sense = data['senses'][pred].synset().definition()\n",
        "\n",
        "        print(\"Predicted sense for\", polyseme_text + \":\")\n",
        "        print(\"\\t\", predicted_sense)\n",
        "    except:\n",
        "        print(\"No sense definitions for polyseme found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['can', 'i', 'ask', 'for', 'the', 'bill?']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"Can I ask for the bill?\".lower().split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try it out! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sense for lovely:\n",
            "\t appealing to the emotions as well as the eye\n"
          ]
        }
      ],
      "source": [
        "predictSense(\"This is such a lovely day!\", \"lovely\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sense for stand:\n",
            "\t be standing; be upright\n"
          ]
        }
      ],
      "source": [
        "predictSense(\"Could you stand up please?\", \"stand\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sense for stand:\n",
            "\t hold one's ground; maintain a position; be steadfast or upright\n"
          ]
        }
      ],
      "source": [
        "predictSense(\"Let's take a stand against the government.\", \"stand\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x5I3y0a98T_H",
        "BJ0wHYFs8bjo",
        "LGHkvHoK8Pqm",
        "lbFaiOfG85dd"
      ],
      "name": "WSD_with_BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
